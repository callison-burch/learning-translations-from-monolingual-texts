--------

ln -s /mnt/data/anni/Experiments/az-en az-en 
soft link, linking az-en to first path, where you want the link to appear

Babel grant report:

1. Tamil stuff for student seminar:
/home/hltcoe/airvine/bin/moses/ta_en/NOTES
working on processing wikipedia files
after those are ready, ready to score with them. dictionary based PT exists. need to generate compositions and filtered phrase pairs too
No context found!

2.
Backing up crawls data from a01: 
Copying over now: /home/hltcoe/airvine/Crawls/Feb12012Backup_ProcessedCrawls
Copying UR now, that should be it

prep data for any language here, on a01:
/mnt/data/anni/Experiments
Running on Azeri now! Collecting monolingual features, supposed to do decode only, based on scored dictionary. Will need to do transliteration of any OOVs after
Note: need to fix up prep step to include dictionary movement and swap direction, etc.

2/17/12 update:
Output on UZ. Problem is that dictionary coverage is so low, output is garbage. Need to go ahead and get phrases from filters and/or LSH NN with corpora. At least induce unigram translations. Pipeline is up, can substitute in phrase table from anywhere to score.

4/10/12:
Have MTurk dictionaries from Dimitri:
/home/anni/Resources/MTurkDicts
@shreejit:has a convertDict.py and dictionaries for the various languages
To get lang-specific dict:
grep "^uz" dictionary_dump.2012-04-19.txt > dictionary_dump.2012-04-19.txt.uz

Prep for SO done:
/mnt/data/anni/Experiments/so-en
Running Somali experiment here:
/home/anni/Tools/moses/atools/alllangs-en
Had to get rid of pipes in devtest set
Done

4/11/12:
somali output done
bengali running
using scp to get Wikipedia pages from proper node
Done: Add translations from Dimitry to babel dict pointer for context scoring (in prepData.py)
@shreejit:in anni/BabelResultsFiles/prepData.py
Problem with ID: same factor error, but don't see any spurious |'s...:
Solved: they were in the test set:
sed s/\|//g devtestdev.id > devtestdev.id.clean
mv devtestdev.id.clean devtestdev.id
ID re-run. Looks pretty good
Rerunning so now. Problem is that there's a space that won't go away...arg. see: cut -f 2 so-en.dict | sort -u | less

To-Do:
Re-run SO and BN with translations in babel context projection step (all taken care of in prepData.py)
If time: induce translations for remaining OOV words
Lower case test sets if roman script?

5/3/12:
Ready to score all OOV words against all target language words (EN Wikipedia vocabulary with some freq threshold?)
/home/anni/Tools/moses/babel/allLangs/lexinduc
@shreejit:anni/lexinducresults/<lang> has  data for 3 runs and individual and summary
1. MRR
2. context
3. edit
4. time data

To do:
Write script to update $SRC$, $SRCPHRASES$, $TRGPHRASES$, and $OUTPATH$, and run scoring, add top outputs to phrase table
Writing all English Wiki vocabulary here:
/home/anni/Tools/moses/babel/allLangs/lexinduc

5/5/12
24 languages with crawled data AND MTurk dictionaries (/home/anni/Resources/MTurkDicts). Highlighted in purple on spreadsheet.
Crawled data moved to all be in Crawls.src-en.01162012/Combined ; changed data and/or InProgress->Combined if necessary to keep consistent

Looking at lexinduct/NBestCollector and DataPreparer code
@shreejit:this is in home/hltcoe/airvine/distribRep/DistributedRepresentations/src/unisaar/preprocessing/
Collect candidate and context equivalence classes for ALL words. CAN add a frequency threshold if want:
PruneIfOccursMoreThan/PruneIfOccursFewerThan for each of candidates and contexts
Then, the seed dictionary and test dictionary are the intersection between
1) seed dictionary: given dictionary and context equivalence classes
2) test dictionary: given dictionary and candidate equivalence classes
Not as big as entire dictionary even if no frequency thresholds b/c not all dictionary words in monolingual corpora
Induces translations for most frequent x words (experiments/NumSource xml param)
OR, for random src words
-> added xml parameter: experiments.DictionaryAllowSeedTestOverlap if set to true, doesn't remove phrases want to induce from seed dict used to project. false and default, does remove

So, for each language, need to:
1. Generate word frequency vs. accuracy (three lines: top 1, top 10, top 100) - vary NumSource parameter (1, 10, 25, 50, 100, 150, 300, 1000, 2500, 5000), pull from output/aggmrr.eval for each
- Need new preprocessing and output directory for each (not most efficient, but easiest)
2. Dump top 100 translations for each source word in the dictionary - when NumSource at max (-1)

Found main.lexinduct.FreqBinInductor, which you can give a number of bins and a number of words. It'll split into equal size bins and report average frequency and accuracy in one nice, neat output. 

--------
(1-clspold) /mnt/data/anni/Experiments/LexInductExps/langExp.py az (or langExpb.py w/ translit dictionary)
@shreejit:not here
(2-clspold) /mnt/data/anni/Experiments/LexInductExps/langExp2.py az (or langExp2b.py w/ translit dictionary)
(3-clspold) python processLang.py az (or processLangB.py w/ translit dictionary)
(8clsp) qsub -pe smp 4 -cwd -l mem_free=20G -l hostname=a01 -v PARAMS=allDict.az.confp -V qrun.pbs
(10clsp) 
(clsp)qsub -pe smp 4 -cwd -l mem_free=20G -l hostname=a01 -v PARAMS=bn.bb/allDict.IND.bn.100.confp -V qrunAll.pbs 
(clsp)qsub -pe smp 4 -cwd -l mem_free=20G -l hostname=a01 -v PARAMS=bn.bb/allDict.IND.bn.100.bplcw.confp -V qrunAll.pbs


For a given language, write babel config and run files, run all jobs for (1)

6/18/12 COMBINED DICTIONARIES: add to script langExp to put here: /mnt/data/babel/Resources/Dictionaries/originalplusmturk/$SRCLANG$/

(1) /home/hltcoe/airvine/inducePhraseTable/LIMT/Experiments/LexInductExps/python langExp.py az notranslit -- specify translit or notranslit.
@shreejit:don't have this directory here at all. Might be missing
need translit dictionary in translitlists/t.langen ; uses all of david's dictionaries and mturk dictionaries for projection. mturk dictionary only for evaluation. three runs.

Then, when above done (column B in spreadsheets highlights when done, bold when running), process output:
(2) /home/hltcoe/airvine/inducePhraseTable/LIMT/Experiments/LexInductExps/python langExp2.py az 


Then, from THIS LAPTOP:
/Users/anni/Projects/unsup_translation/babelClean/lexInducResults
**move old data to mturk_proj
(3) python processLangCOE.py az

This script copies from clsp, deals with getting avg. freq from log files, moves things around, runs r script to generate graph
For now: printing graph of aggregate MRR for each language. Easy to ALSO do for edit distance, context, and time

Evaluation here is, of course, on words that are in the dictionary, which is a particular subset of the vocabulary... words we're actually interested in translating MAY be harder. Other issues: Malaysian is SUPER high in comparison to others; why? --lots of English words. Can't get all of Nepali 0's on graph.

Lexicon induction for oovs based on (1) crawls and (2) wikipedia
(4) translateOOVs.py az translit/notranslit
@ashreejit: couldn't find this file anywhere


When above done, this will report how many of the OOV words could be translated by monolingual data and it grabs top k (now set to 50) translations for OOV words only (50 each from wikipedia and crawls translations), then can add those to old MT pipeline for scoring, etc.:
(5) python translateOOVs2.py az 

Then, from this laptop:
old-clsp (no longer using this eval) python /Users/anni/Projects/unsup_translation/babelClean/LIMTresults/byFreqGraphs/processLangStats.py az (or processLangStatsB.py w/ translit dictionary) This script will get summary statistics for edit, time, context, and aggr over entire dictionary (from 4)

NEXT:
MT w/ induced oov translations
prep MT data:

(7) python prepData.py az translit/notranslit

Then run MT experiment (w/ translations from MTurk dictionary, David dictionary, and OOV induced)... easy to remove David dictionary if want
From /home/anni/Tools/moses/atools/alllangs-en

(8) qsub -cwd -l mem_free=40G,h_rt=36:00:00,h_vmem=40G,num_proc=4 -v PARAMS=allDict.az.confp -V qrun.pbs

Rewrote entire pipeline for decoding in python, including prep modules. Now working
Update: changed babeltemplate files to use mturk dictionaries for phrase table scoring too

******** Indian Languages:
Modified pipeline.py to write monolingual scoring output as it goes and wait for it to end with "===> Done <==="

Prepare everything for translating Indian language test set using same dev set and varying amounts of training data (*no* oov translations)
(9) IndianLangsCorpus/monoTranslate.py bn 

Run EACH job separately, for each size training data:
(10)
qsub -cwd -l mem_free=40G,h_rt=36:00:00,h_vmem=40G,num_proc=4 -v PARAMS=bn.bb/allDict.IND.bn.100.confp -V qrunAll.pbs 
qsub -cwd -l mem_free=40G,h_rt=36:00:00,h_vmem=40G,num_proc=4 -v PARAMS=bn.bb/allDict.IND.bn.100.bplcw.confp -V qrunAll.pbs 

Do monolingual-only job: confp generated from monoTranslate.py
(11) qsub -cwd -l mem_free=40G,h_rt=36:00:00,h_vmem=40G,num_proc=4 -v PARAMS=allDict.IND.bn.confp -V qrunAll.pbs 

Mine OOV translations from this data in same way as before - find oovs, run induce job. Note babel config files now include translit dictionaries
This will produce translation candidates for OOV unigrams using crawls AND wikipedia data (separately)
(12) python translateOOVs.py bn translit

check and extract translation dictionary:
(13) python translateOOVs2.py ta

--------Need to update to COE cluster from here down--------

Prep for experiment with monolingual scoring, dictionaries, AND induced translations from two sources (crawls + wikipedia):
(14) python monoTranslateWOOV.py ta    <---- w/ tamil character fixed

Run job for exp. with monolingual scoring, dictionaries, AND inducted translations from two sources:
(15) qsub -cwd -l mem_free=40G,h_rt=100:00:00,h_vmem=40G,num_proc=4 -v PARAMS=allDict.IND.ta.oov.confp -V qrunAll.pbs


(16) python biplusmonoprep.py ta

prep for supplementing entire training bitext with existing dictionaries (w/ and w/o induced translations). also preps bilingual data only + released dictionaries as "releasecomplete" job

(17) qsub -pe smp 4 -cwd -l mem_free=20G -l hostname=a01 -v PARAMS=bn.bb/allDict.IND.bn.bidict.bplcw.confp -V qrunAll.pbs

for bidict.bplcw.confp, bidict.confp, bidictoov.bplcw.confp, bidictoov.confp, releasecomplete.bplcw.confp, releasecomplete.confp


Transliteration:
Get list of entire vocab for language (from Indian languages bitext, from MTurk dictionaries, from Wikipedia 5-pages) and copy to coe, including dup for english answers (just to have files of correct length there):
(18) python getTranslitListForJoshua.py bg (MTurk dictionaries vocab only)
Get Wikipedia training data if doesn't already exist: /home/hltcoe/airvine/translit_demo/miniscale_translit/wikipedia_data
Prep train/dev sets for transliteration:
(19) /home/hltcoe/airvine/Joshua/Experiments/translit/python prepDataLexInduc.py bg
Execute translit train and test job:
(20) qsub -b y -cwd -l h_vmem=20g,mem_free=10g,h_rt=6:00:00 -V ./run_bg.sh
Post-process output:
(21) python postTranslitListForJoshua.py bg
Then just use configfiles/babelLITJ.xml

----------------------scratch all of this, joshua is awful.
Working for ne, hi
python getTranslitList.py hi
paste translitlists/t.ne translitlists/t.ne.en > translitlists/t.neen
Ready to go for cyrillic. Find lookups for arabic script and add to dictionary! ---> in  map.txt in home dir on coe now. 
a01 down, so can't work on it now... should be relatively easy to add as a transliteration dictionary to the babel config files. As long as that's an option for use in the lex induc stuff, not just the phrase pair scoring...



TO-DO
----
1. Add transliteration to all <- implemented, rerunning (NEED TAMIL AND BENGALI)
2. Induce oov translations from wikipedia data <- implemented, need to rerun for all (only in translateOOVs script WITH transliteration, not the one w/o... add dummy transliteration file to all...or check to see if one exists in script? somehow should merge those _ and B scripts)
3. Finish tech report
4. Add transliteration scores to oov inducting for Indian language MT
Vogel & Zhang - papers on learning translations from the web (targeting specific unknown words)

----
probabilistic dictionary value vs normal
add syntax grammar - can probably learn a lot of syntax from small amount of bitext... using syntax might reduce the need for memorizing phrases, ditransitive 'give' example, so could just use syntax and oov translations and maybe pos information
pos info - e.g. should my oov be translated as a noun or verb


----
Integrating New Version of Moses:
qsub -cwd -l mem_free=40G,h_rt=36:00:00,h_vmem=40G,num_proc=4 -v PARAMS=ta.bb/newpipe.test -V qrunAllTest.pbs


Features from both Wikipedia and Crawls:
For each language have put wiki data in /mnt/data/anni/Wiki - copying over status indicated on spreadsheet

For Indian languages: will use all monolingual features in decoding, including tuning
For other languages: will use all monolingual features just for decoding

Also to-do:
Moses-tokenize Wikipedia data!
Get real tuned parameters for crawls + wikipedia (from tamil, for example)

--------

LSH

Regenerated ngram corpora, gathered ngrams UP TO a given length.
Es and En generated, signatures generated - not position sensitive, two word contexts on either side
Nearest neighbors being generated now

Check that dictionary projection is working properly
write number of unique features in SLSH write output
try high quality small dictionary
apple to apples: only score unique source and target phrases from good phrase table, compare perfect and approximate scores
me: try other ngram corpus, exactly x-gram, x=7.... ask ben about this, will still double count words in sliding window

2/9/12: fixed dictionary projection in ngramsigbuilder.

2/10/12:
meet with ccb: make sure to tell story about the difference between model error and search error in identifying phrase pairs. 
one way to do this is to just score unigram pairs and compare with a dictionary
another way to do it is to limit the search space to those phrases that we observe in the bilingually extracted phrase table; can compare scores with how they're scored in the phrase table (not all pairwise there, but can compare with the ones that are)
that will remove any search error and we can give a discussion of model error. there IS model error when scoring the bit signatures. but the goal is to IDENTIFY phrase pairs, we can always do exact scoring later. but model errors will be when we do consider a pair of phrases in a beam but decide they're not similar enough to be included in the top-k. ah ha. yes. remove search error by considering beam of size of list and identify model errors in top-k, where k varies and errors increase as it gets small and decrease to 0 when beam is max.

2/14/12:
added flexibility in context that's collected: window size, position sensitive or not
also added language code hack and tfidf scoring. lots of stuff running overnight

2/16/12:
added eval scripts: get rank for each pair in trace table, then get % phrase pair types and tokens found in top k graph

2/17/12 update:
Added flexibility: window size, position sensitive or not, direction sensitive or not, tfidf weighting or not, language tags for kbest lists, variable length ngram corpus
Also have evaluation running that eliminates search errors: beam size entire set of potential phrases. Limiting ES and EN phrases to those in the Moses trace table. Outputs now are window size 2 on both sides, position & direction sensitive, with tfidf weighting. Need to try bag of words to be consistent with our old experiments. Maybe increase number of bits.
Output: complete ranking of English phrases that appear in the trace table at all (17k total) for each of the Spanish phrases that appear in the trace table (19k total). Nice because we have perfect cosine computed for the good pairs already. Feasible to get for entire pairwise: 400,000,000 pairs. Could also subsample the space of ES phrases
-Dictionary-based evaluation not great b/c some unigrams appear infrequently, if ever, in particular monolingual corpus (europarl)

2/24/12:
Evaluation setup:
10,000 ES phrases from phrase table (randomly selected or highest frequency or something else?) with their top-1 EN translation. 
Compare:
1. 100,000,000 n^2 comparisons, do perfect cosine contextual similarity. Calculate precision and recall at varying similarity thresholds.
2. LSH/PLEB setup with approximate scoring (use some default beam, # permutations, and # bits). Calculate precision and recall at same similarity thresholds. 
Compare with 2:
3. vary # bits in signatures
4. vary beam search size (up to entire possible search space, 10,000, to estimate only the loss resulting from the LSH approximation)
5. vary # permutations

2/25/12:
Written 5000 most frequent ES phrases (in Europarl training data) that appear in the trace table at all, along with their most used (in decoding) EN translation (random choice if tied):
inducePhraseTable/allPhrases/PhraseTableKeysBestFreq/EsEnPT.mostFreq

In phrase table format:
EsEnPT.mostFreq.phrasetable

Then, scoring exactly:
./run.sh babel.jar babeltemplate.xml 8g   : done - output on 5000 in babelScored/

NEXT:
pair all ES with all EN: do 5k * 5k scoring. 25 million pairs. Should be totally doable. Get numbers, maybe up the 5k number later if want more significant results. Right now these phrase pairs are all really common words/phrases. 
queued up to write EsEnPT.mostFreq.phrasetable.allpairs now
THEN:
qsub -b y -cwd -q himem.q -l h_vmem=90g,num_proc=8 -V ./runBabelTest.sh

LSH Run on same data (5k ES phrases with 3941 EN phrases):
In EuroparlNGramTrace5k/
getRanks.py updated to only look at best trace table entries (as definted above)


Write evaluation to estimate precision at varying thresholds like in Jansen paper: do quick tomorrow morning - done except not *average precision* as is done in that paper (average over each recall position). If I did that I would have to average over average precisions (over different source phrases). Is that what Aren does? Or is he getting a ranked list over all possible pairs... just rank cosines, not within a single source language phrase

Parallelize writing phrase table pairs for all pairs, also parallelize exact cosine context scoring: writing five ways now.

2/27/12
Meeting with Ben:
My precision term is conservative
Get recall values for exact scoring: DONE, and they're worse! Must investigate if cosine is defined the same way...there are no negative values in babel output
Mean absolute error is term for that 0.17 average absolute difference
Drop most frequent 1,000 - so use top 5k minus that very top 1k. They'll translate into everything, stop words, etc.
Use monoDS to propose phrase pairs in MT setup, above and beyond what's already in the phrase table
Start with dictionary, propose additional pairs - make dictionary the baseline, forget about seeing how close you can get to parallel situation

2/28/12
1.
Re-Run of exact:
Fixed approximate IDF calculation, now it is exactly how the exact calculation is made.
Eliminated 1k MOST frequent phrases. Now it's the most frequent 4k ES phrases used in decoding after the first 1k frequent. 13.7 million pairwise comparisons.
Re-running exact scoring: not using stopwords and not pruning contexts by frequency. Now should be *exactly* the same as the approximate scoring (rerun below)
/home/hltcoe/airvine/inducePhraseTable/allPhrases/PhraseTableKeysBestFreq/1kTo5k

Full n^2 exact:
Each partial phrase table should have 1,709,000 phrase pairs (3418*4000). 
qsub -b y -cwd -V ./writeallpairs.sh 1

Running scoring:
qsub -b y -cwd -q himem.q -l h_vmem=90g,num_proc=4 -V ./runBabelTest.sh
1,2,3,4 done
5,6,7,8 done

Or, on new queue:
qsub -b y -cwd -l h_vmem=80g,mem_free=40g,h_rt=4:00:00 -V ./runBabelTest.sh (specify TIME too) running w/ only one thread now

2.
Re-run of LSH:
Running:
Changes on keyset (skip over most frequent 1k ES phrases) and IDF scoring
/home/hltcoe/airvine/LSH/ExpData/EuroparlNGramTrace5k (previous run moved into Top5k/)

3. 
sigs built, nbest done
64, 128, 512, 1024, 2048, 8192, 16384, 32768

expanded number of bits: no error by approximation (type of search error). Model error would be context similarity can't estimate translation probabilities. Other type of search error involves beam size --  running with 120k bits now. Too many phrase pairs. Backoff to subset of traced phrase table. To remove error by approximation bits needs to be = size of vocab that could appear in context (may be possible with no position and direction sensitivity)

qsub -b y -cwd -l h_vmem=100g,mem_free=50g,h_rt=4:00:00 -V ./runPLEBIndexPT.sh

qsub -b y -cwd -l h_rt=4:00:00 -V ./runtrace.sh

Mean average precision/area under curve function for kbest list with correct answers indicated:
EuroparlNGramTrace5k/MAP.py nearestneighborsfile topknumber threshold
python MAP.py Sigs/enes.2.langtag.trace.3.3941.10k.128.nearestneighbors 100 0.75

Evaluation on exact scoring:
Not good. See google doc. MAP isn't great with exact scoring either... top 100 and threshold 0.25 gives MAP 1.25 (or average 1% precision). 
New thought: this is a noisy signal. Not going to work on its own for proposing phrase pairs. Need to combine with other signals. Good news: not that different from exact scoring, so speed up might be there to gain.

3/8/12
Use consensus dictionary instead of big:
Generating 4096 bit signatures for ES. Already have English, so can go ahead and do PLEB once generated

----

3/9/12

1. Correct if in the original phrase table at all (no longer conservative precision; include some noise, but random in different way from context scoring)
2. Percent of ES phrases that have correct translation in top k
3. Update MAP, ignore ES phrases that have no translations above the treshold for a given k
4. Use 10k lines of parallel text; probabilistic dictionary for projection; then just evaluate on additional needed phrases
5. CL journal starting in may
6. score unigrams, then find max alignments using those scores
7. start writing paper (don't lose sight of point of the project). tech report on different ways of scoring (pros and cons of each) and results


3/14/12:
Write rescoring output to file:
python MAP_PT_writeoutput.py babelRescoreTopk/output/phrase-table-p 25 0.5

Can get a reasonable recall for translating ES phrases in top-1 thousand en phrases with 128-bit signatures when EN search space is only 4k phrases. Can do a great job of reranking essentially with exact temporal scores after search space cut from 4k to 1k. Is that also true when search space is really huge? 
---> Increasing search space, generating 128-bit signatures for EN phrases

1. Approximate search with huge number of English target phrases
/home/hltcoe/airvine/LSH/ExpData/EuroparlNGramBig initialized with things, but not used yet. Is it worth doing something big if I know the output is going to be crap?
Running signatures now

----
Moving towards combining proposed phrases from multiple signals (context, topic, temporal)

3/20/12:

For temporal scoring: all I need is a list of up to trigrams (if that's still the max phrase length) that appear on each day. Looks tricky to change Jerboa code to keep up with current filename in the stream processing. So, just need to preprocess by-date files, then can build signatures based on those dates instead of context window.
Could have different files for each date, or could list dates (comma separated) on a line for each ngram. That will probably be more efficient than reading in many, many lines with the same ngram
-> go with this. Just need preprocessing script to output a single file with phrases, frequencies, and list of dates
File input format: occurring phrase (with no context) \t frequency,file date \t frequency,file date \t frequency, file date

Nice thing about this is can use exactly the same file input format for topic signatures, just list topic instead of file date

Debugging:
./anni-process-stream-en-pt-tt.sh /home/hltcoe/airvine/LSH/ExpData/EuroparlNGramTrace5k/Props/signatures.ngramtemptop.langtag.en.temptop.properties
NGramSigBuilderTempTop.java
Example file format:
test of my      12,12251999|||21,06042001|||1,10152004
phrase \t weight,stamp|||weight,stamp|||weight,stamp

Doing preprocessing on europarl:
/home/hltcoe/airvine/inducePhraseTable/temptopPhrases
qsub -b y -cwd -l h_vmem=25g,mem_free=20g,h_rt=16:00:00 -V ./makeTemporal.sh


Generating temporal signatures:
qsub -b y -cwd -l h_vmem=80g,mem_free=40g,h_rt=12:00:00 -V ./runNGramEnEuroTT.sh
qsub -b y -cwd -l h_vmem=80g,mem_free=40g,h_rt=12:00:00 -V ./runNGramEsEuroTT.sh

Indexing:
qsub -b y -cwd -l h_vmem=100g,mem_free=50g,h_rt=20:00:00 -V ./runPLEBIndexPTTT.sh

3/21/12:

ES monolingual temporal/topical preprocessing:

Preprocessing ES-EN Gigaword temporal phrases:
/home/hltcoe/airvine/inducePhraseTable/temptopPhrases/Gigaword
qsub -b y -cwd -l h_vmem=35g,mem_free=30g,h_rt=12:00:00 -V ./makeTemporal.sh
->taking longer than 12 hours, rerunning with 36
Updated time wall mid-job with:
qalter -l num_proc=1,h_stack=256M,h_vmem=80g,mem_free=40g,h_rt=120:00:00 107603

Preprocessing ES-EN topical phrases:
/home/hltcoe/airvine/inducePhraseTable/temptopPhrases/WikipediaTop
qsub -b y -cwd -l h_vmem=35g,mem_free=30g,h_rt=12:00:00 -V ./makeTopical.sh

Positive examples for classification here:
/home/hltcoe/airvine/inducePhraseTable/classifyPhrases/goodPairs

Generating negative examples for classification:
qsub -b y -cwd -l h_vmem=30g,mem_free=25g,h_rt=12:00:00 -V ./genRandomPairs.sh
/home/hltcoe/airvine/inducePhraseTable/classifyPhrases/badPairs

/home/hltcoe/airvine/inducePhraseTable/classifyPhrases/badPairs
Scoring exactly with gigaword and wikipedia babel code separately (no reordering of course)

Converting toy phrase table into Mallet format (in this toy example, phrases in trace table are marked as good. Will have bigger phrase table and more even balance of good/bad in real experiment):
/home/hltcoe/airvine/inducePhraseTable/classifyPhrases/tracedOnly
qsub -b y -cwd -l h_vmem=35g,mem_free=30g,h_rt=12:00:00 -V ./malletConvert.sh
-had to change memory in $MALLET_HOME/bin/mallet
*Naive Bayes classifier does not support numeric feature values?!
Using MaxEnt
Problems with Mallet:
weirdest thing is that it runs just fine on sample data. No apparent difference between the two...ahah, apparently just some bad line in there somewhere. Works ok with subset.
Want to see nothing with:
grep "NaN" balanced.classifytable.trace
41627 training examples if evenly balanced (b/c trace table isn't super big)
qsub -b y -cwd -l h_vmem=40g,mem_free=20g,h_rt=24:00:00 -V ./malletClassify.sh
Binary classification performance on used-in-trace-or-not about 75%
With less balanced classifier (all data), classifies everything as bad, 99% accuracy.

3/23/12:
Have wikipedia scoring of random pairs as negative classifier training
Have wikipedia scoring of good (entire) phrase table as positive classifier training
Go ahead and just use those features (gigaword temporal and contextual not done, will take forever, ignore for now)
Changes that need to be made to this evaluation:
1. Only use phrases extracted from ~2000 sentences as positive evidence. I.E. Will evaluate on phrase pairs extracted from other 500 sentences. There'll be some phrase overlap, but that's ok I guess... Maybe not?
2. Feature set shouldn't be exact similarity scores. Should be approximate, etc. Need a pipeline to get approximate scores for any arbitrary pair of phrases, without PLEB search stuff. Probably already exists, but need to find it
3/26/12:
Results from above Wikipedia-exact only scoring: awesome. 98% accuracy on test data, when balanced training and test. 
Former to-do:
1. After scoring of random pairs is done, Combine phrase tables in the same way that were combined for above
2. Reformat both good and bad tables into mallet input format, scramble examples: see what was done in inducePhraseTable/classifyPhrases/tracedOnly/
3. Run mallet classification, confirm that can use signals to classify this training data (will mean it's hopeful for other proposed pairs)
-> Check: classification is good

----

3/26/12

Extracted phrase table on 500 sentences of development set:
/home/hltcoe/airvine/inducePhraseTable/devSetSample
qsub -b y -cwd -l h_vmem=15g,mem_free=10g,h_rt=2:00:00 -V ./pipeline.perl
-> Done

To do:
1. Gigaword preprocessing:
Divided into chunks of 500 files (there are 5249 each english and spanish files)
/home/hltcoe/airvine/inducePhraseTable/temptopPhrases/Gigaword
qsub -b y -cwd -l h_vmem=25g,mem_free=20g,h_rt=36:00:00 -V ./makeTemporal.sh 0 250 0
Done for Spanish, and English
*No reason to combine files, can read in multiple files in signature building

2. Generate signatures for:
- For es devset phrases: topical (wikipedia) - DONE, contextual (wikipedia) - DONE, temporal (gigaword?), contextual (gigaword?)
- For all of en phrases in...? : topical (wikipedia) - DONE, contextual (wikipedia) -DONE, temporal (gigaword?), contextual (gigaword?)
Start with EN phrases in europarl -> for now, remove errors from insufficient list of EN phrases

Wikipedia topical signatures:
qsub -b y -cwd -l h_vmem=80g,mem_free=40g,h_rt=12:00:00 -V ./runNGramEnEuroTT.sh <- done, 3088436 signatures (of 14966935 keys, many don't appear in wiki pages)
qsub -b y -cwd -l h_vmem=80g,mem_free=40g,h_rt=12:00:00 -V ./runNGramEsEuroTT.sh <- added langtag to keys, done
Generating nearest neighbors: qsub -b y -cwd -l h_vmem=100g,mem_free=50g,h_rt=20:00:00 -V ./runPLEBIndexPTTT.sh
parameters: 5k NN, beam=10k, permutations=4

Generating Wikipedia ngram corpora:
qsub -b y -cwd -l h_vmem=25g,mem_free=20g,h_rt=12:00:00 -V ./getNGramCorpus.sh es 5 (es and en, 5 and 7) <- done

Note: don't think filenames on nearest neighbors files are quite right (4 permutations, not 8, etc.). Change those after jobs complete

Gigaword temporal signatures:
qsub -b y -cwd -l h_vmem=80g,mem_free=40g,h_rt=12:00:00 -V ./runNGramEsEuroTT.sh  <- done
qsub -b y -cwd -l h_vmem=80g,mem_free=40g,h_rt=12:00:00 -V ./runNGramEnEuroTT.sh  <- running for unigrams only and all


3. Get SVM light up and running, ready for reranking
Ready to go:
/home/hltcoe/airvine/Code/svm_light
Faster version:
/home/hltcoe/airvine/Code/svmlight (still need to test)


4. Then organize proposed phrases from different sources (LSH contextual, temporal, topical, dictionary, and dictionary composed)

5. Score proposed approximately: trial run of runSS.sh using simscorer.sh and edu.jhu.jerboa.sim.SimScore. Forgotten had already done that. Should be easy to do approximate scoring of given tab separated pairs and already built SLSH objects
See: /home/hltcoe/airvine/LSH/Jerboa/src/scripts/anni-simserver.sh
And: /home/hltcoe/airvine/LSH/ExpData/EuroparlNGramTrace5k/runSS.sh

6. Classify, NO, RERANK W/ SCORES (from CCB meeting 3/26), with SVM light

7. Intrintic (precision/recall) and task-specific evaluation (MT)

3/28/12:

Changing evaluation again. Don't need to translate ALL phrases. Have:
1. Dictionary translations
2. Translations from a seed training text
3. Things that look like they can be duplicated can be safely duplicated (numbers, punctuation, etc.)
Try to translate REMAINING phrases. 
New set, with 1k or 5k seed training text. Excluding the above and only including unigrams and multi-word phrases that appear more than once:
/home/hltcoe/airvine/inducePhraseTable/trainingSeed
Will still SCORE the dictionary and training data seed tables monolingually, because won't have reliable translation scores for any of that. But only try to find new translations for other things
-Might need to include some words only seen once in training data too, later.
N^2 comparisons of unigrams, THEN propose and rerank phrase translations
How many English unigrams are there on the target side? In what? Europarl? Gigaword? Wikipedia? What?

3/29/12:
Unigrams:
wikicont signatures for English unigrams (119632 of them) only -> done
qsub -b y -cwd -l h_vmem=60g,mem_free=30g,h_rt=8:00:00 -V ./runNGramEnCont.sh
wikitop signatures for English unigrams only -> done
qsub -b y -cwd -l h_vmem=80g,mem_free=40g,h_rt=8:00:00 -V ./runNGramEnEuroTT.sh
Top translations of spanish unigrams over english unigrams only:
1. wikitop.uni.plebindex.8.5k.50k.128.nearestneighbors
2. wikicont.uni.plebindex.8.5k.50k.128.nearestneighbors
3. Voweless dictionary entries for an input voweless word. And/or prefixed dictionary.
4. Just duplicate all words

Combine these outputs:
OutputForDevNoTrans/
Together include translations for 506 of 665 unknown unigrams
Simscoring all... should probably add found and dictionary to this list
qsub -b y -cwd -l h_vmem=20g,mem_free=10g,h_rt=8:00:00 -V ./simscorer.wikitop.sh
VERY, VERY, VERY, VERY FAST TO SCORE! Seconds.
Simply adding scores for now, out of simple interest. Combination script can be made to print out all scores for later reranking, etc.

Frequency filters: 109611809 pairs, 
Have dev-phrases-care-about's monolingual frequencies.
Writing output file with matching target frequencies
writePairsT.py
/home/hltcoe/airvine/inducePhraseTable/March12FiltersWork/dev.notranslation.paired
Recall: 159 of 1213 phrases, including 33 additional unigrams, beyond discovered above
Probably not worth using as-is for proposing, but can be added as a feature for reranking

----
3/30/12 meeting with CCB
-Auto align with huge dataset, then just use Xk sentences with good alignments
-Use phrase table for contextual feature projection instead of or supplement to dictionary
-
-Babel final report; dictionaries from Dimitri, induce translations for OOVs, score dictionary+oovs, decode
----

3/30
Running baseline: 
5k sentences, b-b, pl-m, bpl-m (with big LM)
1k sentences, b-b, pm-m, bpl-m (with big LM)
10k sentences, b-b, pl-m, bpl-m (with big LM)
pl-m scores from gigaword AND wikipedia

Then:
Supplement pl-m tables with above OOV. Goal: outperform b-b and above pl-m. Might need to play with feature space to include original b probs (like bpl-bm)

4/1/12:

Running unigram es signatures, then:
Gigaword indexing:
Uni: qsub -b y -cwd -l h_vmem=20g,mem_free=10g,h_rt=20:00:00 -V ./runPLEBIndexPTTT.sh 

4/2/12:
Using relaxed dictionary match, duplications, wikitop, wikicont, and gigatemp recall of 579 of 665 unknown unigrams (based on phrase table extracted on that dev data)
Training a reranker on all of that data:
/home/hltcoe/airvine/Code/svm_light
Note: need separate training and testing data for this reranking... another devset of similar size?

Got phrase tables for rest of dev set and devtest set here:
/home/hltcoe/airvine/inducePhraseTable/devSetSample
put in two additional directories
Then can induce translations for all

4/3/12:
building three signatures for ALL oov unigrams (in dev500, dev2053, and devtest), 902 unigrams:
Keyset here:
/home/hltcoe/airvine/inducePhraseTable/trainingSeed/allsets.notranslation.uni
signatures .uni.all same directory
TO-DO:
relaxed dictionary matching for those OOV unigrams (need to modify script)

First order of business: append top-k to phrase table as is (scored with three features, duplicate for phrasal and lexical feature...maybe add others as well in terms of where item came from, plus one about whether extracted from bitext or not)

4/4/12:
Getting signatures for all dev500, dev2053, and devtest no translations, dictionary found, and to-duplicate - all oovs in those three sets, no matter translation plan:
wikicont/wikitop/gigatemp.128.uni.all.es.*
English signature keys are: found duplicates, all devtestdev duplicated, dictionary translations, relaxed dictionary translations, and huge list from europarl before:
wikicont/wikitop/gigatemp.128.uni.all.en.*
{ES/EN} Key sets are here:
/home/hltcoe/airvine/inducePhraseTable/trainingSeed/{allsets.ddn.uni.langtag/allsets.ddrd.big.en.uni.langtag.u}

Once signatures built and top-k, place all hypotheses here:
/home/hltcoe/airvine/LSH/ExpData/EuroparlNGramTrace5k/OutputForDevNoTrans/originals-devtestdev/

4/5
1. Score concatenation of all hypotheses using 1. wikitop 2. wikicont (do single scoring, then just lookup scores for tables below)
/home/hltcoe/airvine/LSH/ExpData/EuroparlNGramTrace5k/OutputForDevNoTrans/originals-devtestdev

2. Add dictionary elements to pl-random phrase table using script combinetopcont.py in /home/hltcoe/airvine/LSH/ExpData/EuroparlNGramTrace5k/OutputForDevNoTrans

3. Tuning with:
A. Dictionary only: dictsupp-randomsupp
B. Dictionary + duplicate: dictdupsupp-randomdupsupp
C. Add dictionary + duplicate + relaxed dictionary: dictduprsupp-randomduprsupp
D. Add dictionary + duplicate + relaxed + gigatemp proposed: ddrgsupp-randomddrgsupp
E. Add dictionary + duplicate + relaxed + wiki-context proposed: ddrwcsupp-randomddrwcsupp
F. Add dictionary + duplicate + relaxed + wiki-topic proposed: ddrwtsupp-randomddrwtsupp
pipelineMultiFeat.perl updated to make it easy to do tuning and decoding only. the script automatically generates a random reordering table from given phrase table



----
Classification
Installing boost:
./bootstrap.sh --prefix=/home/hltcoe/airvine/Code/boost --libdir=/home/hltcoe/airvine/Code/boost
Trouble...
/home/hltcoe/airvine/Code/boost_1_49_0

Nevermind, it doesn't need to be installed. Updated makefile to point to unzipped boost directory
Compiling VW in:
no, need to reget git directory and start over. make sure specify BOOST_ROOT in path. Done.

/home/hltcoe/airvine/inducePhraseTable/VW/vowpal_wabbit

Still trouble with boost importing correctly in building vowpal wabbit. Abandon that. Using mallet instead. Just need good maxent classifier anyway
Installed:
/home/hltcoe/airvine/Code/mallet-2.0.7
echo $MALLET_HOME
To see possible commands:
bin/mallet
Help page is here:
http://mallet.cs.umass.edu/classification.php
My own directions for how to train classifier are here:
/home/hltcoe/airvine/Code/mallet-2.0.7/ANNI_NOTES

qacct -j 12345 | grep maxvmem

--------

To Do: 
2. Dictionary experiments: score dictionary, use differing percents of it. Supplement phrase table with those suggested by LSH. Include feature for "this pair is from dictionary or not"
3. topical preprocessing for signature building like temporal
4. generate candidates from approx contextual, approx temporal, approx topical, and maybe also dictionary itself


--------

Induce Phrase Table:

Read both Schoner papers in detail (EMNLP and image/text paper)

4/11/

Socher (EMNLP, read) and *Westin paper
Daume - Online Learning of Multiple Tasks and Their Relationships
 
 - Joint representations - individual words are tasks, learn or describe similarity between tasks
Direct comparison with Rapp methods, real benefit is the compositionality component
sparse alignments as regularizer
master's student working on this too
visit: understand math in two papers and then what needs to be done in terms of coding
Hal's paper:
multi task = individual words
interaction matrix = derive from information about related words from alignments

1. Single word translation induction
2. Phrases


--------

Urdu SMS

Put together distributable corpus
Put corpus on CCB's private github repo


--------

Task-based MT

http://trec.nist.gov/data/tweets/

Paul: LSM paper. 
Ground truth annotated:
Arabic, Farsi, Urdu (Arabic script)
Hindi, Nepali, Marathi
Russian, Ukrainian, Bulgarian

Experiments:
Huge amounts of in-domain data for both languages
Large amounts parallel corpora

Gender-id task
1.
- Train English (unigram) feature set, trained on labeled English tweets
  - Performance supervising over thousands of users, 100 tweets/user ~90% accuracy
  - IDs taken from user pages
2.
- Train Spanish (unigram) feature set, trained on labeled Spanish tweets
3.
- Project English unigram feature space, maintain feature weights, evaluate on Spanish
4.
- MT system to translate Spanish tweets into English

Nice story: the types of words that we're good at translating are the types of words that contribute a lot to these classifiers

Sebastian Pado, Manqual Faruqui (corpora list email about German-English formal/informal address)

Normalization: two people on SCALE are working on it, twitter specifically. Throw in as preprocessing later.

Tokenizer from Ben (he's working on now)

Spanish twitter set: already tokenized (in "accumulo", cell-level security credentials), will retokenize *Spanish* monolingual, unlabeled tweets with new
SCALE pushing everything into an accumulo database

MITRE dataset has gender labels, Ben will retokenize English and Spanish supervised sets

LibLinear - Jerboa has built-in wrapper for training, then uses the output weights for classification
Weighted accuracy on feature set (assuming have gold standard dictionary for the entire feature set...I can write if not in dictionary)

Alex Clemmer has used Jerboa before and knows classification pipeline - will be around COE
Olivia also will be working on it
Svitlana also knows how to do blackbox training stuff a lot

Remove all features really close to 0, and work with what's left (as long as that's somewhere around 1k features). At some point test and make sure performance doesn't go down too much

PLEB stuff:
not necessary with unigrams, but we can make it necessary with bigrams

5/16/12:

/home/hltcoe/airvine/LSH/TaskBased
578 of 1429 most informative features are 'hard' to translate (can't do by dictionary and punctuation indicators)

--------

DAMT

5/21/12
/home/anni/vw

PSD example:
/export/ws12/damt/springlab/psd_example_small


9/4/12:
- VW classification of context, time, and edit scores
- Somewhat different results from old b/c generous on ties (if 10 en words tied for 100th place, consider all in top 100) and also, more importantly, using half for training supervision and half of data for evaluation. so evaluation set is different. especially for bins with small numbers of words, could change look of performance
- Performance MUCH better

9/5/12:
Meeting with CCB:
- Push through new ranked list of OOV translations for Tamil/Hindi/Bengali
- Look at words that end up being really low ranked: what's going on with them? bad translations or just really hard examples? prune bad turk work and/or error analysis of what's hard
- In future think about tuning parameters in contextual/temporal/etc vector space 
- Think about reordering: gloss + then reordering to be natural
- Read Ben email and Alex paper and decide on those projects
To do by next meeting:
- Update thesis document with new lexicon induction graphs
- Erorr analysis on low ranked correct answers (maybe for Spanish)
- Push through new ranked list of OOV translations for Tamil/Hindi/Bengali

- Realized had only considered first English word if many listed on line separated by spaces. Fixed, rerunning for all languages. stdout and stderr aren't being written in qsub submission script...
- Doing regular lexicon induction using preprocessed wikipedia data too. 
- Then: features based on both crawls and wikipedia lexicon induction output
- Printing list of fr words and translations when en never on any of the ranked lists (no possibility of being reranked correctly). Accuracy is only on those fr words for which correct answer is somewhere in lists... probably shouldn't do it that way. Not ranked b/c English not observed in monolingual text?
- Finishing copying over ES wikipedia pages

- Ben thinks I should do a short paper: "Vector Space Models XYZ"
- Ideas for spinning it: (1) mt seed bitext learning curves + bilingual lexicon induction (feature space learned from vw), (2) + LSH?

- Worked on sanjeeval a bit: tried making some changes to write out dictionary from phrase table to see why scores are SO low (obvious bug). 
- Also worked on R script for making stacked bar charts with Sanjeeval output

TO DO:
- update repo paper with, at the very least, sanjeeval outputs on all emea data and emea manually aligned subset
- update tacl paper with: old-domain, old-cat-new, new outputs. explain sanjeeval there a bit

9/10/12:
- Finishing copying over ES wikipedia pages
- Fixed Wikipedia lex induction scoring to score exactly the same words used by the crawls freq bin inductor
- Fixed learning script that incorrectly ranked lots of things 1. ugh. 
- Fixed learning script to look through ALL possible en words, not just those in the crawls context list
- Fixed script to get edit-distance rank based on those scores, not treating 0 as worst anymore
- Should think about using only a sample of the negative examples to make scores more meaningful... write negative examples w/ some probability? -> Implemented this, can play with it.
- Looks like some bad translations in mturk dictionaries; added other dictionary pointer to script so can switch between them
- Added wikipedia frequency counts here: /home/hltcoe/airvine/inducePhraseTable/LIMT/Experiments/LexInductExps/wikiFreqs Can calculate for as many letters as is feasible for a language
- Using as an additional feature
- preliminary results: bg no help, uz some help in top-100 only, az huge help

- Still debugging new Sanjeeval script... utf-8 problem? Writing dictionary derived from phrase table to file. Maybe I shouldn't be doing this from a phrase table though. if a -> b c, then both alignments should be marked as correct, not neither because not read from phrase table. Should consider phrase-internal alignments in the phrase table! yes. that's a lot of computation, but will keep things consistent for S4 analysis. can always write out dictionaries as planned. 
- Ok, using phrase-internal alignments now. Also using FILTERED phrase table, filtered against test set only
- Finally properly debugged. Config should work now, science-unseen on-old and subs-unseen on old run successfully

9/11/12:
- All jobs running to process Spanish Wikipedia on coe /export. When done just get final counts files made and then email Paul.
- Lots of jobs running for new classification using Wikipedia data and frequency info. 
- Upper bounds: accuracy in top-k where is length of concatentation of all ranked lists from crawls (top 500 lists) and wikipedia (right now just top 100 lists).
- Changed Wikipedia lists to top-500... 
- Problem with babel version, projection wasn't working correctly for nbestcollector. Fixed, rerunning all wiki scoring. Looks like even topic scoring was messed up; EN candidates not kept if don't have all properties and had zero context vectors? Probably.
- Results are good again! Wikipedia helps a lot. 

DAMT
- Added two existing figures to paper (workshop report) and explained manual alignments and results (manual vs. automatic and within)

Thesis:
Added lexicon induction text on reranking/vw classification to thesis, some udpated figures. Looks pretty good. Update again in morning before meeting. 
Talk to Chris tomorrow about upper bound in ranked lists (could vary nbest list lengths...) and adding LSH scores... how to spin the best paper on just this lexicon induction stuff. 


9/12/12
TO-DO from CCB meeting:
- Stemmed topic, contextual, etc. scores. *Add stemming to babel code <- prefix done, running experiments to check if helps
- Learning curves: add monolingual data sampling config parameter to babel code <- major do 
- OOV translations: get top-k list from reranker, add to end-to-end MT <- major do for Tamil (will be fastest), Bengali, Hindi
- Bar chart with accuracies across languages <- easy to do
- Try bigger top-k lists for some languages, see if helps translation recall <- easy to do

- Made data section in thesis, including new table on WALS features
- Worked on prefixing: have working in babel code, just use PrefixEquivalenceClass. Prefix size is hardcoded right now; make that a config parameter
- Modified vwrankCW.py script to use features from prefix-based scores
- Ran reranker with prefix-similarity-features included for uz: 

TO-DO:
- revisit training examples balance: make sure ok
- increase top-k number for few languages and check difference

9/13/12:
- Cleaned up vwrankCW.py -> rerankClean.py no more true/false dictionaries, all based on similarity score names. exactly same results as before
- Fixed prefix-similarity-features use, running WIKIPDIA prefix scoring on az, bs, cy, ne, so, sq, uz
- Prefix scoring started for all languages (so far looks like actually hurts performance...maybe b/c lots of added en candidates)
- Emailed DAMT about experiment directories/config files

9/14/12
- Results on using 5-character stems don't look good. Don't change or hurt performance in most cases.
- Increased memory limits on Prefix jobs from 80G to 160G
- Emailed Paul pointers to wikipedia and crawls data on /export
- Set up config file for reranker, so can choose whether or not to use prefix features, point to a subdirectory, and use features but not candidates from prefix scored lists
- Working on learning curve (sampling Wikipedia pages). Hard b/c gathering pages separately for source and target just using regular expressions. Want to apply random filter to one and then use filename list for the other...
- Ok, added corpus sampling to wikitemp corpus type only. Can specify config parameter that is probability any page will end up in sample. Ensured that page titles would correspond across English and Foreign-language
- Added sampling to corpus based as well, also working. Fantastic.


9/17/12:
e.g. run with new rerankClean.py : ./runqsubCW.sh sk noPrefix noPrefix.config (lang, subdirectory, config file)
@shreejit:this rerankClean.py seems important for learning curves
- Learning curve runs set up: python iterateLangSample.py az
- Need to set up same for vw learning, gather accuracies for each point
- Made thesis github repo private (ccb is collaborator)
- Script to run .01, .05, .1, .25, .5, .75, 1.0 samples of crawled and wikipedia data on language: python iterateLangSample.py lang (knows which languages to transliterate and which to not)
- Script to learn from those once all done: python runqsubCWSampleIterate.py lang
@shreejit: seems like some steps here
- Script to postprocess output: python in noPrefixSampling, python postProcess.py lang
@shreejit: and here
- r script ready for learning curves. Probably won't do for all languages, so manually add numbers from lang.summary output from postProcess.py
- Running Spanish and Urdu scoring learning curve jobs
- Cleaned up comments written to standard out from babel code and redid jar file: now latest stamped 091712
- DAMT: running baseline 1-of-32 hansard job, maybe can improve on that for paper...
- Need to reinstall boost, with libboost-all-dev libraries... from svn boost-trunk. Still didn't work :( Need to find that library directly, I guess. Boost 1_49_0 in boosttrunk right now but that doesn't do it. Problem is with binary script extract* Tried and failed: v. 1_50, 1_49, and alopez's. 
- Solution for now: getting phrase table and reordering table on clsp cluster
- Cleaned out 250G of storage in home directory, re: email from Scott Roberts
- Getting wikipedia frequencies for spanish: will probably take a while

9/18/12:
- Got wikipedia training on Hansard-32, tuning and testing on Science running (w/ Hansard-FULL and Science LMs) on clsp cluster. Boost 1.51.0 also didn't work w/o separate install of that library on COE.
- Tuning scores look much lower (~19 so far) than on full Hansard. Maybe have better shot at improving that score? Will still be using that phrase table, appended with monolingually learned stuff, so who knows...
- Made learning curve for Spanish; up to 10% sampling scoring completed
- Updated monolingual data vs. performance graph in thesis
- Talk to Alex: agreed to spend 2 days/week on the MT part of the multitask learning project
- Got Hansard-32 phrase table from CLSP cluster. Updated merge.py to use config file so can remember how supplemented PT was built. Supplementing now, will tune and decode with that. Use config files from August, no need to change anything but pointers to model files (copy over from clsp) and now PT (need to gzip)
- Decoding w/ phrase tables 13 and 14. If work need to see how the same thing works when seed marginal learning with hansard-32 ttable. Right now seeded with full. Either way, should revisit learning (I think I did some fix-ups when writing the final report...)


9/19/12:
- Emailed Josh Langfus from last spring semester about being a TA

From CCB meeting:
- Suffixing scores (might want to distinguish short words for which suffixing doesn't change the token from long words where suffix is the same as an existing short word)
- Type/token ratio from sample of n (e.g. 10k) words from corpus, add to table. How that ratio changes with prefixes of length k=3,4,5,6,7,etc.
- Figure 23: don't sample at frequency, do overall
- Bar charts of accuracies
- Show deltas in Table 7
- ccb coe directory for monolingual wget crawls: webcrawls/monolingual/$lang -> work through setting up daily cron job to mirror list of websites (recursively to some depth)
- average, min, max over all languages delta between w/ and w/o feature (or box and whisker plots); or bin languages by class (language families, or by amount of monolingual data)


From Chris Quirk and Hal Daume meeting:
- eyeball dev output, will manual evaluation be promising; error analysis
- decode with learned weights on old features
- initialize with weights learned from baseline experiment
- indicator features at 1, log(1)=0 worry
- upper bound experiment: supplement phrase table with 1-best translation for all oovs from aligned new domain data

- Found bug in merge.py script: was producing top-k based on p(f|e) instead of p(e|f), which is much more reliable (pigmentla passed through instead of pigment, which has higher p(f|e))... fixed, generating new phrase table
- Also running tuning and decoding on coe cluster on phrase table copied over, exactly the same moses version, etc now: 9.19.nosuppBaseline
- Added config parameter to not use indicator features; no real need to tune those numbers, a word will only be added if oov after all
- Omg, table 18 has a tuning bleu score of 0.207404... this is the pt w/ only (CORRECTED) top 1 appended, along with p(e|f) and p(f|e) but no indicator functions. That's a good bleu point higher than the baseline tuning result. Hope it holds on test set...


9/20/12:
- 1.24 bleu point improvement. Bam. That's including only top-1 translation. Down 3 bleu points when add top 10. Trying top 3 next.
- Oracle experiment: add all translations based on dev/test alignments for dev/test set oov words, along w/ alignment-derived probabilities. B/c phrase table is at word type level, that's the best we could expect to do by adding word type translations to the phrase table
- Also doing a 15-iteration run of learning w/ same parameters as end of summer (except maybe some fixes I made to the jointM.py script) to see if learning curves will level off after more data or not
- Debugged wade analysis for John, his config files not lining up. Otherwise script is ready to go.
- Need to remember: marginal learning initialized w/ joint from full hansard, not hansard-32. May have included some translations that were oov wrt hansard-32 in original joint. Need to rerun marginal learning with only hansard-32 initial joint
- Also running baseline hansard-32 emea experiment, to have number ready. on same phrase table as using for above experiments. Got reordering and phrase tables from clsp, filtered on EMEA data. Moved over to COE: /home/hltcoe/airvine/damt/moses_exp/hansard32-newtestdev-emea
- Should get wikipedia frequencies w/ normalization. Right now some period, etc errors... may help a little
- Talked to Matt about teaching; no go for me
- Made jointcounts file for Hansard-32. New science config science.32joint.config points to that. It also points to Wikipedia word frequencies that are cleaned up (periods stripped from words) and docpair.py now points to .ucounts words for all Wikipedia files that are cleaned up in the same way (periods stripped). Were tokenized before but apparently not very well <- new round of marginal learning running overnight with these fixes
- Added thing to merge.py script and config file to allow printing of % oov word types in dev set that appended a dev-test-set-oracle-translation for in the phrase table
- In merge.py script should strip punctuation and sum over those words... if not gotten rid of in new joint...

9/21/12:
- Meeting with Josh:
    - He's going to work on learning more unix and emacs. I pointed him to a shared directory on clsp grid where I can put things for him. Right now gave him wget command and list of newspapers
    - Meet next Tuesday at 3:15
    - Then, maybe he should write cron job to wget all Azeri pages each day... timestamp directories by date crawled?
- From lab meeting: Don't use 0 probabilities in Moses! It ruins things. I was doing that in all of my phrase table phrase-pair appending experiments. Now running a bunch that have values of 1 for old features on new pairs. Tuning scores look ok (about like baseline) for top 10 and top 3... tuning on experiments also scoring existing phrase pairs don't look so good. Also using real jointout from Hansard-32 now, so it's an honest eval. Running that marginal matching run up to 20 iterations. Could do more; until run out of Wikipedia documents. 
- Also running new oracle experiments, w/ same problem fixed. Adding ALL english translations for a given french word to phrase table, w/ probabilities from aligned data. 
- Actually need to fix dev set type/token counts... base it on the aligned dev set instead of what I'm doing currently, and count tokens and types directly. That will be more accurate. Right now credit is given if en translation for an fr-type for each token instance, which isn't really what we care about <- done, below

9/22/12
- Fixed dev set word token counts: type doesn't make sense, just reporting % of dev set oov tokens, not including oov-freebies, for which we append correct answer.
- At .45 BLEU improvement right now, marginal match still learning

9/23/12:
- Checked up to iteration 28: over 1% increase in dev token coverage, so should continue marginal matching learning...
- Got list of all dev/test tokens and their frequency in the training data: science.devtestoov.wrthan32.withcounts 


9/24/12:
- Iterate over 25k documents more than once: running on second run through of 25k
- Figure out why evaluation isn't the same on second round of learning... -> was problem w/ conditional probability cutoff being set too high b/c hardcoded info on it being first iteration. Copied iteration 29 from previous run as iteration 0 on second run learning through all, starting from iteration 1. Evaluation metrics look ok.
- Oracle stats on dev-set items that are low-frequency in the training data: in output from /home/hltcoe/airvine/damt/phraseTableMerge/oracle/findOracleTransCoverage.py and r chart made for frequencies from 0 to 10. At 0, coverate is 0%, of course, at frequency 1, coverage is 25.4%, at frequency 2, coverage is at 43%. Conclusion: appending translations for words that only appeared once in the training data but not for those that appeared more than once (or at least running that experiment; covers an additional 1144 fr words in dev and test)
- Try adding translations for low-frequency words too, not oov but maybe that appeared once in training data... check dev set: what percent of the time is the top translation for low-frequency words in the training data the correct answer wrt dev set? list of words and training data freq written, just need to modify merge.py to take parameter for max freq to be considered (e.g. 0 (just oovs), 1, 2, 3,...) and print stats about devset answers and phrase table ----> Done. Parameter implemented, devtest file w/ training data frequencies now input specified in config file to merge.py instead of just oov list
- NOTE: what to do about word pairs proposed by both old phrase table and new appended things (e.g. nomades, nomadic): should probably combine probabilities into single entry (old scores from old PT, new 2 scores from learned joint). I guess? Or let them compete? 
- Realized some words weren't in dev/test word files (e.g. nucleoplasma), written to standard out of merge.sh. Reading actualy test1 and dev1 files now to get vocab list. Those standard out files shouldn't list any words, that's the point of the warning! -> experiment 30 is a duplicate of current best #26 except with this fixed
- MM: ahhhhhhhhh realized wasn't using wikipedia similarity scoring the whole last few parallel iterations of marginal matching learning... because of mistake in copying config parameters, just left out those. ugh. think i had never done it for science (at least not on the coe computers, filename was inconsistent w/ emea so would have noticed). rerunning: science-docpair.100.8.30.sept24

DAMT-Analysis:
- Got WADE results for on-old emea, science, and subs. Waiting to hear from John about pointer to news

9/25/12:
What is OOV anyway? 
- A. Long ago was defining it as appearing on LHS in phrase table. But if in test set, we see it in the same context as an entire LHS phrase, then inducing new translations is a bad idea
- B. So what if we've seen it in the phrase table, in any LHS context, or, similarly, what about just if we've seen it in the training data at all
- A has the problem of adding too much stuff to PT in some cases
- B has the problem of not adding enough to PT. Sometimes we'll see a word in a new context and DO want an individual translation for it
---> How you define OOV turns out to matter
- New idea: new parameter M: append translation to PT if have seen word <= M times in training data. Maybe even if we do have a single word translation for a word, if it's based on a single alignment, we want to add new stuff anyway. In my Science experiments if M=0, 492 remaining test set OOVs (seen, but only in particular context with bad alignments). M=1, 151 remaining test set OOVs. For these low M's, can we mine phrase table for word translations. 492-151=341 test set words that we've seen before but still don't include in PT, probably because of bad alignments (that's the case for 'respiratoire'). If we can translate those 341 words correctly, that's substantial. 
----> Or maybe OOVs need to be defined in terms of the FILTERED phrase table
--------> Changed merge.py yet again to include words in list to append if EITHER (or both): 1. oov wrt *filtered* PT or 2. training freq <= max freq
Good examples of NON-cognates!:
CORRECTLY INEXACT ADDED:        soudage welding 3       welding  |||     welding soudage
CORRECTLY INEXACT ADDED:        nuage   cloud   3       cloud frame-invariant    |||     cloud nuage
CORRECTLY INEXACT ADDED:        macareux        puffins 3       atlantic puffins large-scale prey-base   |||     puffins macareux
CORRECTLY INEXACT ADDED:        araignes       spiders 3       spiders  |||     spiders araignees
CORRECTLY INEXACT ADDED:        tordeuse        budworm 2       budworm amount increase canopy openness specific distribution within stands spruce disturbances  |||     budworm tordeuse
CORRECTLY INEXACT ADDED:        poumon  lung    2       lung     |||     lung poumon
CORRECTLY INEXACT ADDED:        perglisol      permafrost      2       manitounuk permafrost    |||     permafrost pergelisol

- Emailed Jags for him to provide baseline
- pt-*-appended lists all phrase pairs added to phrase table
- pt-*-analysis: iterate through all dev set pairs for those frwords which appended translations for, write whether appended *that pair* token or not. So the result is something about recall, denominator is total correct pairs could have gotten. Precision will be correct out of those translations that you added (size of appended file). Note if appending top 1, there's an upper bound on recall at number of tokens corresponding to fr words appended / number of correct pairs. 
- Note that as the set of fr words for which I append phrase table translation changes, so will the denominator on the recall (total correct pairs could have changed)
- Did baseline + strip punctuation experiment)

Meeting with Josh:
- general note: ccb coe directory for monolingual wget crawls: webcrawls/monolingual/$lang -> work through setting up daily cron job to mirror list of websites (recursively to some depth)
- He'll work on two things for next Tuesday:
1. Script to strip out HTML (next: iterate over pages in directories)
2. Python script to run wget commands over list of newspaper articles written in Azerbaijani (next: same type of thing with cron... )

Chris Quirk and Hal Daume meeting:
Prep notes:
- LOTS of error analysis. Interesting points are:
- Oracle experiment: All w/ alignment-based probabilities is worse than top-1 only. Full oracle doesn't beat the limited oracle. Recall/precision tradeoff here (high entropy in what's appended... all positive results, but better w/ only top... alignment errors just adding noise? -> still, point that noise is bad if top-1 is pretty good)
Other experiments:
- Got rid of all indicator features, cleaner and works well
- Adding translations for low training-data frequency items also helps a little (alignment mistakes? maybe we should be mining the PT for additional info...)
- Still doesn't help to score existing entries: giving them uniform probability right now... maybe should score low freq items that I'm adding new, competing translations for. Chris Q says don't worry about too much

Meeting notes:
TO DO:
 ---- 1/rank instead of probability: will look peaky-er (for existing phrase pairs)
 ---- Freebies: indicator on those: 2.718 (e) or 1 (to distinguish from pairs from joint)
 ---- Plots of should be translated identically vs lots of things like frequency, data analysis to decide how should pick those (and maybe for those shouldn't add translations from joint)
 ---- Systematically measure precision and recall on devset? On top-1 translation for fr words appending translations for? On type level basis only? Yes, do this.

9/26/12
Example of sense shift
consolids -> conglomeration or consolidated. Marginal chooses consolidated by far.
cohrentes -> consistent or coherent. consistent higher in initial joint, marginal chooses coherent by far. In Science parallel data: coherent 49 times, consistent 14 times
TOTALLY LEARNS enceinte !!!
 - Another oracle experiment: - Don't add other translations for words which should be translated only directly and don't add exact translations for words which shouldn't be translated exactly vs. my current best output. That shows how much room for improvement by deciding which words to add exact translations for. Running now: frtypeoracle experiment; same as #38 but not adding identity for words only translated other way and not adding other ways for words only translated as identity (see how much gain could be gotten by automatically figuring that out)
--> This doesn't look like it's working super well... maybe only consider top dev/test translation for each fr word instead of all to determine type?

Meeting with CCB:
- Read his 06 (maybe 07?) NAACL paper on translating paraphrases... oracle and manual evaluations might be similar
- Should have Josh meet with Ryan
- Send him what looks good at Grace Hopper

Analysis paper:
- Running jobs on stack size 10, 200, 1000 for on-old news (maybe should also do 100 and 500) w/ decoder flag -stack NUMBER 
- to do after that: same on emea, science, subs
- Then: figure out where john's tuned mixtures are for the combined experiments, do same

9/27/12
- Worked on analysis paper (couple figures), beam size experiments. Moses jobs keep dying with permission denied error, but enough are getting through for Monday's deadline, I think
- Running marginal matching over Science data another time doesn't seem to help intrinsic results *at all*. they're stabalizing a little below where they were at the end of the first read through. 

9/28/12
- From lab meeting: should probably use my babel translator to translate same french domain data (my note: i actually think it will do much better)

10/9/12
- Fixed Figure 23: overall instead of sampled at frequency (monoVperformance.R) at top-{1,10,100}
- Bar charts of accuracies (have top-10 now, easy to add more in monoVperformance.R)
- Made script to get type/token ratios: /home/hltcoe/airvine/inducePhraseTable/LIMT/Experiments/LexInductExps/typeTokenInfo, new table in paper w/ ratios and prefix-ed word types. Need to add deltas and mark languages w/ biggest difference between prefix-5 and general

Notes from Hal/CQuirk meeting:
- Paper draft checked in
- Baseline w/ accent-stripped OOVs: 22.2 
- Tried oracle use-freebie or not, no huge gains (apples to apples experiments are 36 and 9.26.sci.ptoracletypes: 23.25 vs 23.43). Could try to automatically classify this stuff, but I'd be surprised if we got much from that.
- I haven't played with features yet, waiting for learning to stabilize, will be more productive once a baseline number is finalized (and that part is relatively easy)
- Numbers from Jags
Why 2nd epoch doesn't help:
1. gradient is small (why don't need to do second pass)
2. gradient is big (but not in direction that doesn't help)
Compute L1 gradient on each document pair and print on second epoch: see if size of step goes down
ML literature:
 - Steven Boyd *Vanderburg optimization paper - Hal's high level understanding is that basically they have big problem, break into small and force them to agree. Note similarity to legrangian relaxation/dual decomposition. Each doc pair places some constraint on joint, want something that makes all doc pairs happy. Would take forever to use those methods
 - response from Hal's talk: general reaction was 'haven't people done this before?'
 - in ML land 10 years ago, everyone was working on binary problems, so probably not
 - remember copula work (super fringy stuff)
 - include reasonably good related work: make it look like we put some effort into finding that stuff
 - simple formulation, surprised not to find prior work. we present optimization method for this which works well
 - related work on optimization and model side
 - Hal will send bibliography, annotations later but not before Thursday

10/10/12
Meeting with CCB:
 - General stuff
    - Update os on macbook pro
    - textutil (mac-specific) for cleaning html tool
 - Josh web crawls:
    - should only do 1-2 levels of recursion if going to be daily snapshot
    - after clean up, talk to courtney about dedup: 1. will get rid of navigation stuff 2. will get at temporal shift b/c getting rid of everything already seen
- LRMT:
    - better display of TTR information
    - eventually: apply babel methods to domain data
    - increase test set size on all reranking experiments: spanish was too small b/c original by-freq run didn't finish and list copied from there. made learning curve results funny. it's ok for other languages
    - short NAACL paper on reordering? maybe. look at what submitted last spring, possibly clean up: could use classification (multi-way or multiple binary?). could throw in english side POS info
    - NAACL paper on LRMT: ****sketch of paper for next week: appending feature set and phrase pairs for OOVs
    - Compare with Kevin Knight's student's output (email to ccb didn't include english source test text, so can't yet)

Brainstorming:
    - Problem: Spanish byFreq hadn't finished, but used that list of words for learning curve: very small sample size. Rerunning byFreq now.
    - Problem: was only measuring accuracy over list of LANG-source words that scored at all. If a requested word is OOV wrt monolingual data, it wasn't included as a missed item. But we really want percent of attempted words, denominator the same over learning curves. This is particularly true for learning curve stuff, but should probably update everything. One way to do this is to not worry too much about frequency graphs (for now) and just run sampling iteration script only for sample size of 1.0. Can then update bar chart and those text scatter plots
    -> This is fixed, and not a big problem for Azeri (which has tiny crawl data), so probably not a big problem for others as well. Don't worry about rerunning until have updated byFreq results (on new source word list). Only showed up in first few points on learning curve, so don't worry about fixing for other graphs (in fact, sampled to appear 3-10k times, so all WILL always appear in full dataset). -- this is only a learning curve problem
    - Realization: by-freq stuff always only include words that appeared in monolingual text between 10 and 5000 times. I used that word list to calculate overall accuracies, so measure is only over words that appear 10-5k times. Then split into 10 bins and averaged. 

- Added number-of-gigs as third argument to langExpFreq and langExpWiki, so can easily vary that by language. Written to lexinduc.sh file w/i lang directory, so if memory issue can see what it was and increase later.
Rerun: 
1. langExpFreq: Fix: now doing words between 3-10k times in monolingual corpora (webcrawls, gotta do one or the other first...). Also collecting for 8k words instead of 3k. This script scores using crawls.
2. langExpWiki: Same words as langExpFreq, score using Wikipedia
Need to rerun:
3. VW and generate by-Freq graphs
4. Sampling on same word list

Feature Ablation Study:
1. Run experiment using all features can think of: running for all languages now
./runqsubCW.sh az useAll useAll.config
2. When that's done, run script that uses existing files w/ all features extracted, and 
    a. use each feature individually to train: script ready (indivfeatsQsub.sh))
    b. Box-Whisker Plot those results: bwIndividual.py
    c. Write script to greedily add best performing single feature to previously added, best performing over ALL languages (that'll be the hangup)...but can use the old by-Freq results for this I guess? Or should wait? Going ahead and setting this up for old results now... once results better, rerun all (should be easy then...)
       - don't have to re-extract features, can just take from existing files, so should be relatively quick to train and test all models.
       - Algorithm:
           - Choose single best 1 using bwIndividual.py script, add to current feature list in indivfeatsQsub.sh setup
           - While performance goes up:
	       - Try adding each non-used feature to current feature list separately, using indivfeatsQsub.sh setup
	       - Identify single best to add using bwIndividual.py script
	       - Add that feature to current feature list in indivfeatsQsub.sh setup

10/12/12
- Added some writing to bigone about feature exploration
- Fixed rerankClean.py: use candidates from all edit distance calculations, but only use single score: edit distance between a pair of words won't change. Add-alone results running in cleanED.
- Keeping top feature, running with second in useAll. At this point look at it, but take from cleanED instead. bwIndividual.py already updated. Everything ready. Get rid of that annoying '.1" thing in the stdout stderr pointers

Meeting with Josh:
- JoshCrawl:should only do 1-2 levels of recursion if going to be daily snapshot
- Iterate over pages in directory

10/14/12:
- Started EMEA MM learning

10/15/12:
- Fixed feature extraction in rerankClean.py - was using -1 as default if no rank found, messed up minimums. Rerunning all and then alone, *.plus, etc. to cleanED
- Added EMEA intrinsic results, which are coming in, to intrinsic results graphs
- Started merging/moses jobs on science iteration 57: exactly lines in tables in paper that I need
- Get average gradient over iterations here: getGradients.py
    - For EMEA, it's generally decreasing but very irregularly over first 15 iterations
    - Goal was to have this information for iterating over same data again, but don't want to use compute cycles to do that, instead learning over new data still

10/16/12:
- Updates to MM paper: intrinsic results and oracle sections mostly, cherry picked output examples (script for printing old, new, learned in /home/hltcoe/airvine/damt/marginalSolver/dataWade)
- Getting Subs set up for MM learning. Going to cut off all (for now) at 50k document pairs. Looks relatively level at that point for Science. Had to run Chris's triples code (easy) and gte wiki 100k frequencies. Science is ALMOST done. EMEA and Subs running.
- Wrote initial version of NAACL LRMT paper (abstract, intro, previous work, examples of a couple figures to include)

Meeting with Hal and Chris Q:
- My updates:
    - Science numbers all in, including oracles, etc. 
    - Stopping learning at 50k document pairs
    - Should do BLEU learning curve over document pairs?
    - L1 Gradient (abs val sum of updates): have numbers on EMEA. Haven't rerun over same document pairs (not worth compute cycles right now). DOES go down slowly and somewhat erratically,  but overall going down on EMEA.
    - Just need to improve prior work and get results on other two domains (running now)
    - Jags emailed a question about data, so he's working on it
    - Is there anything else on the critical path?

10/17/12:

Meeting with CCB:
 - Get data ready for Malayalum, and Telugu (need to extract Wikipedia pairs)
 - Use CCB's Azeri crawl to do context scoring, see if helps
 - Get Josh to focus on extracting Indian languages data and languages for which we have very small amount of text now. Can at least use for context scores, not a lot of days between now and deadlines for temporal scoring. Maybe recursive a couple levels
 - MT experiments

- SuffixEquivalenceClass
  - tested on Azeri and works fine!
  - Keep pushing through pipeline of sepproj/wiki2/wikipre/wikisuf
  - Then add suffix-based features to alone runs (at least for subset of languages) and try those again.
  - Note that might want to distinguish short words for which suffixing doesn't change the token from long words where suffix is the same as an existing short word... doubtful this is worth it

- MM Paper:
  - Oracle experiments
        - realized there was a bug in the oracle experiments for science (alignment file wasn't for .unseen en/fr files). Fixed that, rerunning science now.
	- emea running
  - Baselines:
        - emea baseline+strip oov: running
        - subs clsp baseline running
            - when done: strip-accents baseline, 2 oracles
	    - actually ready to do this now: data all copied over. just need to produce oracle lists and accent stripped list

- LRMT features experiments:
  - Realized there was a bug in reading the prefix files when tried to add suffix files... updating that, testing on azeri. Prefix looks good (deleted file b/c it's big). Writing suffix file to check now...
  - restarted lots of the lexinduc jobs, tried restarting a tamil job and it finished almost immediately, was hung before. 

10/18/12
- Upgraded pro to mountain lion: not hard. Had to reinstall xcode, git so far
Meeting with Alex, CCB:
 - Email Alex paper
 - Pointer to Matt Post's paper
 - Log in and access their code (svn repo)

- MM Paper:
    - Experiments:
        - Baseline and oracles done for EMEA and Subs
	- Babysitting MM learning for EMEA (at 45) and Subs (at 31). Need 63 iterations of each, then supp MT experiments
    - Going ahead and doing EMEA & Subs runs to see what BLEU is going to look like, on premature iterations 43 and 30, resp.
    - Also going ahead and running a BLEU learning curve on several doc pair iterations (5, 10, 20, 30, 40) for Science. Nervous that all of learning happens at beginning... Intrinsic results show that shomething keeps happening, but my analysis scrips comparing to aligned dev set says not much is changing in what's being appended after Iteration 5

- LRMT features experiments:
  - Realized was using a default edit distance score of 0. Instead made average of two lengths of words. Also was always considering highest score to be best in rank-finder. Added option there (lower edit distance and log freq distance is better).
  - Running all feature extraction (w/ train, test, and blind splits) for 6 languages for which have sepproj, wiki2, wikipre, and wikisuf done.
  - put bigone repo in dropbox account
  - Worked on getting Indian languages baselines set up on CLSP:
      - Learned that Giza doesn't allow sentence pairs with worse than 9:1 word ratio, so cleaned those from corpus: /home/anni/data/gizaclean-indian-parallel-corpora 
      - Also: for using multiple reference translations for tuning: must name files reference0, reference1, etc. I just pointed to reference and it seems to be working (hopefully it's looking at all...). Can run just pointing at reference0 and compare BLEU scores... ugh, except bleu scores are really low. except log file (run4.extract.err) DOES say that references are being loaded, so hopefully it's just a tokenization problem.

10/19/12
 - MM Paper:
     - Realized was pointing to old unigrams-counts directory, which wasn't complete for EMEA and Subs. Ugh. Updated, restarting MM learning for both at Iteration 31. Need to get to 63, so it'll take the weekend.
     - On premature learning iterations: EMEA MM supplements helps a lot (~2.5 BLEU above strip-only baseline) but not at all for Subs. Not surprising, I guess. Strip-only also didn't help subs.

 - LRMT:
     - Slowly slugging through all scoring on up to 8k words for all languages. Slow for some (esp es, ru). Keep babysitting over weekend. Updated feature experiments to use those new results, box and whisker plots much more compact now, interestingly.
     - Right now 13 languages are done w/ scoring, 10 still working. Expect the following to finish w/i next day: ro,sk,sr and the following by end of weekend: bg,id,uk and possibly never:pl,tr,ru,es
     - Met with Matt about tokenization. He's going to tokenize and put on github repo, using penn-treebank and perl v 5.16 (v 5.12 doesn't work properly!)
     - Indian Languages github repo cloned here on clsp: /home/anni/data/indian-parallel-corpora/ and here on coe: /home/hltcoe/airvine/Data/indian-parallel-corpora
     
 - W/ Alex learn:
     - Still need to email pointers to Matt's data and paper, and my draft NAACL paper

10/21/12:
    - Lowercased Matt's EN tok files: cat tok/training.ta-en.en | ~/bin/mosesdecoder/scripts/tokenizer/lowercase.perl > lc/training.ta-en.en
    - Started TA-EN baseline job: need to test on devtest and test sets
    - Pointed Jags to 50k doc pairs
    - Babysat lots of lexinduc jobs...
    - Eval only for sq (didn't finish writing all feature-extraction, but have for 5k words...looks like might have to do the same for lv), az
    - Azeri data from CCB: using this command to get all text from a directory: find . -type f -print0 | xargs -0 cat > ../www.yeniazerbaycan.com.out.txt
        - then sorted and uniqed lines: in su.sh, then cat and uniq and lowercase all: 36,418 lines, 1,626,709 tokens
	- then score contextual (what English? Sample from some random crawl...?)

10/22/12:
 - LRMT:
    - lexinduc socring:
        - id lexinduc: using 3k words from before while new scoring 8k not finished... have all suffixes scored already: /home/hltcoe/airvine/inducePhraseTable/LIMT/Experiments/LexInductExps/id/Sept12/temp
    	- languages done lexinduc scoring: az, bn, bs, cy, fa, hi, id, lv, ne, so, sq, sr, ta, ur, uz. almost done: sk, ro. That'll be 17. bg, uk edit distance scoring taking forever. es, pl, ru, tr taking even longer. Couple learning curves then sample those languages? Or bigger min on EN word freq? 
	- Made lots of changes to rerankClean.py in order to be able to import for use in Indian languages stuff (below). Might have to debug a little next time run as main function
    - Indian languages baselines:
        - Baselines running for bn, ml, te right now. Ready to run: hi, ur <- send results to Matt when finished
	- Translate all words in dev and devtest sets for indian languages:
	    - Done: /home/hltcoe/airvine/inducePhraseTable/LIMT/Experiments/IndianLangsCorpus/monoTranslate.py lang gigswiki gigscrawls
@shreejit:are indian languages treated in a separate pipeline? transliteration?
	- Extract features and use trained model to predict translations, output list of top-k (10) and scores
	    - Done: /home/hltcoe/airvine/inducePhraseTable/LIMT/Experiments/IndianLangsCorpus/featureScore.py lang config
@shreejit:this too
	- At bottom of featureScore.py script, can:
	      - Append new translations to phrase table
	      - Eventually add: also score entire phrase table monolingually and then use it w/ ems
 - MM: 
    - Learning on iteration 52 for both emea and subs. Need to 63 before doing full Moses experiments. Hopefully by tomorrow morning.

10/23/12:
 - MM:
    - Started all EMEA and Subs merging for PT experiments using iterations 56 and 57. Not quite there but wanted results soon. 
    - Running comparison w/ Jags' OOV translations. Using his top-1 for all OOVs, also including accent-stripped version of all OOVs w/ constant score. 

 - LRMT:
    - Emailed Alex paper intro, some notes, and info about indian languages corpus
    - Azeri Crawl data:
          - Using UZ En sample from 2006-2010 b/c only ~3 times as big
             36300   826857  6363809 aztest.az
   	     78363  2803736 15672240 aztest.en
	  - Faking timestamp w/ just today's date. Collecting time (will be useless) and context similarities
	  - Running reranker. Exact same set of src words, same wiki features:
	      - Using old crawl: [0.0529661016949, 0.305084745763, 0.529661016949] top-1, top-10, and top-100 accuracies
	      - Using new crawl: [0.177966101695, 0.341101694915, 0.536016949153] top-1, top-10, and top-100 accuracies


 - Meeting with Hal and Chris Q:
   - To do:
     - Play with my own default values a bit
     - Implement the following baselines:
         - Choose EN word(s) w/ closest edit distance
	 - Choose EN word(s) which have the most similar document pair distribution similarity (from Chris's BM25 weighted vectors and cosine similarity matrices)
	 - Improve Jags baseline: top 5 translations
     - Background
         - Read papers that Chris Q emailed
	 - Read about dual decomposition
     - ADMM - dual decomposition + amm
     - Cite sentences/document 1/4 or 1/5 parallel sentences - orthogonal direction
     - Thinking about alternate doc sources for subs... nothing great comes to mind. Instead, explain: 
         - lexicon vs register (way people talk in subs)
	 - domain vs genre

10/24/12:
 - LRMT: 
   - Realized tokenization on indian languages wikipedias used wrong version of perl: 
       - Reprocessed: ta, hi, ne, bn
       - ur is ok
       - don't have ml and te extracted yet anyway
       - Rerunning lexinduc experiments for ta, hi, ne
   - Tokenization also not right for crawls data:
           1. Make directory here on a01 w/ scripts:
	       /mnt/data/anni/Crawls/Crawls.bn-en.10242012 
	   2. run scripts to copy data based on what's on Crawls spreadsheet
	   3. Run combineSources.py script to combine two sources of data, if there are two. Then can copy that data over to the COE.
  	     - ta, ne done
	     - bn, hi need a03DW back (backup: use /home/hltcoe/airvine/Crawls/Sept242011Backup/Crawls.DeutscheWelle.08252011.tar.gz)
    - Logged into Saarland computers and transfered data, emailed Alex pointers

10/25/12:
 - Responded to Rich Wicentowsky
 - Worked on MM baselines (wiki-distribution and edit-distance based)
 - Finished getting TA data to Alex. Waiting for info on a03, bn/hi
 - Read about dual decomposition and Hal's notes a bit

10/26/12:
 - MM:
   - Made wiki-distribution and edit distance based baselines for emea and subs as well as finished for science. 
   - Need to work on writing w/ notes from meeting!
 - LRMT:
   - Tokenization for bn, hi crawls fixed! Copied to COE directories and Saarland.
 - Talked to Courtney about dedup. She sent Jerboa pointers.
 - 1. will get rid of navigation stuff 2. will get at temporal shift b/c getting rid of everything already seen
 - Drafted SIGCSE BoF: due Monday
 - Met with Chris Q and Hal: notes on paper

10/31/12:
 - Finished first complete draft of MM paper. 9 pages exactly, no remaining to-do notes. Not embarrassing.
 - Talked to alex about where all of the cross-lingual representations will live. To play with BN translations Friday afternoon. See chat log for reference.
 - Break, then read over once more later
 - MM note: 507 of the science test set sentences outputs are exactly the same in two outputs compared for table 6

11/1/12:
 - Babysitting LRMT runs a little bit. Should probably stop Spanish, up minimum number of EN occurrences, and do learning curve.
 - Noticed some bn words in the IndianLanguagesExperiments weren't getting translated, even though on dictionary list and appearing in the wiki data. Not included in src list
 - Modified babel code, added flag for not cleaning up the dictionary (was getting rid of things that included punctuation or numbers and lowercased everything): TestDictCleanup flag
 - Still not finding all words in that list. It looks like it's because of matching with the srcEqs kept in the corpus. Need to revise that part of the code potentially
 - Made MM figure to explain old joint, comp corpora marginals, how to update the joint
 - Submitted TACL paper

11/2/12:
 - Updated macair to Mountain Lion
 - Wrote script to classify translations based on only non-aggregate features (no min rank, no MRR). Compare performance to overall. Hopefully don't need those... looks like that's true! Will get better picture of different signals this way for sure

11/5/12:
 - Trying to figure out why some bn words being left out of translation list (about 500 of 6700 right now)
    - Problem was with garbage filter in the src eq classes. Added boolean to config that allows you to turn that off (default is to do 'garbage' collection). Really, tokenization should take care of these issues. If something isn't tokenized, it's a problem with the tokenizer and shouldn't be considered garbage. Right?
    - That's taking way too long. New soluation: don't do that. Do another round of tokenization before trying to translate, then untokenize again before translating
    - Still need to finish dealing with this

 - Got TE and ML wikipedia data: on a02 running download.pl /mnt/data/wiki/tools to get dumps
    - Then directions for getting pages and pagepairs here: /mnt/data/anni/wikitools/wikiextractor/tools
    - Done and processed: on coe /export with others

 - LRMT Probabilistic projection: 
    - Wrote ProbabilisticDictionary class that maps src words to trg word-prob hashmaps and projects w/ some probability 
    - Also had to write FungS1ScorerProb and ProbDictScorer, which actually are exactly the same as their originals (w/o 'Prob')
    - Made test ta dict w/ all 1.0s. Result should be EXACTLY THE SAME as previously. Running both now.
    - Along the way, corrected regular dictionary reader to split on tabs instead of spaces...
    - What's running now in ta/wikitest prints size of dictionary as key-value pairs. Changed to just print size of src list
    - Probabilistic dictionaries from giza-intersection alignments on Indian languages training data:
         on mmci: /home/airvine/Data/ProbDicts

 - Play with Alex's induced word representations and ranker
    - Aligning dev and devtest data to get oracle ranked translations for OOV words (for evaluation):
	/home/anni/LRMT/IndianLangsBaselines/bnEval
	- Cleaned up bad lines for giza as did for training only data before
	- coredump problem was because of blank lines! Got rid of them and runs fine
    - Script for getting topk outputs ranked by distributed representations is on mmci dodo here: 
       /home/aklement/Projects/Java/DistributedRepresentations/scripts/neighbors

11/6/12:
 - Dev and devtest alignment done in xEval directories for hi and bn, on clsp.
 - Dictionaries with counts from dev set alignments:
     - Two scripts: one for getting from intersection alignments, one for getting from grow-diag-final alignments
     - Just need to have run moses jobs like bnEval and hiEval before
     - Scripts and output files are here:
         /home/anni/LRMT/IndianLangsBaselines/getAlignmentBasedDictionaries
 - Evaluate distributed-reps output from Alex's ranker based on any alignments-based counts of src-trg pairs:
     - /home/hltcoe/airvine/distribRep/evalAlignmentsDicts/eval
@shreejit:dunno what this is
     - On that bn test set, about 17% of oov words in dev/devtest sets have correct translations in the top-100 (14% in top-1)
     - Note: only want to evaluate on dev oovs. Get that list, use instead of dev/devtest list that you already have. DONE.
     - Compared my bn translations for dev set w/ distribreps - higher in top-100, many, many fewer in top-1. Good I guess! Preliminary results here: 
        - /home/hltcoe/airvine/distribRep/evalAlignmentsDicts/eval

11/12/12:
 - Got output from distrib reps 80
 - Started some MT experiments w/ OOV output

11/13/12:
 - Realized entire vocabulary was in eval set for the distrib reps stuff, not just oovs... oops. MT results were crap.
 - Code up phrase table scoring:
    - Get svn repo: svn co http://it-svn.coli.uni-saarland.de/svn/it-svn/DistributedRepresentations --username annirvine
    - Added to class Proximity.java and ScorePhraseTable.java, which will score phrase pairs - averaging over aligned words (ignoring null aligned words; is that what we want?)
    - Done in eclipse, scored PT on my laptop pro
    - Copied to my copy of svn repo on coe, but haven't added to repo yet
    - Doing run of the same on COE now
    - For some reason didn't score last ~200 lines of PT
    - Running end-to-end MT experiment anyway, with just extra feature (not oov translations)
@shreejit:reference to end to end

11/14/12:
 - Met with Alex:
   - Talked about penalty for different phrase lengths: implemented
   - Started MT job for Bengali w/ that update: results no different
   - Started max phrase length = 3 moses baseline jobs for bn, ta, hi
   - Checked in PhraseTableScorer and udpated Proximity.java into svn rep
 - LRMT:
   - Fixed bug in probabilistic dictionary: problem was that was calling 'construct' before dictionary read in ProbabilisticDictionary class
   - Running ta job w/ probabilities 1.0 now, make sure results are the same as before. Then use real prob. dictionary based on alignments.
   - Wrote monoTranslateProbDict.py to run w/ probabilistic dictionaries. Running for Tamil. To-do: run w/ same dictionaries, all 1.0 instead of probs, and compare

11/15/12:
 - LRMT:
   - Updated paper: thorough updates, including new by-frequency graphs
   - Script to generate by frequency info based on blind data sets and frequencies in crawls (wrote files for all langs in /export):
       /home/hltcoe/airvine/inducePhraseTable/LIMT/Experiments/LexInductExps/rankCombine/useModelScoreBlind.py
@shreejit:frequency run
       Uses existing model (I pointed to the one with the features found to be best in noagg/) to score blind data, then gets top-k accuracies based on dictionaries, 
       Averages frequency for each bin of 50 words (some languages have more bins than others: some with very small crawls didn't have very many words with freq>3 and that also existed in the dictionaries)
       Just copying and pasting output from above script into R scripts
       Will need to update pointer to model and re-copy results if want to use additional features
       Updated numbers in Table 7 and in monoVperformance.R. Just need: sk (doing feat extraction), bg (doing feat extraction), pl (wiki scoring), es (wiki scoring), ru (wiki scoring, also need crawls word counts), tr (doing feat extraction w/o prefix/suffix features)
 - Probabilistic dictionary projection:
   - Damnit, still not working. Must be another bug, not getting any src contexts projected. Keep trying...
 - Started pl, es, ru wiki jobs and restarted tr suffix job. Good news is that don't have to do prefix/suffix jobs, can go ahead and get features without and get results for section 4.2.3
 - Started TR feature extraction w/o prefix and suffix features since they're not in the top 6, then can train and do evaluate using the individual features script. just used this config intead: useAllNewEvalNoPS.config
 - DR:
     - Started MT job w/ average L2 distance (averaged over dimensionality of weight vectors, which is 80). Used 1-distance as before

11/16/12:
 - DR:
     - Casing problem fixed: looking for lowercase version of what's in phrase table in phrase table scorer now
     - Both Hindi and Bengali experiments with that fixed and using average L2 worse than baseline :(
     - Getting Chinese monolingual (wikipedia) data. Extracting pages now (/mnt/data/wiki/pages/zh/). When done get page pairs (see the wikitools in /mnt/data/anni for pairs), transfer to COE, and get concatenated files to Alex.
 - LRMT:
     - Wrote word frequencies for wikipedia pages in:
        /export/common/data/corpora/comparable/wikipedia/pagepairs/en-uz/normlettersWordCounts/allwordcounts.{en,lang}
@shreejit:maybe export has most of the data?
	Can use these for by-frequency figures in paper. Always bugged me that just had crawls frequencies there... might make smoother
	Updated useModelScoreBlind.py to output based on both frequencies and updated all by-frequency figures
     - Four jobs running for TA oovs: wiki and crawls doing contextual projection with 1. training data derived dictionary and 2. probabilistic alignments-based training data derived dictionary
     - Then: Rerank w/ built model, and evaluate performance on dev set oovs (using script developed for DR)
     - Then: Then use for end-to-end MT
     - Probabilistic dictionary:
        More debugging: there was a problem with using longs instead of doubles. That's why experiment w/ all probabilities of 1 worked but others didn't. Fixed now.
	Even more debugging: works in test case (w/ random double probabilities), but no context scores w/ crawled data.
	Hypothesis: was pruning if context appeared less than five times. Changed this to -1... can up to to 1 or something again if need be, but sum of prob. projections will be small -> nope, didn't do it. Also tried multiplying probabilities by 100, also didn't do it.
  	Ugh. I have no idea what else the difference between wikitest/ and crawsprob/ could be, other than differing data sizes... hopefully it's something about the crawls data just being so small and wikipedia will produce something. Though the non-probabilistically projected dictionary on crawls data did produce something...
	I added a second call to the constructor for the ProbabilisticDictionary, and that seems to have done it, for some wild reason... 
	There's one print statement left in there now, got rid of it, but need to re-jar and send it. Don't want to interrupt now...
	Fixed! For real!

11/17/12:
 - DR:
     - Extracting page pairs for ZH. Then transfer to COE and get concatenated files to Alex
 - LRMT:
     - 3/6 tamil jobs done. When all three wiki-crawls pairs are done, run feature reranker and then evaluate on dev set words using DR script!

11/19/12:
 - DR:
     - Chinese Wikipedia extracted, EN side processed, sent to Alex.
     - Chatted with Alex: going to induce old-style contextual rankings based on dictionaries derived the same way that the interaction matrix is, compare rankings of the two
     - Suggested to Alex that he use GDF alignments instead of intersection
     - Jobs started for hi, bn, ta here, using gdf and intersection alignments based dictionaries, both:
          /home/hltcoe/airvine/distribRep/originalCosComparison
     - Those jobs all done, comparison between original contextual similarity and distrips made in spreadsheet. Not looking great for either...
 - LRMT:
     - Looks like probabilistic TA dictionary didn't help, hurt minorly. Do for BN and HI as well.
     - Results saved for each Indian language here: dictionaryResults <- that was using ALL features, not just context features
     - run featuresScoreContextOnly.sh w/ 5 different config files for each language, then eval using script in distribReps    
     - Also started jobs using grow-diag-final alignments on training data, w/ and w/o probabilities
     - Would like to re-do Spanish learning curve: confirmed that ready to go. When finish extracting features for blind, will have the blind test set written, so then run babel jobs. Then this is ready to go: runqsubCWSampleIterate.py

11/25/12:
 - DR/LRMT:
     - Scoring hi, bn, and ta filtered phrase tables with both wikipedia and crawls. All running. Tamil done already.
     - Script to convert new style PT to old-style (for use w/ babel scoring code): /home/hltcoe/airvine/inducePhraseTable/LIMT/Experiments/IndianLangsCorpus/convertPTNewToOld.py
     - Script to read in two new scored files (bpl-wiki and bpl-crawls) and write new-format PT features of your choosing (Remember to rename corresponding output file!): /home/hltcoe/airvine/inducePhraseTable/LIMT/Experiments/IndianLangsCorpus/convertPTOldToNew.py
@shreejit:some conversion here
     - Running TA MT job w/ 2 classically scored lexical contextual features. When bn and hi scoring done, do the same. Compare w/ DR outputs
 - LRMT:
     - Spanish learning curve: extracting features for up to sample 0.25

11/26/12:
 - DR:
   - Updated lexical induction DR eval script to output file with all src words of interest, the highest ranked correct English translation (if found), rank, bilingual frequency, and monolingual frequency: /home/hltcoe/airvine/distribRep/evalAlignmentsDicts/eval/evalout.py
   - Wrote script to do frequency binning based accuracy analysis over the output file from above: /home/hltcoe/airvine/distribRep/evalAlignmentsDicts/eval/freqOutputs.py
   - Lexicon induction performance comparison with Wikipedia Topic feature (which seems to be doing the best given end-to-end MT results): saved here /home/hltcoe/airvine/distribRep/evalAlignmentsDicts/eval/outputs
      - top 100 accuracy is MUCH better. top 1 accuracy is moderately better
      - what matters for scoring the phrase table? top-100 accuracy probably matters. top-1 matters more  for translating oovs
   - Tried evaluating using all dictionaries that I have access to: performance goes up, but that doesn't change fact that increase not seen in BLEU scores
   - Started Spanish phrase-based Moses jobs on 1/100 and 1/50 of europarl data. Score that PT, see what happens... there's more monolingual data, artificially same size training data as Bengali. Added benefit of large and clean dev and devtest sets


 - LRMT:
   - Found bug in alignments-derived dictionaries (only printed one translation of each word, not all w/ probabilities! major bug!). 
   - Fixed, rerunning ALL jobs: GDF and intersect, w and w/o probabilities, w/ and w/o original dictionary appended. 8 variations + 1 job w/ just original dictionary. =9 for each language. crawls and wikipedia done separately, so =18 total for each language, and 54 total
   - Ran out of space on CLSP home directory. Moved IndianLangsBaselines/ all to /mnt/data/anni and cleaned up a few other things
   - Spanish learning curve still looks funny. Only scored blind test set, and script that I had set up splits it into train/test/blind again and re-trains and only scores test. But still, exactly the same pattern as before where it dips at 25%... Need to update script to just use existing model to score blind set, which is all data, but haven't set that up yet <- not urgent
   - Also started Tamil job using ALL crawls and wikipedia features
   - Finish Polish lexinduc experiment. Russian is the only outstanding one now, and it's running
   - Greedy feature search for indian languages MT experiments: changed 'python convertPTOldToNew.py hi' to write 9 phrase tables, each with one of the 9 features. Run tuning and decoding using each phrase table here: /home/hltcoe/airvine/distribRep/mtExps/tagreedy , choose best according to devtest set, then write 8 phrase tables and repeat until devtest score doesn't go up. Then use that feature space to decode *test* set
   - Small hangup is that can't run more than 4 tuning jobs at once. Use qsub dependencies. Only running for three languages, so not TOO terrible. 
   - Projection dictionaries comparisons: running a few for bn, looks like it's going ok


11/27/12:
 - LRMT:
    - Filled in table w/ comparison of projection dictionaries in big one. Conclusions: intersection alignments better for top-1, gdf alignments better for recall. Better to use both dictionaries not just one, and probabilistic doesn't help except for top-1 gdf for bn and hi (ta probabilities too unreliable?)
    - Greedy search for best MT features. Second round of TA ready to go: 
        - /home/hltcoe/airvine/distribRep/mtExps/tagreedy/tagreedy.wpc-x
	- Just need to update train-model.perl to have 2 extra features, and add qsub dependencies
	- For other languages just need to modify convertPTOldToNew.py to reflect best feature for that language and run to get 8 for second round

    - SCORE OOVs + LOW FREQ WORDS W NEW BEST PROJECTION DICTIONARY: 
       - when OriginalDict jobs are done (scoring topic, temporal, and edit distance)
       - use featuresScore.py, useAllNew.config, and just COPY w-topic, w-ed, c-temp, c-ed to directories with best dictionary projections. Then point useAllNew.config to those directories
       right now (using only original dictionaries), it looks like, for example for bn, getting about 7% of 1842 dev est oovs correct in top-1... probably not enough, but worth a try

 - DR:
    - Scored Spanish 1/100th table w/ distrib reps epoch 48. Running that end-to-end MT job. 
    - Confirming baseline Spanish number w/ COE run (might be lower than CLSP b/c of Moses version)
    - Got top-100 translations for all dev set es words, based on DR epoch 48, evaluate against existing dictionaries for now. Eventually: get dictionaries from aligned training data and aligned dev data. For now results are on word aligned training data dictionary (1/100th) and all existing spanish dictionaries that we have: /home/hltcoe/airvine/distribRep/evalAlignmentsDicts/eval/es.devset.epoch48.ranks
       - frequencies from parallel and monolingual data that alex had pointed at on mmci
    - Also going ahead and doing Moses run of full Europarl data on clsp grid, to have the word aligned full training set for evaluation anyway

11/28/12:
  - Meeting with Alex, Chris
     1. Add difference between 40 dimensions in low dimensional space directly to phrase table, tune with MIRA
     2. score same spanish phrase table with contextual feature using exactly alex's monolingual data
     3. Alex is going to start inducing representations for phrases. Use that data to score PT after weekend
 - Meeting with Chris,
     1. Careful with which OOVs you translate (nothing that even includes a 0-1a-z).
     2. Induce translations for all words, threshold by bilingual frequency later. In reranker, include features for "did I appear in bitext", "was I aligned to this word", "was I intersection-aligned to this word"
     3. Write 2 papers: 
         A .Reranking features for bilingual lexicon induction
	     - W/ box and whisker plots
	     - Current lexicon induction results
	     - Compare with other ways to combine signals
	 B. Low resource MT on indian languages: score phrase table and induce translations as described in A.
	     - Use monolingual features to rerank low frequency words, add them to phrase table
	     - Also score entire phrase table monolingually
	     - Include reranker score for those that were added (maybe? don't have to)
	     - Here could see limit of rescoring existing phrase table by using oracle (BLEU-best) translation in nbest list... assume no search errors. That will get rid of modeling errors. Then can decide how much to blame on features and how much to blame on phrase table itself. 

  - DR: 
      - Wrote class called ScorePhraseTableLowDimFeatsDiffs that scores a phrase table w/ absolute value of difference at each dimension, averaged over aligned word pairs, times penalty 1 / percent of aligned src and trg words   
      - Scored Spanish PT based on 80-dimensional model. Emailed Alex details, waiting for confirmation
      - Got job ready to tune on 10x as much data. Will run after the above Spanish job finishes

  - LRMT:
      - Tried using Jon Clark's multeval package to get nbest oracle BLEU scoring. He says he's going to release 0.5 soon which will include Moses BLEU, not just his. 
        - That stuff is in: /home/hltcoe/airvine/distribRep/mtExps/jClark
	- Tomorrow: update to current github version and try again
      - For each Indian language, created wordstodecode file that has:
         langword \t freq-in-dev/devtest/test \t training-data-freq
	 wordstodecode.info has type and token oov rate
	 Some of those words hadn't been transliterated. Reran Joshua transliteration pipeline for all -> in mt_translitlists3
	    Note that this is still pretty easy to do with this: /home/hltcoe/airvine/Joshua/Experiments/translit/postDataMT.py
	    - Decided to transliterate not only non-roman strings, because some are a combination. 
	    - Reran for all, postprocessed, in mt_translitlists3 now
      -  Now running crawls and wikipedia scoring for ALL dev, devtest, and dev words for hi, bn, and ta
         - Realized why some words aren't included in scoring at all: if they only appear a few number of times in monolingual data (say, once) and appear in a context where none of the words translate, then it has a zero feature vector and can't be scored contextually, so it's not allowed to be scored by any of the metrics. Could fix this so that topic and temporal can still be scored, but this is probably ok. Just use transliteration in these cases.
	 - Updated featuresScore.py and useAllNew.config to:
	     - use only best 6 features from greedy search
	     - append translations for src words with training data frequency<=config-parameter OR that don't appear as src phrase in phrase table
	     - use transliteration if no reranked translation is available
	     - write output phrase table
	   -> giving this a try with bn OriginalDicts output right now... 
	      - running mt job w/ only 2 new features (rerank score and transliteration indicator) and lots of new translations.
	      - running mt job b w/ same translations added, just changed default feature values on old phrase pairs for new features to mirror french domain experiments
	      - found bug in reading training data frequencies. fixed. rerunning second mt job


11/29/12:
 - Scoring dev, devtest, and test phrase tables monolingually with wikipedia and crawls here: /home/hltcoe/airvine/inducePhraseTable/LIMT/Experiments/IndianLangsCorpus/$lang$-en/scorePTddtt
@shreejit:some executable here
 - If add more than top-1 translation for OOV and low frequency words, may need a much stronger EN language model
   - Combined scored phrase table (in scorePT or scorePTddtt) with word translations: no need to rescore tables, already have all scores for both phrases and word pairs. Just read in all and combine.
 - Lots of updates to featureScore.py -> now combine one of the score phrase tables with new translations from reranker and with all original monolingual scores, if none from there then transliteration, added option for only using english words that appear x times (seems to make it higher precision), add optional additional features only on new pairs...
 - Wrote first page of two NAACL papers: not sure if they should be one or not
 
11/30/12:
 - LRMT:
   - Option in config file to only use English candidates >= some minimum frequency threshold and to only translate foreign words with some minumum frequency threshold (sort of like a confidence thing... we can only expect to have translated words correctly if we observed them x number of times in monolingual corpora)
   - Started jobs not pruning source candidates but pruning English candidates to not include those with freq<=3. Looked like there was a lot of garbage in English translations, effort to reduce that, scoring all 3 langs w/ crawls and wiki again. Then should run featureScore WITH reranking, new rerank directory 
   - Option in config file for including transliterations as backoff or not
   - Pruning low-frequency target word candidates:
     - As of Friday afternoon, wiki scoring is still running for ALL of them. Have been using the original dictionary only scores from projection dictionary experiments...
     - BestScoreAllPruneFewer10 -> prunes if occurs fewer than 10 times <- note that reranker trained on these candidates
     - BestScoreAllPruneFixed -> prunes if occurs fewer than 3 times
     - BestScoreAll -> doesn't prune target candidates at all
   - Lots of new options added in useAllNewTA.config
     - use usererankerfeat as additional feature or not (if not, no default feature on old phrase pairs)
     - addedfeatures are on the new phrase pairs where the old phrase pairs are also scored monolingually
     - extrafeatures are only on new phrase pairs, and default value given to old phrase pairs


12/3/12:
 - LRMT:
   - Realized on Friday was using untokenized dev and devtest sets, and that's why performance was so low on using OOV translations. Ugh. Fixed, ran lots of HI and TA jobs over the weekend
   - Have pretty good results now! Yay! Lots of options in useAllNew config files 
   - Trying to segregate coverage from accuracy fixes
 - DR:
   - tried difference-based features on 80-dimensional vector. rerank top-10000 for each word   
   - got alignments from full set, only extracted phrase pairs and features from 1/100. running baseline and baseline + 80 features. tuning now. also tuning baseline w/ that grammar.
 - Wrote papers

12/4/12:
 - LRMT:
   - Ran experiments for Hindi
   - Running experiments for Bengali
 - DR:
   - Chat with Alex:
      - NAACL short paper on bilingual lexicon induction
      - Compare old definition of contextual similarity w/ new distributed representations. First use just Euclidean distance between representations and then learn a distance metric that gives (hopefully better) alternate reranking of English candidates for source words. Train svm light, features will be 80 differences between each element in representation. Try both magnitude and signed distance?
      - I started repository, got table that Alex wanted to see of results so far on three Indian languages
   - Getting top-5000 ranked translations now. Idea is to rerank those? Or get complete list and rerank? Complete I guess, whatever the size of the English vocabulary is. Problem is that classifier now has to classify src-words-of-interest*all-target-words. That's ok. Write class in DR code that will output svm light files w/ features. Output for given list of src words, all pairwise. Then write python script to divide into dev set words and words for training. Or divide devset into two random parts and rerun over evaluations. That's easy too... 
   - Right now checking performance in top-5000 anyway
   - Then Alex mentioned metric learning. Not dealing with that today until we talk about it tomorrow...
   - Need to add baseline of just what's given to the interaction matrix as supervision


12/5/12:

 - DAMT:
   - Both TACL papers rejected, mine harshly. :(

 - LRMT:
   - Realized difference between moses runs on clsp grid and moses runs on coe grid might be because of differences in reordering table. Before I'd cat'd the two filtered reordering tables to use in the confirm directories, but when I use the full one and filter it, it's still difference from the confirm directory tuning filtered table. If this matches, then use this reordering table always and rerun all experiments. Since know what to focus on now shouldn't take too long
   - Got Jon Clark's multeval working: problem was that was splitting on single space, sequences of spaces treated as sequence of words instead of single space. But problem w/ oracle scores is that they're derived from nbest lists w/o much diversity, which I realized might be due to the reordering tables...
   - Looks like that problem with the reordering table MIGHT also take care of the problem I had w/ all-monolingual-features not being better than only some... would be nice to keep it clean and simple and not select subset of features.
@shreejit:some sort of sequence below on how to run a language
   - Ooooh realized that original grammar extraction was done on NOT clean datasets. Crap. Rerunning original stuff on clsp now. Queue is busy. Ugh. Probably why BN is weird. Maybe tokenization affected it the most. In any case, get those done, then copy to COE and then need to get old version of PTs from new version. And then rescore phrase tables. And then run script to get back in new format and get subsets. And THEN rerun jobs. At least figuring this out now and not later... fortunately phrase table scoring only took about an hour last time for Hindi. That part will be quick, then can start all tuning jobs. And hopefully will still see improvements...
   - OMG ran out of space on CLSP login AGAIN. Moving Spanish baselines and damt-marginal solver stuff to a02, a01, a04. /mnt/data/anni
   - New phrase tables are made, scoring is running in scorePTddtt directories on filtered for dev/devtest/test sets.
   - Note test filter is actually still based on pre-tokenization, if want to evaluate on that, will have to re-do


 - DR:
   - Meeting with Alex, Ivan, Chris
   - Ivan pointed out should use src side word or phrase frequency for binning on phrase table features that we talked about last time, b/c the point is to discriminate between english phrases.. that makes sense...
   - Told Alex no time for metric learning business, code is in matlab. Going to try simple classifier instead for now. Not going to be a super exciting paper. Should also add Spanish and French or German.

12/6/12:
 - Rerunning tuning jobs w/ clean tables: 
    - I'd added all of these hacky options like frequency thresholds and such, but turns out they don't matter (I also pruned English candidates to appear at least 10 times in last babel scoring, so that's redundant to some extent). So great. Keep it simple.
    - Rerunning tons of tuning jobs. Hoping to not have to use subsets of features...
    - Once have some settings fixed, rerun +coverage only experiments
    - Switched to using scorePTddtt. Though note that phrase table was filtered on clean dev and devtest but NOT clean test set, if ever want to use that.
    - Also re-running confirm-moses job w/ new tables on coe cluster
    - Started coverage oracle experiment using GDF alignments. Could try intersection if they don't seem high enough, but coverage may be lower.

12/7/12:
    - Just worked on papers...

12/8/12:
 - DR:
    - Linear classifier for ranking
      - Use devtest/test words that don't appear in dev as training data.
      - Gold answers for training data: from existing bilingual dictionaries for now, because they're super clean. Maybe really should use data from interaction matrix to keep paper writing clean? For now just see if this is going to work...
      - Results worse than weighing everything at 1.0
12/10/12:
 - Three papers submitted to NAACL

12/12/12:
 - Meeting with CCB:
   - Finalize grant report by the time I leave on 22nd. Draft to Chris early next week, he'll look over, one more iteration
   - Add two future work sections:
     - 1. Reordering models/grammars - some POS tagging on unknown words and phrases - morphological and/or language model based features to do prediction
     - 2. Phrase translations: evaluation, challenges, preliminary work. which phrases? data sparsity? 
   - Make it clear that realistic model for low resource MT involves a small amount of bitext (shout out to crowdsourcing)
   - Rerun no-resource (only dictionaries) translations for all languages on those Wikipedia pages, including OOV words mining and translations w/ updated pipeline like for Indian languages
   - Maybe add stuff about work with Alex and Ivan
 
 - OT Paper feedback taxonomy:
      - Suggestions type 1 (1 day or less):
         - Can address by writing w/o looking at old data
      - Suggestions type 2 (1 day):
         - Can address by writing + reference lookups
	 - e.g. How are my alignment constraints different from Costa's? Segregate ideas of GLA (perceptron-like learning, w/ plasticity) vs. SOT (add variation before make prediction)
      - Suggestions type 3 (~2-3 days):
         - Revisit data and existing experimental results (significance testing)
             - Detailed analysis of examples where Perceptron HG gets something right and OT GLA gets something wrong (all b/c of ganging up effects?) 
             - Significance testing based on differences between left and right constraints
      - Suggestions type 4 (~1 week at least):
         - New types of experiments:
             - Different ideas about how to use harmonies to produce distribution over word orders
             - Description length of maxent learner
      - Suggestions type 5 (dependent on 4, multiple weeks):
         - Big stroke rewriting suggestions: primary focus on KL
      - Adiditonally should:
         - Update literature from past 1.5 years - couple hours

12/13/12:
 - LRMT:
    - Rerunning Wikipedia page translations for all languages
    - For all translit languages, getting new translit lists for ALL words in 5 Wikipedia pages
    - Working here:
        /home/hltcoe/airvine/inducePhraseTable/LIMT/Experiments/TranslateOOVsReranked
@shreejit:ranking of translated and transliterated languages here. Also a pipeline on how to clean up scored tables
	- Now, translit languages getting those lists: only ru and bg left
	- Non-translit lists, scoring w/ crawls and wikipedia
	- Once scored, use existing models to rerank candidates for all words w/ rerankTrans.py  (in config file dolearn=true), write phrase table based on those translations, and score it w/ crawls and wikipedia 
@shreejit:can't find this file because inducePhraseTrable is missing

	- Then: clean up phrase table (get rid of "NOT FOUND" and NaNs) and combine two scored tables
	- Then: use weights from something else to decode pages as before: modify moses pipeline to point to some set of weights and use given phrase and reordering tables, and just give srcpages as test set)

12/18/12:
 - LRMT:
   - Almost done translating Wikipedia pages for all languages. Just used feature weights from Bengali tuning on its dev set w/ same features
      - got Moses config file to point to that weight file, skip training and tuning and evaluation on test set (no reference)
      - Only languages not done: tr, pl, ru, es (still getting word translations from wikipedia), fa (still scoring phrase table using crawls)
   - Grant report in pretty good shape: all content is there, just probably need to move stuff around and make a little more cohesive
   - Azeri dataset: 
      - Running commands saved here /home/hltcoe/airvine/Crawls/CCB/txt/commands
@shreejit:Azeri crawl commands here
      - Had some segmentation fault errors in the sort uniq command. Halved the data for one of the sites a bunch until seemed like wouldn't lose much by skipping a chunk of it
      - Now concatenating and sorting and uniquing all pages together. Need to preprocess as all other data. Then copy to that aztest directory one level up. Then need to do contextual scoring on it and use that score instead of old and see how much performance goes up on blind set. 
@shreejit:what's a blind set


12/19/12:
 - LRMT:
   - Grant final report:
     1. Hindi table: 1. original, 2. w/ dictionary glosses, 3. transliteration only, 4. from table 11, 5. translit only of table 11  6. using small bitext alone 7. using small bitext w/ mono stuff
        - Using small bitext:
	   6. Using that model only - running w/ retuning b/c using big LM
	   7. Using that model plus monolingual scoring, transliterations, induced translations:
	       Monolingually scored UNFILTERD table here: /home/hltcoe/airvine/inducePhraseTable/LIMT/Experiments/IndianLangsCorpus/hi-en/scorePTunfiltered/output/phrase-table-all
@shreejit:hindi scoring here
	       Combined the mono-scored bilingual table and the mono-scored mono tables here: convertPTOldToNewWSmallBitextTable.py
	       Combined the reordering tables here: combineBiMonoReorderTables.py
	       Rerunning tuning w/ new language model here: /home/hltcoe/airvine/inducePhraseTable/LIMT/Experiments/TranslateOOVsReranked/hi/translate.wsmallbitextmodel.plusmono
     2. Colors for Table 11 <- Mostly done. Polish, Urdu, Spanish, and Farsi still running.
     3. Formal definitions of monolingual signals from paper <- done
     4. Slides as figures whenever you can use them
     *5. Azeri output:
        - Scoring contextual similarity here: /home/hltcoe/airvine/inducePhraseTable/LIMT/Experiments/LexInductExps/az/crawlsnew2
	- Will then need to retrain classifier and evaluate on same blind set
	- Ugh, OOM and waiting for job to start again...

1/22/13:
- Previous two weeks: fixed OT paper, sent to Mark for final read-through. Ready to submit after his OK.
- DRMT:
  - "Continuous Space Translation Models with Neural Networks", NAACL 2012 paper
    - n-gram translation model
    - Uses's Bengio's LM idea, learn a NN for each language separately and tie w/ a hidden layer
  - Frequency binned phrase table scoring:
    - Remember that Ivan pointed out should use src side word or phrase frequency for binning on phrase table features that we talked about last time, b/c the point is to discriminate between english phrases.. that makes sense...
    - New class: ScorePhraseTableFreqBin for adding DR feature based on SRC frequencies
    - Running experiments on two different epochs for Hindi and Bengali and for final epoch on Tamil. Also double checked phrase tables and running on correct tables. 

1/23/13:
- DAMT: 
    - Made game plan for fixing up TACL submission for ACL re-submission! 
    - Running Science experiments w/ no accent strip-boosting, using cond prob cutoffs at every iteration
    - *Run ablation experiments
- Meeting with Chris
    - Working with Matt this summer sounds good, but he's not on SCALE
    - Talk to David about who best outside person would be (Ideas: Kevin, Chris Q, Hal, Rich Wicentowski)

1/24/13:
- DAMT
  - MM
      - I thought that using conditional probability cutoffs would make learning faster (it should make it faster to read/write the joint distribution anyway), but that's not the case and it hurts perormance. So, no more cutoffs
      - Not doing that accent stripping boosting doesn't hurt performance too much, so getting rid of it
      - Running no-boosting, min diff w/ last iteration run right now. That should be the all-penalties-in run for the future. Then do ablation (or use each penalty alone... that might be better) experiments
      - Also running no-boosting, min diff w/ last iteration run and NO conditional probability cutoffs anywhere run
      - Last experiment results look good
  - SNS
      - Updated my features based on latest JM results
      - Started writing new token-based feature: % of words in window of size 1 that were seen in same window before, etc. Or something along those lines. But there's an inconsistency between the data files (of french sentences) and the corresponding psd files. Hal says it was his fault, it's on the table to be fixed

1/25/13
 - LRMT: 
   - NAACL author responses

 - DAMT-MM meeting with Hal and Chris
   - Like passive aggressive, one has a margin constraint and one doesn't. Kirby defines both in thesis. McDonald's first publication got names mixed up. MIRA tries to minimize regret but doesn't necessarily minimize it
   - Take as axiom: comparability entails matching marginals, so we're going to try to take advantage of that. Proof is in the pudding
   - Use number (% of parallel sentences) with Smith 2010 citation
   - Learning rate really isn't that small, probabilities are small in the first place
   - Parallel sentence extraction: we're going for coverage, so ttable feature will get sentences with lots of coverage
   - Cite Wu 98, Pascal won't be reviewing
   - Make document pair thing more clear: written example of two pages, using documents concatenated or separate

1/28/13:
 - DAMT-MM:
   - Realized only had 25k wiki pages listed on list on coe. So all results from before had only used that many. Oops. rerunning experiments with new list, starting from iteration 31
   - Started ablation experiments: 
      1. without edit distance
      2. without wikipedia similarity score (to do)
      3. without frequency score (to do)

1/31/13:
 - DAMT-MM
   - And realized was pointing to unigram-counts instead of unigram-counts2, which has counts for pages beyond the top 25k lists. Ooops again.
   - Ablation experiments running:
      - No wiki, no ED, no freq, no penalties at all
      - Adding intrinsic baselines: MRR using edit distance based rank only, on OOV words only. 77577 oov science words
      - Getting MRR-so-far. Have different English minimum frequencies, and so far performance is much worse (good for a baseline) when there's no cutoff. That's fair to use though I guess.
      - TO DO: 
        - Only evaluate on French dev set words instead of all (duh) <- TO DO
	- Try full model + edit distance only, then if that looks reasonable to small grid search on ed penalty <- job started
	- Clean up writing in MT section <- TO DO
	- Can't optimize penalty weight b/c would learn zero. Duh.

	
 - LRMT:
   - Meeting with CCB:
     - Do experiment where append MRR-ranked translations to phrase table. Without extra scores. Hope that falls between baseline and supervised system.
     - Write ACL paper that combines two NAACL short papers
    
2/1/13:
 - DAMT-MM:
   - Modified evaluation scripts to compute MRR over ONLY the top English translation for each FR word in the DEV set only:
     - For baselines: qsub -cwd -l mem_free=10G,h_rt=100:00:00 -V getBaselinesAllWords.py 0 (last number is minimum English frequency)
     - For systems: qsub -cwd -l mem_free=10G,h_rt=100:00:00 -V getScoreAllWordsLearner.py science-docpair.100.8.60.noboost.previt.nocpc.abl-nofqnows.jan24/
     - Hopefully baseline numbers aren't great. The only problem here now is that currently old distribution is doing better than first iteration of learned distribution... maybe should try using not just top-1 English translation but some prob threshold? Otherwise for NOW (baselineSoFar.py) baseline numbers are looking w/i range of learned, and looks like learned is steadily increasing...

2/4/13:
 - DAMT MM:
   - Jags baseline on dev set OOVs only: 0.148481502816 MRR
   - Note that this also only considers words in his top-10 list
   - To compare with MM method, need to write script to get MRR over OOV words only, not entire vocabulary. The problem with that is that the edit distance baseline is SO GOOD. 
   - Tried OOVs for which the answer is not the same string, which is the default MT output. Penalizes the ED baseline, sure, but don't need to get those right. Those results look pretty reasonable, and nice to have Jags' AND simple ED baselines for same set of words. Should go with that. 
 - LRMT:
   - Ran MRR experiments, using and not using transliterations also, for hi, ta, and bn

2/5/13:
  - DAMT MM:
    - Learning really slow, so re-introduced CPC cutoff for Science.
    - Updated getScoreAllWordsLearner.py to read old evals and only compute mrr eval for new iterations that have shown up.
    - Updated ACL paper w/ MIRA and new intrinsic results and only ED penalty
    - TO DO: start comparable learning experiments for emea and subs    
  - LRMT: 
    - Got MRR MT results, look reasonably inbetween original baseline and the supervised better baseline. Nice.
    - Put together ACL long paper based on two NAACL short papers: right now 7 pages, so have one more to work with.

2/6/13:
 - DRMT: 
    - Correlation coefficient between scores or ranks in phrase table. Compare:
      - Scores given by small bitext (lexical scores I guess)
      - Our lexical scores (averaged across phrase internal alignments)
      - Our phrasal scores (composed phrase pairs)
    - TO DO:
      - Implement pearson's coefficient and spearman's coefficient comparisons of 'gold' p(e|f) and three models above
      
2/7/13:
  - DRMT:
    - Decided pearson's coefficient makes no sense (not normally distributed, and we really care the most about the high probability mass items), and neither does spearman's rank (again, difference between lower ranks isn't super important)
    - Proposal: KL-divergence, averaged over source language phrases, either token or type level (maybe top 50 or 100 candidates for each src phrase) <- CCB agrees
    - Could do experiments on additional Indian languages, or Spanish
    - Do learning curve exp over number of sup lexicon induction training instances

 - DAMT Spot new sense:
    - Job dependency note:
    qsub -cwd -hold_jid 314189 -l mem_free=8G,h_rt=1:00:00,h_vmem=8G -V run.sh Subs
    - Had to update run_experiment to make regex work for finding those token files
    - Added two token-based features, both based on two word context of a given token, and whether that word had been seen in context of word before or not
    - So far not helping performance on Science :( but helping slightly on Subs and EMEA

 - DAMT MM call with Hal and Chris Q:
    - Chris Q going to work on paper, check in changes by Monday. Then I'll take a pass and have done by Wednesday, then Hal
    - In the meantime just babysitting experiments

2/11/13:

- DRMT:
  - Meeting with Alex: he has europarl subsets together here: /work/distrib-reprs/data/alex-data/parallel.short
  - I should go ahead and extract phrase tables from those subsets, get baseline phrase tables ready.
  - Also maybe should re-extract big phrase table from Spanish europarl for gold standard
  - Then: score old table based on learning w/ interaction matrices from smaller datasets
  - Compare new scores with p(e|f) estimated on big data
  - Alex will send me tool to score any pair of phrases sometime soon

- DAMT-MM:
  - Started MT experiments for Science Iteration 50. Just want to make sure still in ballpark with previous results
  - TO-DO: update experiments section of paper with new results and more concise experiments (maybe just tables 2 and 5 combined into single table, and just mention results of tables 3 and 4 in text).

-LRMT:
  - Learning curve experiment varying amount of training data - just for one or two languages: have running for so, bg, id, es now. Also have R figures ready to produce with results... Note: done by sampling rate, not raw number of examples. Might want to change that and rerun...

2/12/13:

- DAMT Analysis:
  - Additional sanjeeval evaluation done: /home/anni/damt/sanjeeval/compAnnotatedAlign.py
  - Compares how annotations on particular alignments change from old-system to mixed. Confirm that everything is consistent with results in old TACL paper. Emailed science output to Hal.

-LRMT:
 - Learning curve over supervised bilingual lexicon induction training data experiments run
 - /home/hltcoe/airvine/inducePhraseTable/LIMT/Experiments/LexInductExps/rankCombine/learningCurve.py (and genLC.sh) uses varying amounts of original training data to evaluate on same test data for a given language. 
@shreejit:seems like a central runner for running data over existing prepped data
 - results in figure learnLC.{R,pdf}


2/14/13:
- DAMT MM:
  - Merge Tables 4 & 5
  - Try to add space for the other two learning curves, maybe side by side. Add numbers from Table 2 to plots themselves
  - Plot docpair similarity vs learning, for us to see at least
  - Make caption sizes smaller


2/18/13:
 - Made all above updates to DAMT-MM paper. Looks pretty ACL-ready to me...


2/21/13:

 - Meet with CCB:
   - Email David, tell him I'm interested in combining monolingual (morphology induction stuff) and bilingual (bilingual lexicon induction stuff) into single framework to induce translations for inflections of words not seen before... essentially cluster source words and cluster target words and then search for best translations within clusters
   - Think of thesis as making different predictions on source language data: when to transliterate, how to translate, spotting new senses, when phrases are compositional and need a new translation or not 
   - Could do project on figuring out what phrase pairs or translation rules are ok to throw out: or what regular morphological patterns are ok to throw out b/c redundant
     - There just need to maintain bleu score, not improve. Want to improve MDL or something
     - For compositionality, think about Ken Church's old chi square test stuff
   - Remember what's going on with original and transliteration stuff (predicting who's from where)
   - Maybe let Xuchen know about the supervised syntax-informed alignment stuff that I worked on
   - Work on automatic dictionary quality control paper that CCB has going w/ Ellie: he will send what they have so far

 - DAMT-Analysis meeting
   - Get result that Hal asked for (WADE column sums using old-domain test data and old-domain training data - i.e., what's typical) <- done
   - Read new notes file, reread paper, and comment; punchline would be nice <- done


2/26/13:
 - Brainstorming morphology + SMT (or: using morphological-analysis-inspired techniques to reduce MT/BLI data sparsity)
   - Possible Approach:
     - Cluster and predict
     - Yarowsky paper ("Minimally Supervised Morphological Analysis by Multimodal Alignment") produces hypothesis-root pairs (really, a clustering over words into those that share a root) and, optionally, an morphological analysis (stem change/suffix, and POS)
     - I could assume:
       - An English clustering (derived, e.g., via David's method)
       - Some parallel data
     - Would want to produce:
       - Clustering on the target side
       - Probabilities over pairs of source-target clusters
     - Approach would be:
       - Get some seed source language clusters based on alignments
       - Use frequency and string similarities (for example) to learn inflectional patterns and induce even more (a la David's paper)
       - BASELINE here would be just stemming at some number of characters
       - Then learn p(Tc|Sc) for all source and target clusters Sc and Tc, respectively based on aggregate distributional properties (could be supervised, given same aligned data)
     - Intrinsic evaluation:
       - % of time a source language word's translation is in a corresponding cluster
     - Extrinsic (MT) evaluation:
       1. Use supervised approaches to predicting translations for source words, given cluster pairs (ranked list or discriminative +1/-1 classification)
       2. Throw all target translations in a cluster corresponding to source word's cluster into an SMT model, maybe w/ some cluster-based probabilities
   - Papers to review:
     - Modelling lexical redundancy for machine translation (Talbot & Osborne)
         - I don't want to get at redundancy alone, more morphological clustering

2/28/13:
 - Meeting with CCB:
   - Engineer way to give list of pairs of words and score with all signals and then score w/ given learned model (make vw test file). Could add other features here too, like turker location
   - Email w/ Ellie about that paper, and also about Lisbon ML school
   
- Josh crawls: his home directory, then DeepCrawls.txt has information about where big crawls are, including if cleaned. I told him to clean for all languages that he hasn't yet

3/1/13:
 - Pre-David meeting:
   - What I'd like to do includes:
     - Noisily supervised morphological analysis on source side (e.g. by projecting over alignments): POS tags and segmentations
     - Now attack is both macro (align clusters/word lemmas) and micro (w/i macro clusters, align inflected words), using morphological information as well as other signals
     - Tasks:
          - Inform reordering model (use priors over word classes to smooth and/or apply to new translations)
     	  - Could also use to inform BLI/PT rescoring
   - Engineering to be done: add to babel code option for inputting an equivalence class dictionary, where words get scores based on those equivalence classes (similar to prefixes now, but want to input a dictionary to look up instead of just stemming)
    
 - Post David meeting:
   - Don't work on with Matt
   - Alignment: 
     Corpus 0: all 1:N (source:English) words from phrase table as new training data, including 1:1
     Corpus 1: try all segmentations of source words 
     Corpus 2: pruned segmentations of source words
     -> Add weights to 'sentence pairs' to alignment model, according to their probability
     -> Iterate to multiple segmentations per word up to length of English string (maybe +1) greedily (my idea)
   - Segment corpus that way, and LOOK UP segmentation on test corpus based on training data
   - Should help with training data alignments in general though too: e.g. no more garbage collecting
   - For OOV test words: try segmenting based on model, MDL or something. Max number of substrings based on corpus stats from training data. Could use multiple segmentations as lattice input
   - Eventually do literature search on this stuff. There's probably some word on German noun-noun compounding
   - Check out Dipangan paper on universal tag set
   - Also look at Drabeck ACL 05 (I think) paper on tag set projection

3/4/13:
 - Pairs scorer, for paper w/ Ellie
 - Camera-ready NAACL paper

3/11/13:
 - Got latest Moses running on the COE end to end! Didn't need libboost-dev-all after all. Just checked out main moses branch instead of DAMT branch, and all is good. It is here now:
     - /home/hltcoe/airvine/Code/moses-clean/mosesdecoder 
 -  Started on David's project: /home/hltcoe/airvine/mtMorph
     - working on extracting relevant phrase pairs in runMorph.py
     - figured out how to run giza stand-alone: runGiza.sh

3/12/13:
 - MT-Morph
   - Need to make list of stop words for the English multi-word phrases
   - Now that have alignment stuff working, actually run on extracted phrase pairs and 1:1 pairs
   - Working on alignIt.py, need to continue

 - Worked on Pavlick dictionaries:
   - Got scorers running here: /home/hltcoe/airvine/Projects/pavlickDicts
     - To do when have controls:
       - Score controls dicts and random negative examples in the same way
       - Train a classifier using controls and random word pairs 
       - Then rescore phrasal-scored translation pairs
       - Give Ellie her original dictionaries, w/ single score for each pair
     - Transliteration (do later, once have things set up for roman script languages):
       - Transliterate phrases for non-roman script languages
       - When writing features file from two pt.dict.lang-p files, also compute edit distance on the fly
     - Note one issue: random negative examples aren't going to be representative of the types of incorrect answers received in MTurk. E.g. looks like lots of people are just copying the input into the output. So string similarity shouldn't be useful, or even a negative feature. But w/ random negative instances, it may be useful... Seems like what we really want are some turker judgements on translations.

3/13/13:
 - Mt-Morph
   - Getting new format moses es-en phrase table from full europarl: clsp /home/anni/LRMT/esPhraseExperiments/samplefull
   - Have multi-iteration alignment/splitter running: /home/hltcoe/airvine/mtMorph/alignIt.py
   - Each iteration, for each phrase pair, prunes lowest scoring split
   - Prints out 'interesting' output in splitoutput for each iteration
   - runMorph.py takes a config file like 'config' and will output pruned phrase table given parameters specified
   - Right now each Spanish word aligns to at most one English word, but English word can align to multiple Spanish words. Need to do inverse alignment and read both outputs
   - Added inverse alignment, and then scores with sum of two
   - Also added variable in main method that is the minimum length of target string to allow for splitting on source side; was just playing with 2, but looks like could get some help w/ 1 as well
   - Thought about: Find out what that GIZA score actually is, consider if sum is the best way to go (maybe should normalize probabilities first?): it's probably the p(s|t) - in forward direction each target word aligns to potentially many src words, but multiple target words can't align to single src. In backward direction: p(t|s). Forward direction probabilities are much higher right now, because some of split s's are very, very rare.... maybe I should be multiplying these probabilities, since they are probabilities... but that's p(s|t) * p(t|s). What I really care about is either p(s|t) or p(s,t), which would be p(s|t)*p(t) or p(t|s)*p(s). p(t) is the same for all competitors, because what varies is the source phrase segmentation. p(s) could be estimated as product of unigram probabilities over segmented corpus and could incorporate the alignment probabilities as well (p(s)=p(s|t)), for the given segmentation. and p(t|s) is given... ahha, ok, so p(s,t)=p(t|s)*p(s), and p(s) estimated using p(s|t)...
   - 'prefer' is a good test word to look for
   - looks like when i multiply (esPracticeEs1En12_mult) basically nothing segments, get all original words. but when i add (esPracticeEs1En12_add) it looks slightly better. And when I go back to just using forward alignments (esPracticeEs1En12_forwardonly)...
   - Really need a way to automatically evaluate! Maybe even just hand segment... or use some gold standard from a morphology competition or of david's?

 - To do:
   - Reconsider allowing splitting source of 1:1 word pairs...it sort of gets away from the heart of what talked to david about
   - Need to think about automatic evaluation here. Minimize vocabulary size? Maximize probability of corpus?
   - Note that all of this is phrase pair type level. In future should think about incorporation frequency/token level information - maybe into frequencies in auto-generated .snt files?

3/14/13:
 - Pavlick dictionary project:
   - From here write context dictionary and to-score dictionary: /Users/anni/Dropbox/Papers/PavlickDicts/turker-demographics/code/src/anni-score/prepDict.py
   - **** TO DO: add more randomly generated negative examples
   - The cp.sh script will copy over to correct place on COE too
   - Then from COE, run "python runLang.py lang" - will submit wiki and crawls scoring jobs
   - After scoring is done, run "python collectAndRerank.py lang" - will write VW train (control src words + random neg examples) and test files with wiki, crawls, and freq features
   

 - Meeting with CCB:
   - Go ahead and apply for travel approval & ML Lisbon workshop?; travel request submitted
   - Project with David
     - Weight phrase pairs (as sentences) w/ alignment probability throughout iterations instead of deleting!
   - Ellie's dictionaries
     - Negative examples not random (lots of transliterations/copies) <- some are judged good and bad; emailed Ellie asking for details
     - Write script just to check w/ matches in existing David's dictionaries; done

 - LRMT TO DO: 
   - Run baselines for x% of Tamil, Hindi, and Bengali data. Move to COE computers. Prepare to score phrase tables over learning curve, supplement with translations, etc.
   - Make sure don't need to add OOV words to reordering table?

3/15/13:
 - Meeting with Alex:
   - Command for phrase similarity scorer: java -Dfile.encoding=UTF8 -Xms40G -Xmx40G -ea -cp /home/aklement/Projects/Java/DistributedRepresentations/jars/PhraseDistribRepsScore.jar:/home/aklement/Projects/Java/DistributedRepresentations/lib/Jama-1.0.2.jar:/home/aklement/Projects/Java/DistributedRepresentations/lib/args4j-2.0.21.jar unisaar.dreps.langmodel.PhraseSimilarityScorer --fromLang l1 --toLang l1 --save-prefix /fibre/work/distrib-reprs/experiments/models/phrasetest.es-en/ --num 10 --mode r --testfile testphrases.txt
   - If there are a lot of phrases that aren't getting scored, can comment out line that checks whether a phrase is unknown or not. Can use the default unknown representation instead of nothing.
   - I should play with phrase table scoring and comparison with other bilingually estimated features by next Thursday

 - Ellie's dictionaries:
   - Script for checking matches in existing David's dictionaries: /home/hltcoe/airvine/Projects/pavlickDicts/checkDavidDictOverlap.py lang
   - Outputs dict with pairs and booleans for $control $correct $observed $indaviddict (correct now comes from control dict and/or pos/neg judgements)
   - Need to update /home/hltcoe/airvine/Projects/pavlickDicts/collectAndRerank.py to do 10-fold validation and prediction on controls themselves. And use all controls to predict all others
   - Shouldn't be a lot of work, then just run that on all outputs and give to Ellie. She can modify her dictionaries to include other features too, and I can relatively easily pass them through
   - Running scoring on es, tr, sr, and az now

- MT-MORPH
   - Updated alignIt.py to use alignment probabilities as sentence weights (counts), iteratively, and not drop the worst one (that's a boolean that's easy to change)
   
3/17/13:
- Did 10-fold scoring on controls for Ellie, and scored using existing dictionaries; sent all to her

3/18/13:
 - Worked on camera ready NAACL paper: new, pretty, complete learning curve

3/19/13:
 - NAACL camera ready:
    - Comparison w/ logistic regression (in spreadsheet): difference is not statistically significant
    - Fixed up all figures in paper
    - Reviewed the reviews: everything addressed
    - Tomorrow do an end-to-end read through. Have about 3/4 of a column to add more
 - MTMORPH:
    - Currently initial counts are uniform probabilities over target types: 1/(number-of-src-splits-for-target)
      -> Added option to config file (useInitialCounts=True/False) to use phrase pair frequencies from original phrase table (smoothed, so using max) instead: now still uniform over target types, but instead: PPFreq/number-of-src-splits-for-target
    - Another option to use those counts EVERY time, in addition to just first time (in config file: useCountsThroughout)
    - Also added explicit epsilon, in addition to GIZA's null. Doesn't seem to make a difference (nothing ever gets aligned to it), but the config option is useEpsilon
    - Also, on each iteration, write two files w/ word-level translation probabilities, p(t|s) -backward and p(s|t) - forward
    - Added another option (wordPrefixDivisor) to set minimum prefix length (relative to src word's length)
    - Figured out how to sort by one column as string and another as a number w/ scientific notation... see commands file

3/20/13
DRMT: 
 - Wrote script to compute KL-divergence between two features of a scored phrase table: /home/hltcoe/airvine/distribRep/ptFeatureComparison/compFeats.py
 - Added random number baseline
 - Added capacity to do average probability mass overlap instead
 - Alex hasn't added unisaar.dreps.langmodel.PhraseSimilarityScorer class to svn repo, and the output format is annoying...
 - But still played with scoring ES phrase tables: mmci /home/airvine/032013
 - Only 1726 out of 122745 source phrases scored by shortphrasetest model and only 4051 scored by phrasetest model... ridiculously small fraction
 - Issues (most just need to see code to answer):
   - Why 'too long'? Can't compose up to any length?
   - Output format should be same PT format: src ||| trg ||| score
   - Output should write to specified output format
   - Should be able to input phrase table with existing features... maybe just add column with ||| score at end
   - Why is KL-divergence higher for random?
MTMORPH:
 - Want to use a bigger starting phrase table. Restarted europarl-full run on both clsp (old run was hung) and coe grids

3/21/13
DRMT:
 - Got new svn project: svn co http://it-svn.coli.uni-saarland.de/svn/it-svn/EventDistribReps
 - Chatted with Alex. I'm to read through that code, use unknown token instead of skipping those phrases, and try to not use unigram predictions (only bigram and trigram). 
 - See notes on this computer here: /Users/anni/Projects/unsup_translation/distribReps/EventDistribReps
 - When reduce phrase table to only include max phrase lengths of 3 on both src and trg, only 39647 src phrases. 1453 scored by short model and 3229 by full model... still less than 10%... scoring isn't going to be very good if all the rest map at least one word to unknown...
 - When only consider src-trg phrase pairs that have moses counts of at least 3 each, 752 src phrases scored of 5670 remaining in phrase table, or 13%. Still not great... these are counts in the sample2000 phrase table too... that src lang vocabulary, among those src phrases, is only 1848...
 - -->>>Ahha, l2 is spanish, l1 is english, strangely. Now 5440 of 5670 are scored, 95%. Much better
 - Wrote testeval.sh and testlearn.sh along with sample data in tinyData/
 - Also made build.xml that you can run 'ant' on to compile all. Fancy.
 - On compFeats script, created minimum threshold on src counts. IE don't care as much about matching dist. of very poorly estimated probability distributions anyway. Made comparison much better!
 - Note that probabilities don't sum to 1 in phrase table I think b/c of good-turing smoothing

3/22/13
DRMT:
 - Scoring a real phrase table with DRs here: /home/hltcoe/airvine/distribRep/EventDistribReps/pteval.sh, then can evaluate scores
 - Added optional param --max-learn-epochs to limit learning epochs b/c when stopped learning myself on tiny datasets, sometimes caught it in the middle of writing an output and wouldn't work...
 
3/25/13
DRMT: 
 - NOTE: moses features are: phrasal p(f|e), lexical p(f|e), phrasal p(e|f), lexical p(e|f)
 - Comparing phrasal p(e|f) distributions, averaged over all f
 - When use *weighted* average kl-divergence (src phrase relaively frequency based weighting), DRMT score does better than random prob dist. Either weighted or unweighted PMO looks better for DRMT than random scores. 
 - Not as good as vs. lexical PT probability -> not even close...
 - Weighted KL divergence:
     - Random: 6.1800
     - DR Score: 4.7835
     - Lexical p(e|f): 0.4914

Crawls:
 - Adding Josh's output to COE files: /home/anni/joshCrawls/processOut
 - Problem w/ encoding: not in utf-8. Emailed him, he thinks he fixed.

Ellie's dictionaries:
 - Rerunning all w/ her new dictionaries.
 - Allowing for using only crawls or wiki data, if only one is available.

3/26/13
MTMORPH:
 - Added beginning of word and end of word markers on splits
 - Figured out math: start at line 92. Using product of translation probabilities now instead of alignment probabilities. Makes way more sense


3/27/13:
MTMORPH:
 - Worked on formalizing the math a bit in a notebook... totally redid the way I'm scoring alignment pairs in EM iterations (M-step). E-Step is given by GIZA t-tables. 
 - Might need to put some kind of prior on splitting Spanish words (want to split them)... penalize not splitting them heuristically (w/ some single parameter value)?
 - Seems to work better w/o using counts throughout (as they are...maybe want to make that contribute but not SO much?)
 - Emailed David for another meeting. Before meeting with him should write something up
 
3/28/13:
DAMT-MM:
 - Drafted author response for ACL paper
DAMT-SNS:
 - Need to look over author response
Pavlick-Dictionaries:
 - Sent Ellie all updated dictionaries. Only need to add additional languages now.
Josh-Crawls:
 - Pointed him to a script that will skip non-utf8 files (clsp: /home/anni/joshCrawls/encoding)

For CCB meeting:
 - NAACL camera-ready paper (learning curve)
 - ACL short resubmission: indian languages paper. Reviews were mostly editorial, though a few suggestions for additional analyses that would be possible to add. Will be easier to write now that I can reference the NAACL BLI paper
 - Josh: Amazon web services account (AWS) - w/ credit card info, etc.
 - Dictionaries that I emailed to Ellie
 - DRMT: comparison w/ bilingually estimated phrasal translation probabilities
 - MT-MORPH update of some kind
 - ACL reviews

CCB Meeting:
THESIS
 - Reordering: SCFG parameters 
 - Phrases: revisit old stuff; maybe decision tree over filters...
Papers:
 - Definitely resubmit ACL short paper
 - Read over Ellie's TACL paper: she's submitting Saturday
 - Keep MT-MORPH notes in thesis document

4/1/13:

Josh Crawls:
 - Script to 1. process crawls (tokenize, lowercase) with moses tools, 2. Match w/ English data (up to three times amount of language data), 3. Copy data to COE in correct place and w/ correct file names is a go! Used ssh keys to password-less copy files. Run from here: /home/anni/joshCrawls/processOut/
 - TO DO: add his big crawls w/ random date from winter too, once they're re-processed... though they don't look relatively that big, so maybe no big deal to skip them

ACL short resubmission work
- Rerun w/ adding new translation pairs to reordering table; rescoring w/ additional data
- Running experiments on Telugu, Urdu, and Malayalum as well; set up everything for them
- For all 6 languages: translating all dev and devtest words w/ new data and dictionaries (lexinduc.*.sh jobs); scoring all phrase tables w/ new data and dictionaries (runscorer{W,C}.sh jobs); generating new baseline runs
- THEN (once all words and phrase pairs are scored): update featuresScore.py and rerun experiments

4/2/13:
ACL short resubmission work:
- Fixed reordering stuff: adding new translations to reordering table w/ average probabilities of all other phrase pairs
- So far +accuracy results look good, even with all features! (for ta, hi, bn)
- Also got rid of all 0.0s in the phrase table, replaced with 1e-07
- Got lex induction models learned for te and ml
- Pushing through those experiments, commands file is well-documented
- Running eval-only alignments on clsp here: /export/a01/anni/IndianLangsBaselines. Corresponding dictionaries needed for coverage oracle experiments (get those here: /export/a01/anni/IndianLangsBaselines/getAlignmentBasedDictionaries)

4/3/13:
ACL short resubmission work:
- Have average of three runs for all baselines, some variation, so probably a good idea
- results aren't great using batch-mira and all 9 new features. below baseline on almost all languages! Trying two things:
  - Other tuning methods (mert, pro, w/ j=300 batch mira option). Pro and mert aren't doing great on most languages... trying to vary J in batch-mira; doesn't help
  - Oh, had changed so that scores of <=0.001 would just =0.001. That's one difference. Rerunning batch-mira w/ j=60 (default) without that modification, w/ all 9 features
  - Ooooh nice, this looks much better already. Note to self: don't mess with the bilingually estimated features! Playing with how to make 0.0s new new features nonzero (at what value to cut everything off: worktest_ac is 0.001; worktest_ac2 is 0.0000001. Probably won't be much difference. The default values for new phrase pairs may matter more...though maybe not if they're all added for OOVs only). .001 is better for most langs.
  - Also played with default values for new features for new pairs... and those min freqs so not to mess up ML results w/ bad new translations, but nothing consistently helped. Keeping it simple; also no greedy search for features

4/4/13:
ACL short resubmission:
 - Ran all experiments three times and averaging results. 
 - Oracle coverage horrible w/ GDF alignments. Running with intersection instead now.
 - Urdu STILL scoring phrase table w/ crawls
 - Make thread of thought "How much does OOV translation accuracy matter?" Refer to table 3 and oracle OOV experiments... no gain for Tamil and Malayalam but OOV translation accuracy also horrible (using single best prediction). Mention experiments adding more than top-1, but not good. Refer to coverage oracle as upper bound if Table 3 was 100%. 
 - Coverage oracle: oops, was using only dev set oov words! Now using both... get dictionaries here: /export/a01/anni/IndianLangsBaselines/getAlignmentBasedDictionaries and then run python coverageOracle.py hi plusTransOracle.config to write mt directory for run
 - Totally rewrote and reorganized short paper: in really pretty good shape now! Do one read through and then go ahead and send to CCB for feedback. Should have plenty of time before deadline (in 10 days)

CCB meeting:
 - Cite Clark and Dyer ACL paper when do 3 tuning runs for my ACL short paper
 - Send ACL short draft to CCB Monday or so
 - Prague MT marathon?
 - Go ahead and submit application for ML summer school!

4/5/13:
 - Meeting with Josh:
   - Schedule for summer: send to me and CCB
   - Definite yes? -> yes
 - Worked on NAACL camera-ready

4/7/13:
 - NAACL camera ready done
 - Finished UR experiments for ACL short paper

4/8/13:
 - ACL notifications: 1/2. Not the one I would have picked. Ugh.
 - ACL short: totally rewritten, sent to CCB 
 - Lisbon ML School application: working on it

4/10/13:
 - Submitted camera-ready of NAACL SRW paper
 - ACL Short Indian languages submission:
   - Scoring all 1 word source language words that have at least 1 word translation in the training data (basically, just the dictionaries. allows for some in the Hindi data too, which doesn't have a dictionary). Going to report top-1 accurcies in Table 3 of that paper. Trouble is that scores are really high... higher than for true OOVs. Are OOVs different in nature than the common words in the dictionary? Probably. That's why I'd done the dev set OOVs before, but CCB thought it was 'convoluted.' Talk about tomorrow.
- DRMT:
   - Added input parameter to src/unisaar/dreps/langmodel/PhraseSimilarityScorer.java to alternate between skipping OOVs or not. Appears to be working: scoring more of phrase table than before (before skipped 882645, now skips 557295). And those that are skipped look like they really are too long. Good.
   - Right now:
     Table skipping OOVs: /home/hltcoe/airvine/distribRep/EventDistribReps/Exp1
     Table not skipping OOVs: /home/hltcoe/airvine/distribRep/EventDistribReps/Exp2
   - A few results here: /home/hltcoe/airvine/distribRep/ptFeatureComparison
   - Extracting 20,000 line system with phrase limit 3 here: /home/hltcoe/airvine/distribRep/esPhraseExperiments/sample20000_PL3
   - Got the above table, scored it here: /home/hltcoe/airvine/distribRep/EventDistribReps/Exp3
   - Scored 20k phrase limit 3 phrase table (Exp3) and used to supplement feature space in MT; compare performance here: sample20000_PL3_newscore and here sample20000_PL3 (with and without DR added feature)

4/11/13:
 - DRMT:
   ES 20k without new score: 18.49; ES 20k with new score 18.59
   - Getting 200k, 1M, and full phrase table w/ phrase limit 3. When done, score PTs, and retune and redecode 
   - Chat with Alex: Try MRR, Top-K Acc, Email out write-up to CCB and Ivan and Alex
      - He mentioned product of experts with three predictors, force them all to be good
   - Now: Exp3 is shortphrasetest, Exp4 is phrasetest-epoch55, and Exp5 is phrasetest-epoch132
   - Wrote big write up in /Users/anni/Dropbox/Work/CrossLingWordRep/PTScoreResults
 - Meeting with CCB:
   - Work on filter/decision tree stuff again!
   - Could add dev set OOVs evaluation to Table 3 in ACL short paper if that makes explanation of Table 4 results easier. Would also need sentence to explain.

4/12/13:
 - Friday meetings. 
 - Finished results on Post training data for ACL short paper; polished paper, sent to CCB
 - Making learning curves for all six languages: /home/hltcoe/airvine/Data/gizaclean-indian-parallel-corpora/LCDataSetup.sh
@shreejit:not sure what these learning curves are for. Everything from last comment to this one is confusing and not sure if she was working on the induction project or not
 - Goal is to make those learning curves for all languages like had before: standard, +Trans, +Acc, +Both. 
 - Should be able to look up monolingual scores from existing scored (full) table (shouldn't extract novel rules from subset of data...) and already scored and reranked ALL dev/test words I believe, so can use those in the same way as well. Awesome. Just need baseline table for each LC run and a new python script to grab mono scores from scored tables for what ends up in new LC tables.
  - Started all BN baseline jobs here: /home/hltcoe/airvine/inducePhraseTable/LIMT/Experiments/IndianLangsCorpus/bn-en/LC
  - When those are done, write script to produce a scored table (run all of those), use featureScore.sh to add translations (Separately, and run all of these), and then do both
  -> This is easy to work on over the weekend if want to get something really productive (for thesis!) done. 
 - Monday start working on 1. Decision tree filters, 2. Alignments (David project), 3. Make these figures
 - Oh you CAN extract novel phrases from new data because alignments are different. 

4/15/13:
 - Learning curves for Indian languages: working through them. See commands file

4/16/13:
 - Kept running through learning curve experiments.
 - Cleaned out some files to free up space on COE grid
 - Made learning curve plots: /Users/anni/Dropbox/irvine-bigone/paper/indLangsFigures/learningCurves.R
 - Read one Holgar Schwenk paper about using comparable corpora to discover parallel sentences: uses existing SMT system and Lemur toolkit to do IR, then TER thresholded for pairs. Poorly written EACL paper, should probably read version in Machine Translation journal
 - Worked on segmentation-alignments stuff: Wrote 2+ pages of a document to share with David tomorrow

4/17/13:
 - LRMT:
   - Made LC plots for ta, ml, te, bn, and hi: basically +trans helps but +acc not much more. Will help for UR more probably
 -MTMorph:
   - Keep writing segmentation/alignment paper, with example *outputs*
   - Made file to align from full Spanish Europarl data (before was from sample). Limiting to es and en phrases that appear at least 3 times each (same as before), es length 1, en length 2 exactly: X entries
   - Ran lots of iterations of segmentation model, kept writing document.
   - Meeting with David is documented in the write up (now 8 pages long, with tables). 

4/18/13:
 - LRMT:
   - Beginning decision tree for inducing phrase table: /home/hltcoe/airvine/ptRules
      - First: get some training data together: relatively high probability phrase pairs from ES phrase table and some random phrase pairs
      - Using Norvig's Python implementation of decision trees, which chooses split points based on information gain (reduction in entropy essentially)
      - Src and Trg same length a very good predictor!
      - Framework exists now to try lots of differen features. The idea is that it should be possible to implement features such that decision tree split points are very fast to evaluate. Example question: What's the one feature that we can use right away to throw out as many probably bad phrase pairs as possible (w/ high precision: don't want to throw away good pairs along the way)? Something about their lengths or frequency differences?
      - Remember: there's going to be a recall/precision tradeoff... DT is information gain focused, but we may care more about one type of mistake than the other... maybe
      - Relative easy to add some other metric into code instead of IG. It's just a function of the attribute and examples (knowing the target). Could replace with like highest precision feature or anything
      - Added parameter for information gain in order to spilt a node. By default this code was always splitting until there were either no more instances left or no more attributes to split on. Now only splits if information gain is at least X (input to DecisionTree object initialization)
      - Getting phrase counts from Wikipedia, up to trigrams, here: /export/common/data/corpora/comparable/wikipedia/pagepairs/en-es/phrases
         - Let those jobs run, then run: 
	     1. processLetter.sh for each letter
	     2 python combineAllCounts.py en, python combineAllCounts.py es
 - French w/ Ruth:
   - Doing decoding of texts that Ruth picked out here: /home/anni/frenchTransTheory
   - Using full Hansard baseline phrase-based model
   - Could think about adding another LM

4/19/13:

Meeting with CCB:
  - MT MORPH update:
   - Met with David, document, doing MT experiments (three ideas)
   - Code to combine probabilistic alignments?
  - Decision tree: phrase table induction
Next time:
  - Indian Languages Learning Curves
  - Schwenk paper: EACL bad, but look at MT paper as well

Random:
  - Taught myself about ant: /home/hltcoe/airvine/test
  - There can compile java file(s), make a jar, and run java files. Took forever.

Decision tree stuff:
  - 113,104,692 spanish phrases in Wikipedia; should probably stop reading after frequency is less than 10 or something. Looks like on the order of 3,500,000 es phrases with frequency >=10
  - If doing to have dynamic features added, need to account for that in the names somehow! Or keep features binary or from some precomputed list of possible values

   - Work on comparable corpora stuff from Holgar Schwenk (mine parallel doc pairs)

4/23/13:
LRMT:
 - Finished off learning curve experiments for Indian languages: Urdu
 - Rerunning all _ac_ and _act_ and _baseline_ and _t_ telugu experiments; average over 2 runs
 - Also rescoring Telugu phrase table with new crawls data to see if helps
French Lit:
 - Fixed Ruth's typos and reran decoding
 - Added some statistics to notes about phrase translation probabilities
 - Started workshop paper
Crawls:
 - Ran job to copy new crawls to COE
CS Ed Reading Group:
 - Read 2 papers for meeting

4/24/13:
French Lit:
 - Worked on paper a little
LRMT:
 - Averaged TE,TA learning curves over three runs
 - Working on averaging BN learning curves over three runs
 - Added learning curves section to thesis
Decision tree Stuff:
 - Added section to thesis. 
 - Took some text from an old paper draft called mono_phrasetable/mono_pt.pdf
 - Wrote lots of text in thesis...

4/25/13:
LRMT:
 - Averaged BN learning curves over three runs
Decision tree stuff:
 - Updated figures, made table of results varying IG threshold
 - Added feature for exact words in source and target
 - Added feature for stemmed dictionary translations, variable allows for changing character prefix stem length (experiments show 5 is ok)
 - Making dictionary of all src words paired with top 50-by-edit-distance target words: /export/common/data/corpora/comparable/wikipedia/pagepairs/en-es/phrases/unigramED.py
   Should only take about 160 hours. Though as src words get longer (likely as they get less frequent), may take longer. If run out of time just run another job... output is from most frequent source to least, so should be useful by early next week w/ single processor.

4/26/13:
LRMT:
 - Averaged HI learning curves over three runs
Decision tree stuff:
 - Got rid of all punctuation in all source and target phrases.
 - Added ED-dictionary features, but will have to wait for that file to finish generating to really test it. So far it's using that dictionary on *stop* words, which seems a little silly. That feature was really designed for content words. Only about 20k top-50 lists right now, check again Monday.


 TO DO: Get estimates about how many phrase pairs will be pruned in pairwise comparison...
 TO DO: Additional features: target frequencies (maybe should just ignore most of them)
 TO DO: make target phrases (for random pairs) not from phrase table but from monolingual text anywhere
 TO DO: Score full Europarl phrase table: somehow compare 1/0 predictions w/ translation probabilities


Meeting with CCB:
- DAMT datasets
- Learning Curves
- DT stuff in thesis     
  - try on OOV phrases
  - don't use source or target phrases w/ punctuation
  - could add learning curve to Indian languages plots w/ induced OOV phrase translations
  - use EDIT DISTANCE in feature: compute pairwise edit distance between all source and target unigrams, then make dictionary from source to like top-k edit-distance target words

4/29/13:
Decision Tree Stuff:
 - Added top-k ED for reading ED dictionary
 - Added option to split data based on weighted f-measure. This way can prioritize recall. It's looking obvious that I should do unigram BLI first, then build up.

4/30/13:
Decision Tree Stuff:
 - 144,185 of 2657482 (total) and 323,752 (freq>=10) done w/ edit distance top-k scoring. Will need to run another job(s)
 - For some reason using numberEDStopTransWords and numberEDStemStopTransWords are useful as opposite features (1 -> predict 0; 0 -> predict 1). Not sure why, so getting rid of those. Doing so barely reduces precision and doesn't affect recall. Was an overtraining step for sure. Adding requirement that each branch of split must have at least X phrase pairs (x=50 right now). ie. the rule must be reasonably general.
 - Switched to using target phrases from entire Wikipedia data instead of just from phrase table
 - Added target phrase frequency (Wikipedia) as feature. Want to learn this cutoff. 
 - Played with making training data much more unblanaced as well (90k false, 10k true in train, same 9:1 ratio as test), but that results in a low recall, even when beta (of F_beta) set to 16, which says recall is 16 times as important as precision. Will have to come up with argument to keep training data balanced: pay more attention to positive examples in learning than their rate in actual data (similar to focus on recall, but actually better)
 - Made requirement that all predictions must not be the same for a splitting node (duh)
 - Started writing filter.py script that, given list of src phrases, will filter trg phrases using inverted indices (manually implement filters in ppfilter class based on learned decision tree
 - In filter.py was missing lots of EN in output because their frequencies in monolingual data were so small that weren't included in original set. Decision: filter src frequency as well. If don't have enough occurrences of src phrase, then not going to be able to estimate monolingually reliably anyway... maybe just do ED-based translations or something? Looks like they're mostly mult-word phrases.
 - Implemented the DT learned here: dtLearn.py.o4381253, estimated will take about 10 hours to filter all 5000 src phrases (7 seconds/phrase). Could parallelize: building inverted indices actually doesn't take much time. Running this right now (overnight).

jones-irvine paper:
 - wrote an introduction
 - made figures bigger so text easier to read
 - paper is right at 5 pages right now
 - needs a lot of polishing to bring it all together, but it's generally in pretty good shape
 
5/1/13:
 - Total number of English target phrases: 83,024,068. With 5000 source phrases of interest, would be 415 billion comparisons. 
 - With the filter learned here: dtLearn.py.o4381253 and freq min of 5 (freq of 4 and less pruned), recall is 99.16 (4958/5000 maintained) (src phrases with monolingual freq<5 thrown out, so a little higher than reported in dtLearn.py.o file), and average number of saved phrases is: 1,270,554 (1/83 of original). So total number of comparisons would be: 6,352,770,000, or 6 billion -> two orders of magnitude less. Note that the freq<5 cutoff alone would have reduced the EN phrases from 83 million to 13 million. Getting rid of singletons alone brings it down to 18 million. W/ filters I'm getting it down to 1.2 million. 
 - Running filter w/ min freq of 2 as well now (4383407)
 - Also running dtLearn with min freq of 0 now (4383405)
 - Started looking at sequences of oov words (call them oov phrases). Looks like tons of them are proper names. Pretty easy to recognize those with English frequencies, however. Good! Don't want to mess those up. But so only 73 multi-word OOV Spanish phrases that appear at least 5 times in ES monolingual data and don't appear in EN monolingual data at all. This number might be less than what we'd expect from other types of data. Europarl is so homogeneous... But they still look pretty compositional
 - There are still some names I don't want to mess up. Should be easy to check if first word is a name or not... Using list of Wikipedia names as name lexicon: gets rid of stefan, juan, miguel, carlos, pablo, riccardo... down to 44. Also computed min(en-freq) over words in phrase. If that's higher than 100, probably just English phrase. "pared vertical" example of something that might look like two English words but definitely not an English phrase (pared=wall, different word).
 - Working on dev set: 43 multi-word non-English, non-Person, non-number/punc OOV phrases wrt phrase table built from 20k europarl lines. Using 2k instead of 20k, 
 *-* Define an OOV phrase:
     1. Composed of OOV/low-freq words.
     *-* Define OOV/low-freq words:
         - Appear no more than x times in training data
	 - Not punctuation, number, or name
     2. Doesn't look like English:
         - Doesn't appear as a phrase in English monolingual data
	 - All individual words don't appear at least 100 times in English monolingual data (note exception example: 'pared vertical')
     3. Does appear in source monolingual data
     4. Appears in bilingual data no more than X times (in experiments, x usually is 0)
	 
 - Idea: 
   1. do bilingual lexicon induction on all unigrams in dev/devtest. Then will have probable translations for all unigrams.
   2a. baseline: just use induced word translation
   2b. Compose phrasal translations from word translations
   2c. Filter space of phrase translations for oov phrases (defined above) using the induced dictionary
     -> the hope would be that 2c is the best, but as long as 2b beats 2a, it's a good result...
   3. Score phrase pairs
   4. Then add phrase pairs and word pairs to training data, so will get lexically smoothed probabilities in addition to phrasal translation probabilities. May want to weight those training instances, let bilingual data have more weight. 

To-do Today:
   1. Induce translations for vocab of Spanish dev and devtest sets -> running
   2. When done, rerank
   2. Write up the above experimental setup in thesis document -> done

5/2/13:
Decision Tree Stuff:
 - Finished write-up, have things cleared up I believe. Target frequency is not source phrase specific, so good first pass. Playing with several different right now. 
 - Parallelizing filtering: filter.py now takes two arguments: name of test file (split test file with splitTest.py) and minimum target frequency. This script runs all jobs: filterSplit.py and then when done, can specify a directory of output files and run filterSplitAverage.py to average recall and filtered set size
 - Made precision-recall graph except instead of precision, average # of target phrases for each source phrase since precision is extremely small in all cases, and what we actually care about is making the search space as small as possible (same thing as precision, but conceptially more clear since trying to estimate computability). 
 - Updated text in paper

5/6/13:
Decision tree stuff:
 - When wiki scoring finishes, get dict of all unigrams. 
 - Add features to DT and retrain
 - Actually write out source phrases that want to translate given 2k lines of bitext and filtered EN phrases
 - Then score phrase pairs: get sense for how long it will take at least****

5/7/13:
Decision tree stuff:
 - Running another job of unigram edit distance calculations because first job stopped after the most frequent 257,655 spanish unigrams. This file is unigramEditDistanceDict2; note to self: combine them later
 - Gathering myself about where I was before. Actually looks like filtering all 83 million phrase pairs is ok. Recall goes from 92.5 to 92.2 using decision tree, and average target search space goes from 83M to 7M. 
 - Wrote script to check how many unigrams in devdevtest set don't appear in big es-en dict. A lot (7713), as it turns out: dictDiff.py 
   Some look like names but many don't
 - Started jobs to collect Wikipedia contextual and ED unigram dicts separately, because big job is taking forever; wiki topic done, now just need context and ED from wiki
 - oovPhrases.py now writes output file of all src phrases to induce depending on what's implemented there (now described in section 8.2.1 in thesis)
 - Arggggg found bad bug in probabilistic dictionary extracted from 2k aligned training data. Rerunning both unigram induction steps for *context only*
 - Started writing script to compose and permute phrase translations composeTrans.py. Tomorrow need to start scoring those early to estimate time

5/8/13

 - oovPhrases.py: wrote dev AND devtest oov phrases separately. 

 - composeTrans.py:
   - uses stemmed dictionary as well as phrase table to translate each word in a given source phrase (so there can be some mismatched phrase lengths b/c of PT translations)
   - So right now translations *just coming from bilingually extracted translations*, just to get an idea for how long scoring will take
   - Added induced translations as well; right now they're just not very good -> running now on both dev and test src phrases of interest
   - word alignments w/i phrase pairs are maintained and put into phrase table for lexical scoring as well
   - Scoring output phrase table here:
     - /home/hltcoe/airvine/ptRules/scoreTables <- w/o induced translations and for dev set only
@shreejit:probably something of interest. Not sure 
     - /home/hltcoe/airvine/ptRules/scoreTablesDevTest <- w/ induced translations and for dev AND test sets
         ->>>> When this is done, combine w/ monolingually scored bilingually-extracted phrase table and decode

 - reranking/featuresScore.py: 
   - Reranked dictionary for all spanish OOVs based on *crawls only data and w/ buggy contextual feature*:
       /home/hltcoe/airvine/ptRules/bli/sample2000/rerank_BestScoreAllPruneFewer10CrawlsOnlyBuggy
   - Need to rerun once all unigram scoring is done; and recompose with them too

 - Monolingually scoring bilingually extracted table:
   /home/hltcoe/airvine/distribRep/esPhraseExperiments/sample2000_PL3/monoScored
   ->>>> When this is done, convert back to new format and redecode
   ->>>> Also now available to combine with induced translations

5/9/13:
Phrase table induction stuff:
 - Running jobs:
   - I really *should-not* be scoring composed phrase pairs (or filtered) for reordering b/c will want to prune that space down much more, no need to score reordering now...
   - Unigram scoring still running: Wiki-context and Wiki-ED
   - Reranking on buggy unigram scoring still running: should be topk=100 instead of topk=500, which takes forever to write features...
   - Scoring bilingually extracted phrase table still scoring
 - Modified composeTrans.py quite a bit to allow for scoring testset phrases pairs set that I used to evaluate filtering strategies, also added a lot of text to paper about this compositional approach. Parallelized testing on test set: composeSplit.py
 - Realized should redo dtlearn w/ induced unigram translations instead of the be prior en-es dictionary. Want to not use that in experiments at all, keep it simple. If use big top-k from induced list, size of filtered set may increase. 
 - Trying to score composed table w/ sampling monolingual data: /home/hltcoe/airvine/ptRules/scoreTablesDevTestSample
   - Updated babel jar to sample for phrase pairs: babeljar_050913.jar

Measuring how much compositionality is in a given phrase table:
 - Uses lexical translations from the phrase table itself and the external lexical dictionary, AND stemmed dictionaries (src and target side) all around
 - compositionality.py
 - Right now running a job for Spanish

Meeting with CCB:
- Set max number of tokens observed for a given phrase
- Recall computed optimistically where stop words can be inserted at random on target English side translations
Predicting Compositionality:
- How many multi-word phrase pairs are compositional: write simple script for a given phrase table
- Foster and Roland Kuhn on phrase table pruning
- Lexical association features like chi-square
- moving to syntax model could define features over parse information too
-- Can predict compositionality
-- Throw away what's compositional, how does that affect bleu scores
-- Then if you induce translations for what's not compositional, how much can you improve
- Write better intro: break problem into its hard parts (d, n^2, gathering signatures) and then try to address each one
- LSH on d, for all signatures not just contextual, then PLEB for k-NN

 - Interesting HITs for Josh to design
  -  US newspapers: http://newspapermap.com
  -  GunFails: http://www.dailykos.com/story/2013/05/04/1205265/-GunFAIL-XVI
  -  Let's use web-crawling and MTurk to track gun violence in America. Apparently the CDC isnt' allowed to do that, so we will. All gun shots/violence: type, ages, fatal or not. Could get MTurk annotations and then try to automatically classify with different labels. Cooool.

5/10/13:
Phrase table induction:
 - Running jobs:
  - Wikipedia Context scoring on unigrams died; restarted it. That's the last feature that I need for getting the induced unigram dictionary.
 - Compositionality of phrase tables:
  - Added some flexibility to compositionality.py to allow for stemming or not, various external dictionaries or not, deletion of source phrase stop words or insertion of target phrase stop words or not. Computing percent of high-probability (>0.1 both phrasal translation probabilities) phrase pairs that are compositional using several settings. Added table to paper: table 16 right now. 
  - Need to run for phrase tables of other sizes as well. Looks like that will take a while. Parallelized!: 
      - Split phrase table entries that we care about are in phraseTablesParallel
      - Run all jobs from here: compositionalitySplit.py
      - Edit compositionality.py for three booleans as well as phrase table to read for constructing lexical dictionary, then run more jobs. Will have to change number of files for different phrase table splits (e.g. 2k, 20k, etc.)
  - Then: remove compositional phrases from PT and re-tune and decode: just read in these files that print what pairs are compositional and which aren't

5/13/13:
- Wrote testCompositionalityFast.py that is much faster and more memory efficient than before (doesn't pre-compute all permutations and combinations, just checks each one until a match is found).
- Throw away what's compositional in phrase table: removeComp.py description in paper. Don't get rid of phrases that also have non-compositional high probability translation(s). BLEU only went down 0.23, phrase table 29% smaller. Doing same thing for bigger (20k) corpora now. When that's done, run compositionality.py for 200k corpus too. That will be 1400 instead of 255 jobs...should run in batches
- Then compare with get rid of all phrases, just unigram translations: (/home/hltcoe/airvine/distribRep/esPhraseExperiments/sample2000_PL3/unigramOnly). Makes much smaller (63k -> 45 remove compositional; -> 14k remove all multi-word). There were many phrases before that simply didn't have high probability phrasal translations. 
- Then compare with just unigrams plus any src phrase that has a high probability non-compositional translation: /home/hltcoe/airvine/distribRep/esPhraseExperiments/sample2000_PL3/unigramNoncomp

5/14/13:
 - Siiiick.
 - Running compositionalitySplit for 200k corpus; 1400 jobs, running 100 at a time
 - Realized shouldn't be using that probability cutoff. Rerunning now w/ suffix "NoProbCutoff" -> on all phrase pairs in phrase table. Note that lexical dictionary was never using cutoff anyway, this is just about an additional set of phrase pairs. Overwriting all of these files w/ new compositionality indicators: NoStem.NoEx.NoStopID.notcompositional.test
 - Should probably use a *top-k* list of target translations. Ok, redoing now... using the product (log sum) of both p(e|f) scores to rank, and top-3
 - Rerunning remove-compositional mt experiments. 
 - Outputs here now: phraseTablesParallelTopK
 - Paper updated
 - Appended 2k PT with unigram induced translations for OOVs/low freq (modified featureScore.py slightly to do this easily like did for ind langs. this time no translit but could add plain words as translit file...): worktest_t_2k (up to top 10 if scores>1) and worktest_t_2k_b (only top 1 if score>1)
 - Changed worktest runs to also add identity and accent stripped versions of oovs: worktest_t_2k_c
 - Running 2k compositionality test using induced dictionary instead of david+mturk dict

5/15/13;
 - Made compositionality script much faster (limited search space to words definitely in one correct answer), but then Found bug that I introduced. All fixed. Results updated in paper. 
 - Want to run for indian languages too, but need phrase limit=3. Right now just getting those phrases from bigger table. Might not be a subset though? 
 - Running Indian languages baselines with phrase limit of 3

CCB Meeting:
 - WMT paper
 - Josh project idea: annotate US by hand, rest of world as MTurk hit perhaps, public health school collaboration mid-way through summer
 - New intro, new section on compositional phrase translations
 - Number of phrases in a phrase table that are compositional; a few MT experiments using those lists
 - Plane tickets on US airlines: buy cheaper, screenshot more expensive. check with laura graham. 

5/28/13:
MTMORPH:
 - Bird names segmentation
 - Translated notes from meeting with David into document as full text
 - Ran first jobs, quick and dirty: 
    - Test 1: as before
    - Test 2: got rid of common names with more than 2 words; declared minimum length for each morpheme in split source name to be 3 (and added option to alignIt.py)
    - Test 3: allow for no segmentation, and otherwise min morpheme length of 3: allowing for no segmentation gives same degenerate results of never segmenting as before
    - Test 4: allow for no segmentation, but added explicit null word in source for that: still degenerate
    -> What I really want to ensure is that alignment is always 1:1. I should implement this myself (instead of using GIZA) for maximal control
    - This led me astray on a long path to implementing 1:1 alignments myself, which aren't even quite working. Should take a break from that.
WMT-Submission:
 - Running experiments varying M, training frequency for which appending new translations. Started first tuning run on M=1,3,5,10 for all 6 languages.
 -> need to fix te, starting table doesn't exist for some reason.


5/29/13:
LRMT:
 - Working on WMT paper: added lots of results, varying M and K. Trying to flesh out paper to be legit long paper. 

5/30/13:
MT-MORPH:
 - Cleaning up script that does the work. 
 ->>>> Looks like GIZA is *not* weighting examples by scores. I really want it to. Figure out how to do that. 
 - Integrated home-grown aligner into alignIt.py script - just use in place of giza files. Write and read output files instead of giza files on each iteration, otherwise use same (working) structure. 
 - Can now use pairToScoreFormat{Backward,Forward,Homemade} translationProbs{Backward,Forward,Homemade}
 - Awesome: homemade aligner is working really well. Added option to config file to not run giza alignments at all, and also can run multiple iterations of homemade aligner each iteration. The advantage might be that I'm considering the original counts in the alignment or that I'm forcing a 1:1 alignment. Not sure. Either way, output looks pretty good.
 0530_birds_test1: 1 alignment iteration/segmentation iteration
 0530_birds_test2: 5 alignment iterations/segmentation iteration

LRMT:
 Read "Model With Minimal Translation Units, But Decode With Phrases"
 - Forces using phrases in an ngram translation model, in order to take advantage of both (ngram has advantage of considering longer context, removes duplicate derivations (fake ambiguities), and different handling of reordering). But the important part for me is that it articulates clearly why phrases are useful in PBMT even if they are, in theory, compositional. 


Meet with CCB:
1. WMT paper
2. Posters & 1 minute slides
3. David stuff
4. Paper that he sent w/ Alex Fraser
- WMT paper if time: Run experiment varying amount of monolingual text for Urdu or Hindi
- Poke Hal about analysis TACL paper
- More figures/examples on SRW paper
- Take SRW talk seriously: try to make it a clear, understandable overview of work
- Start adding example outputs to thesis document

6/3/13:
Meet with Alex:
 - Not much luck so far on learning networks
 - Still trying to make it work...

Compositionality:
 - 20 files of the 3008 haven't finished testing compositionality. It looks like they're all 3-word phrases that are composed of all stop words. Should update script to handle those differently in the future. If there's no content word to hinge on, the translations are almost limitless. Ok, so updated that to skip phrases if composed of all stop words. Running job that skips compositional phrases. Running compositionality testing using all tricks for other experiment that removes non-compositional phrases.
 - Next experiment:
     - Combine unigram model and compositional translations for up to trigrams. Try to recover performance of trigram model. 
     - allPhrases.py writes all bigrams and trigrams in dev and test sets that aren't punctuation/numbers and their frequencies: dev_test.bitrigrams
     - Then run composeTrans.py to write compositional translations for each
     - Issue: so many possible compositional translations!
     - Solution: 
       1. Write lexical translation probabilities based on other probabilistic dictionaries
       2. Score translation pairs using lots of features, predict 1/0. Threshold on what to include in table for decoding.
       ... or do #2 and then #1 later? -> do that. B/C estimated lexical probabilties aren't going to be comparable from different sources anyway. Should *only* use those from the existing unigram phrase table entries for the same feature space, don't combine. So can go ahead and add that as a feature for classification, plus others. And the former can carry over to writing the final output PT when the time comes


6/4/13:
Put together SRW talk slides
Finished two NAACL posters and got them printed at FedEx
Wrote yearly self-review

6/5/13:
MTMORPH:
- Realized need to allow all segmentations and *all alignments* as possibilities. Had unwittingly been throwing away all but 0-1 1-0 homemade alignments. 
- So changed things to iterate over all segmentation and alignment options together. Want to find the best combination. 
- I feel more in control with the homemade aligner now.
- Trying alternating between homemade forward and homemade backward alignments. 
- Experiments:
  - With Q-Score:
   - 0605_birds_test1 - alternate forward and backward
   - 0605_birds_test2 - forward only
   - 0605_birds_test3 - backward only
  - Without Q-Score:
   - 0605_birds_test4 - alternate forward and backward
   - 0605_birds_test5 - forward only
   - 0605_birds_test6 - backward only
  -> Take aways from above 6 experiments:
     - Backward only doesn't work. Very few segmentations.
     - Forward only is less commital in general: leaves probabilities spread out more because no other source of information pigeon-holing decisions into a single segmentation/alignment. That said, some do look good
     - Q-Score vs. P-Score: some differences with back and forth, but it's hard to say which is better. Perhaps a slight preference for p-score. But need quantitative comparison probably
   - 0605_birds_test7: P-Score, forward-backward alternation, no minMorphLength
     - Degenerate output
   - Is homemade aligner capable of doing all of the work actually? 

6/6/13:
MTMORPH:
 - checkHomemade : homemade w/ forward but w/o counts integrated throughout, 50 iterations
 - checkHomemade_wcounts : homemade w/ forward and counts, 50 iterations 
 - checkHomemade_backward : homemade w/ backward and counts (degenerate: all unaligned), 50 iterations
 - checkHomemade_wcounts_500 : homemade w/ forward and counts, 500 iterations
 - checkHomemade_wcounts_5000 : homemade w/ forward and counts, 5000 iterations; very little change up to 5k iterations
 - Decided that homemade aligner really is capable of doing all the work, the model I want is relatively simple. Modified the code and the write-up more clearly to keep it clean. Have two ideas for modification (fuzzy translation matching and interpolating with p(f) to encourage consistent segmentations even if translations are different). 
 - In any case, have ideas to keep working for now but would be nice to meet with david at some point. So comfortable position to be in for this project.
 
PHRASE COMP:
 - Cleaned up composeTrans.py, reading options and pointers from config now
 - Composing translations and writing features now
 - Need to optionally drop stop words from input phrase
 - Looks like actually will be able to do full monolingual scoring on all of composed phrases: ~50 per source phrase, 132k source phrases in dev and test sets = 7 million comparisons. Not bad. This is after pruning target phrases that never appear in monolingual data, so much better than before. 

CCB meeting
- self-evaluation: thesis committee member(s)
- posters printed; srw talk
- hal poked guys about TACL paper
- david project going well
- composing phrase translations: tie this back into the DT stuff when i can
- met with alex

- look into thesis awards (taus,amta)
- talk to david about thesis members: kevin/chris/hal and maybe rich. 

6/7/13:
LRMT:
- Started learning curves over monolingual data for Urdu. Scoring word pairs and phrase tables. 
  - Word pairs: run python monoTranslate.py ur 50 50 and just edit sampling rate w/i monoTranslate script. Outputs will go, e.g, here:
    crawls-BestScoreAllPruneFewer10_WMT_sample_0.02
  - Phrase tables:
    - Edit both .xml files: sampling rate and output directory (make sure to also make output directory) and then run runscorerC_Sample and runscorerW_Sample
  - Rates: 0.01, 0.02, 0.04, 0.08, 0.16, 0.32, 0.64
  - All done for word pairs and phrase tables


Meeting with Josh:
 - He'll be here July 2.

Meeting with David:
- DONE Write script to normalize endings of regular forms of second morphemes in words - at least give upper bound and give model better chance to have confidence in the data
- DONE intra-segmentation variation:
- only use pairs that do have reinforcement
- DONE get rid of single word common names? even if related probably have different morphology (e.g. albus)
- DONE throw away 1 and 2 counts
- DONE clean to noisy
- tradeoff between coverage and cleanliness
- DONE or reweight non-linearlly: e.g. count^2

6/17/13:
Birds:
- re-worked homemadeAlign.py script: 
  - allow for fuzzy matching 
  - read in a config file
  - interpolating between p(f|e) and p(f)
  - setting freq threshold for original data
  - ignoring target (English) words w/o hyphens

LRMT:
- Learning curves over monolingual data for Urdu: scoring all done
- Ran OOV reranking for all samples
- Converted all scored tables into new format
- Ran +Acc experiments for all samples
- Ran all +Trans and +Trans/Acc for three iterations


6/18/13:
Birds:
- Learned intra-segmentation variation (only learned -o and -i, but seems good enough) as well as word final alternations (pairs of suffixes). Right now all implemented as simple suffixes that can be fuzzy-matched with their stripped versions
- Added fuzzy matching to the M-Step, previously was only on the E-Step
- Examples of where this helps: castane{i,o}
- Updated mt-morph write-up
- Started playing with segmenting test data. Need to give it a much lower than 1 weight?
- Getting some nigr- prefixes in the mix in the test data output even for training data items! Need to keep that from happening... maybe don't allow fuzzy matches to happen w/ unstripped versions, just alternations: don't add to those counts, just count those counts in estimating probabilities -> solved by just using alternations
- Noticed only 139 multi-target words paired with src word w/ count of more than 1. But 27,256 w/ count of 1. 
- Only want alternations if they actually occur in data: added srcTrans lookup src stem+suf freq before returning as an alternation, but that makes leuco go to leuc...
   -> changed to pairs of alternations, not just list of suffixes

6/19/13:
Birds:
 - Fixed probabilities so that fuzzy matches are in equivalence class, always have same probability
 - Lots of tiny bug fixes/clean-ups...
 - Following behavior of leucopogon and nigriceps...
 - If the same source word has appeared somewhere else 

6/20/13:
BIRDS
Baseline: 0620_birds_CP4_Pen4_IC-2.0-0.00001_baseline/
Best working: 0620_birds_CP4_TK1000_Pen4_IC-1.0-0.00001/
Compare with: python compareTopAlign.py 0620_birds_CP4_TK1000_Pen4_IC-2.0-0.00001/Iteration405/splitout 0620_birds_CP4_Pen4_IC-2.0-0.00001_baseline/Iteration9/splitout
Look at top with: python extractTopAlign.py 0620_birds_CP4_Pen4_IC-2.0-0.00001_baseline/Iteration12/splitout  | less

- Not allowing cross alignments: 
-- Clean-to-noisy: 0620_birds_CP4_TK1000_Pen4_IC-1.0-0.00001_nocrossalign/
-- Baseline: 0620_birds_CP4_Pen4_IC-1.0-0.00001_nocrossalign_baseline/

Prep meeting with David:
- 1st try: throwing all data into a simple model that allowed for unsegmented and null-alignments, and the model was learning not to segment anything.
- 2nd try: forced 1:1 alignment, and it got a little better, still tons of noise
- 3rd try, backed off of that:
    - Back to model where any kind of segmentation and alignment is allowed:
       1. baseline just throws everything in, no changes, 
       2. baseline plus powering frequencies to trust high frequencies more than their linear counts, 
       3. modified model learns in clean->noisy iterations. 
    - Results:
       1. First Baseline still learns not to segment anything. 
       2. Second baseline w/ powered frequencies is much better
       3. Clean->Noisy iterations helps even more
- Also added a way to automatically learn a model of suffix alternations, both intra-segmental and word-final. 
- Added tons of options that are easily configured: frequency^power; interpolate with backoff model p(f|e) and p(f)<- just probability of segment sequence, ignoring e; top-k most confident added at each iteration; got rid of target words that can't be split into multiple words on hyphen (most of them garbage)

Meeting notes:
- SEGMENT AND TRANSLATE SCIENTIFIC NAMES IN TEST SET: 
  - Note: could also use this segmented to estimate the confidence in other data in the clean->noisy learning
  - Hold out test set: some percent, not so many that can manually label them potentially
  - Google translate unsegmented words: greek and latin, segmented and unsegmented. Oracle greek vs. latin and oracle-best segmentation
Do another pass of grouping equivalence classes:
  - leucura ||| white tailed good evidence that it should be put in EC w/ leuco and leuci: first choice segmentation for a word AND reasonably high translation probability for white
- What's the connonical form? Generous dose of inductive bias. 
   - Shortest, most frequent, smallest edit distance with all others
   - Check the box of this discussion: make it clear that I understand this is part of the problem
   - Or look at morphological literature: what do they suggest is the right way to handle connonicalization
   - What would latin person say is cononical form? -o? 
   - For nouns and adjectives this is more arbitrary than other word classes anyway. 
   - And what do you gain anyway by mapping EQ to connonical form?
   - Need context data anyway: out of the scope of this work. Requires free form latin corpus
   - Morphologists might want to fill out the full space of equivalence classes, even those that don't appear: 
   - In the MT setting: either prepopulate or generate on the fly. 
      - This might not be what morphologists would want 

Prep meeting with CCB:
- Interesting learning curve over comparable corpora for Urdu: matters much more for feature estimation than translation induction
- Hal on committee; jobs
- EMNLP: birds project with David

6/21/13:
- Reviewed TACL paper on using Wikipedia concepts for topic modeling


6/22/13:
BIRDS
Total refactored code to make it object oriented and much cleaner
    - Dataset object: includes lots of sets of pair-objects
    - Clear, concise E and M Steps
    - Original pair object: associated with set of possible aligned segmentations (split object, which contains its own method for initializing its alignments)
      - In E-Step: update these probabilities, and normalize over all possible segmentations
    - Two dictionaries for equivalence classes:
      - Dictionary to map from string to EQ
      - Dictionary to map from EQ to all of its members
- minMorphLength of 2 seems good
- Made confidence better: for a given src, iterate over all splits, average number of times seen each segment. Then sort original src's by this. Sort of like p(f), ignoring e. I'm just looking at counts b/c denominators are constant. Average is to encourage confidence if have seen one segment a lot but not the other at all.
  - Note that with this version, different sets are added for different models. So can't always compare Iterations299 exactly across experiments, must wait for final. 
Found bug late in the game of playing with parameters... need systematic way to evaluate different parmaeters!
 - Which alignments to allow in initialization (-more/all align) - all seems fine
 - Penalty (=8 seems good)
 - Interpolation weights (seems good when use both)
 - this one looks good so far, with new just downweighting count-1 items:
     - 0622_refactored_MML2_allaligns_bugfix_p8_IC22_newrecounting/
 
6/24/13:
Birds:
 - Decoding: assuming decoding in-order. So maybe should go back and only allow those alignments? Could make that an assumption throughout model
 - Generating top-k list and evaluating on # produced at all. Should also to % in top-1 and top-10 too probably
 - doing development on test9: choosing interpolation parameters, eachIteration chunk size, etc. 
 - Started writing paper, esp. related work section. Need to read a few papers in detail. And probably need to focus on dataset: pruning data to likely-to-need-segmentation?
DAMT:
 - Glanced at old paper. Going to run full Hansard MT experiments... already have tables, just need to merge and decode. Also need to generate lists of OOV words before merging. Shouldn't be too much work though.

6/25/13:
The following got 80 right and 33 in top-1: 
0624_MML2_allaligns_p8_IC_2-4_test9_its100_eachit10000/ 
0625_MML2_allaligns_p8_IC_2-8_test9_its100_maxall_fmodcount0.01/
--- not as good at only 50 iterations and not as good at 1000 iterations. 
- Paper: 
  - Wrote dataset and task section (might need to separate?)
DAMT MM:
 - Started baseline hansard-full jobs, got the filtered phrase tables (dev and test), generated .oov.withcounts files, supplemented using merge script. 
 - Just need to wait for science merge jobs to finish, then gzip those supp PTs, then start all 9 tuning/decoding runs
 - Ran all Hansard full jobs; all completed except for some reason segfault on one of the science jobs... probably going to ignore

6/26/13:
BIRDS:
 - Writing paper... the morphological section is tricky. Maybe just confess that did it by hand? Checking to see if i/o is the only one that really matters. Ok they do all matter. But going to just confess that I basically did it manually, on the dev set since there are so few.
 - Do I really need that interpolation crap at all? It's annoying to write about... trying without, just product of segmentation and translation probabilities! <- seems ok. spend all day re-writing the model section. I think it looks mostly ok now...

6/27/13:
BIRDS:
- w/ new model, looks like penalty of 1 is just fine, no need to use that parameter. Good, because don't want to explain it. 
- Might need more than 100 iterations of learning, trying that now...
- Updated script so it does testing on every iteration, not just the end, and prints the output to each iteration directory
- Script to list top-1 and top-10 accuracies across iterations done so far for a given output dir: python quickResults.py output-dir
- Finished writing model section. And it makes more sense than before, I think.
- Looks like the difference between the output now and yesterday (when closer to 30 correct in top-1) is that now I'm producing lots of nulls...can I penalize or eliminate those? I know already that my output will be two words long! So just ignore those options as they come up...? -> that's fair, I want to generate a literal translation for those identified as needing segmentation. ****Should add in paper somewhere that how to identify what needs segmentation for a test set is another problem... or we could just generate and compare later. 
- Settled on the following, which simplify discussion as well as work as well as anything else:
  - penalty=1
  - use all alignments
  - use sum of probabilities over all segmentations that produce a given translation in decoding, not just the single best
- Ran parameter tuning on alpha lambda grid; w/ and w/o BOWEOW

6/28/13:
BIRDS:
- dealt with decoding... it's good now. tried (p(f|e)*p(e) but couldn't estimate p(e) correctly
- added options for summing over segments vs top 1 and also excluding trans w/ first and second words not appearing as first or second words before (EF filter)

6/30/13:
- Finished manual annotation of test set
- Finished annotation of Google results on test set

7/1/13:
WMT paper: finalized camera-ready version, submitted and added to webpage
DAMT-MM:
 - Updated text to response to all of ACL reviewers' comments except for the one about the order in which we processed pages. Hope that no one thinks about it? 
 - Running doc-sim baseline for hansard-full condition. When that's done add to results in-paragraph at end of results section. Not running the rest of the baselines because I know that to be the strongest, and compute time is tight
 - It's at exactly 9 pages now. 
BIRDS:
 - Annotations so far on testData file (in terms of columns):
   - src, trg, freq, good/partial/bad pair (1,2,3), google-oracle-as-word, google-oracle-with-split, google-as-word-correct (0/1), google-as-split correct (0/1)
 - Got Google resuls: 1/77 and 5/77 on unsegmented and segmented input, added table to paper
 - Cleaned up some text in the paper
 - Using my learned model to decode test set items: /home/hltcoe/airvine/mtMorph/ScienceData/manualAnnotation/decode.py
 - Learning model to use for testing (train on 0-7 and 9, then test on 8)

7/2/13:
BIRDS:
 - Wrote first draft of introduction
 - Changed first/second target probs to be >0 only if ratio of when-appearing-in-first is at least 1/5 (if appears fewer than 1/5 times in second position, then make that freq 0)... prevents, e.g., "gray black" output
 - Josh working on annotations
 - Finished running experiments (I think) and started writing up results/analysis. Right now it's pretty dry, but I think it's ok to send that to David and ask him what he thinks the most interesting analyses are. 
 - Sent draft to David!

7/3/13:
BIRDS:
 - Got Kappa statistics
 - Updated paper to evaluate on test set that is the intersection of =1 annotations
 - Talked to David: biggest thing is another MT baseline. So running of-the-shelf Morfessor on training+dev+test data with counts, default parameters. Then Moses over that same data as train/dev/test. And I wonnnnnnnnn!!!

7/4/13:
BIRDS:
 - Finished complete draft. 

7/5/13:
BIRDS
 - Submitted EMNLP paper

7/8/13:
LRMT:
 - Getting head back into scoring composed phrase pairs
 - composeTrans.py reads a phrase table and a list of dev/test phrases and writes compositional translations along with some features 
 - Could score these using MERT/MIRA w/i MT learning or as a separate step

7/9/13:
LRMT:
 - Simply adding compositional phrases to old phrase table, plus some extra (easy) features:
   /home/hltcoe/airvine/distribRep/esPhraseExperiments/writeTables
   Running MT experiment now, just including extra features in phrase table instead of pruning before...
 - Running same experiment on 2,000 and 20,000:
   - For 20k experiment, should really re-score vocabulary here:
       /home/hltcoe/airvine/ptRules/bli/sample2000
@shreejit:have files been moves to this place? not sure what composition has to do with the phrase tables
       Using the 20k probabilisitic dictionary to project contextual vectors
       But since context scores don't matter that much anyway, skipping for now
   - Composing 20k phrase translations right now. When that's done, run same experiment as running for 2k right now (+ all compositional in PT)
   - Right now features are: 
       1. average forward lexical translation probability according to bilingually extracted phrase table
       2. '' according to bitext alignments-based probabilitistic dictionary
       3. '' according to dictionary of induced translations
       4. average backward lexical translation probability according to bitext alignments-based probabilitistic dictionary
       5. absolute value of difference between log-mono-freqs
       6. 2.718 if in phrase table, 1.0 otherwise
       7. 2.718 if there's no translation for src in phrase table, 1 if there is
    - Revised PT-writing script because some entries will be in both tables
 
Better:
  - Unigram only experiment where src and trg are BOTH unigrams only:
    - /home/hltcoe/airvine/distribRep/esPhraseExperiments/sample2000_PL3/unigramBothOnly
  - Now: supplement the unigram only table with composed translations, plus some extra (easy) features
     -> /export/projects/tto16/users/anni/compMTExps/es-en/sample2000_PL3_unigramPlus

7/10/13:
DISTRIB-REPS:
 - Processed latest crawls from Josh. 
 - Gathered data stats for Ashutosh and sent to him
LRMT:
 - Looks like unigram vs unigram+comp straight up experiment wasn't successful. Next up: fewer options: harsher pruning of stuff

7/11/13:
LRMT: 
 - Need to deal with optionally dropping stop words from input phrasese
 - This script combines an old table with a ".composed" output file and puts it in a new table:
    /home/hltcoe/airvine/distribRep/esPhraseExperiments/writeTables/oldPlusCompTable.py
 - Heuristic "top translation" for "oov phrases" (words appear no more than 5 times, phrases don't appear): /export/projects/tto16/users/anni/compMTExps/es-en/sample2000_PL3_unigramPlusOOVPhraseTopComp
 - Get top translation here: /export/projects/tto16/users/anni/compPP/getTop.py (top if src phrase doesn't appear in original PT, and then sort by 1. lexical trans probability, 2. log-freq diff)


Meeting with CCB:
- WMT presentation
- Alex's stuff
- Josh's stuff
- EMNLP papers

7/15/13:
LRMT:
Unigram + OOV top simple composed translations:
 - Got top translations of OOV phrases
 - Then combined with original tables: python oldPlusCompTable.py, and gzipeed
 - Then MT experiment running here: /export/projects/tto16/users/anni/compMTExps/es-en/sample2000_PL3_unigramPlusOOVPhraseTopComp
Scoring all OOV phrase composed translations:
 - /export/projects/tto16/users/anni/compPP/scoreTable

7/16/13:
DRMT:
 - Scoring new models:
    1. Get model(s) and lexicon files from dodo: right now 'system upgrade' and can't log in
    2. Score a phrase table: /home/hltcoe/airvine/distribRep/EventDistribReps/pteval.sh
       Note: can run ant to compile java source (just 'ant')
    3. Then compare scored phrase table with p(e|f) as upper bound and random lower bound: /home/hltcoe/airvine/distribRep/ptFeatureComparison/compFeats.py
    
LRMT:
 - Scoring Spanish composed phrases using 2% and 25% samples of comparable corpora just to have something to play with!
 - Updated composeTrans.py to make a composer object, so can import into other modules and use sample composer with a config file for setting it up
 - Wrote positive and *composed* negative phrase pairs and started jobs to score them monolingually too: /export/projects/tto16/users/anni/compPP/getSupervision/


7/17/13:
- Sponsor visit

7/18/13:
- Set up Hilary to do annotation
- Worked with Josh on his crawls and crawl analysis. Also Alchemy looks cool.
- LRMT:
  - In scoring, getting all NaNs on lexical scores... : because not enough word alignments? 
  - Set up scripts to do 10-fold evaluation of labeled data (from high probability phrase table translations and randomly selected incorrect compositional translations), using a linear classifier and VW
  - Right now just using features from crawls and frequency features
  - To write supervised data for training and evaluation based on a phrase table and a composer object: 
      /export/projects/tto16/users/anni/compPP/getSupervision/getSup.py
  - To use scored versions of supervision and other features to evaluate 10-fold:
     /export/projects/tto16/users/anni/compPP/getSupervision/classify.py composeTrans.config
     Config file for pointers to monolingual frequencies, stop word lists, etc. In near future should also use dictionaries to define features...
     Outputting .evaluate files for each held-out-test-fold as well as an overall confusion matrix
     Hardcoded 'threshold' to threshold classification scores for neg/pos prediction label; a way to tradeoff precision and recall

Meeting with CCB:
- Josh's stuff & plan for next few weeks
- Meeting at sponsor
- Classifying composed phrase translations setup

---> monolingual scores: get at what is likely to be compositional or not

7/19/13
- Tried to do scoring of Alex's new models, but it looks like output format is different and the PhraseSimilarityScorer fails in the pteval.sh run...
- Packed, cleaned, sweated.


8/26/13
- Babel stuff (composed translations)
 - Can't figure out why lexical translation features are all NaN
   - The reason is that, the way the collection is set up now, need to have lexical translations of all individual pairs of words in table too. 
 - Updated supervision file to include all word pairs for lexical translation scoring. Should fix all of those NaNs
   ----> Rescoring w/ wiki and crawls scorers
 - Composing translations and including lexical translations in the output, for spanish dev/test sets: /export/projects/tto16/users/anni/compPP
   ----> running now
 - Need to babel code on *macbook air* for new moses-style phrase table, in any case. Looks like it's up-to-date (generated 050913 jar). Need to make sure synced with repo and macbook pro
  - Maybe in class PhraseTable.java have option to convert back and forth? read in new format, save as old. And when printing output, convert old to new for printing... or not, could also just keep using conversion scripts that i have. 

8/27/13
 - Continuing CC scoring of supervision data as well as test data (oov phrases). 
 - Need gold standard for test phrases: Align and extract grammar from dev/test sets
   - Aligning concatenation of ALL es training data: train, dev, and devtest: /export/projects/tto16/users/anni/compPP/intrinsicResults/align
   - When that's done, use the extracted grammar to find gold translations of oov phrases. Intrinsically evaluate classification performance on the *dev* set. Could even use dev for choosing classification threshold, then apply to dev and test for knowing which phrases to supplement TM with.

8/29/13
- Compositionality stuff:
  - Got phrase table translations of interest from phrase table extracted from cat train+dev+devtest here: /export/projects/tto16/users/anni/compPP/intrinsicResults/gold
  - Looks like some phrases don't have good alignments, so no extracted good translation for them (e.g. alimentos conservados not aligned to "durable foodstuffs")
  - Played with just evaluting phrases using the most similar monolingual frequency. Will of course be able to do better

8/30/13
- Compositionality stuff:
  - Found a bug in the getSup.py script that was causing lots of null alignments and too many NaN lexical translation probabilities. Fixed, re-extracting supervision set. Then will rescore over the weekend. 
  - Rerunning Baseline: /export/projects/tto16/users/anni/compMTExps/es-en/s2000_PL3_baseline
  - Oracle experiment: supplement baseline with dev/test set OOV phrases extracted from full aligned set: /export/projects/tto16/users/anni/compMTExps/es-en/oracle
- Josh's Stuff:
  - He'll have datasets organized as follows by Sept 16: state/city/newspaper/list-of-articles-named-after-timestamp
  - He'll also work on using SVM model to test full test set, not just 1 instance
- BIRDS:
  - Meeting with David
  - Reframe task as glossing scientific names. Do human evaluation on non-filtered test set. Maybe have humans annotate model's 1-best and 2-best or more so that not just all correct
  - Do the other direction translation? Scientists may want to look up vernacular names.
  - Deal with subspecies data: place/person names -> latin. All on morphology here. David says he'll send me notes
  - For next meeting: generate top-k glosses. We'll look at them before deciding if they're ready for human eval
  - Submit to EACL or ACL: probably wait for ACL

9/4/13
EMNLP-MM
  - Working on camera ready version of the paper
  - Running experiments that drop OOV words from output. Expecting BLEU scores to go down (correct):
    /export/projects/tto16/users/anni/damt/moses_exp/
  - Spaced out paper; added paragraph about how much new domain text it would take to cover oov words and about less related languages
UMBC
  - Prepped probability lecture for thursday
  - Started prepping linguistics lecture for next tuesday. Just class questions and quiz to finish.

9/5/13
EMNLP-MM Camera Ready
  - Left to deal with: read through whole paper and make sure no typos, etc.
  - Worked on updating examples in Table 1
  - Thanks to these notes, found script for getting cherry picked examples here: /home/hltcoe/airvine/damt/marginalSolver/dataWade
  - Uses representative phrases, so should be good set of NTS words... got marche->walk instead of work. Best I could do. 

9/6/13
  - At COE all day doing training and going over presentation with Matt
  - Left to do: 1180 training and fixing classification stuff on presentation, and review it with rich wednesday afternoon at 3:30

9/9/13
Compositional stuff:
  - Have crawls and wiki scores for both supervision and oov phrase set; training classifier on supervised set and using that to score the oov phrase data
  - On supervised set, doesn't seem to matter much whether use minimum monolingual frequency down to 0 or keep at 50. Other variable is the threshold for which to keep pairs; impacts precision vs. recall tradeoff.
  - On older experiments, was testing what happened when removed non-compositional and compositional phrases from phrase table. That's kind of synthetic, but the point was that the compositional stuff matters a lot, even if composed from pieces that are already in the phrase table (?). 
  - Need to think about focus of the paper: the take-home could be (1) memorizing these larger phrases is important, (2) we can do that with brute-force search and rerank with a solid feature space (which is similar to bilingual lexicon induction techniques)
 - Realized missing lots of correct compositional phrases, e.g. "aprender ingles -> learn english" because monolingual frequencies were < 50. So rerunning oov compose with a minimum monolingual frequency of 10 instead of 50. Hoping for higher recall in total composed list this time. Also running for MMF 3. Don't think it'll take that long, really. Writing the output is the slowest part. Will have to rerun scoring though...
 -> Moving from 50 to 10 gets to 47.51% source have correct answer in compositions. Looks like the missing 50% is mostly due to lack of coverage in the target induced vocab. Actually, if we have 75% in the top-5 induced list, we'd expect only .75 * .75 to have good bigram translations; need *both words* to be correctly induced. so makes good sense. too bad so low, but if can get very good results on 50% of those phrases, then that might be pretty good.
 - rerun scoring on test set oovs -> running

9/10/13
Compositional stuff:
 - TO DO: add features: e.g. number of aligned words will probably help quite a bit. something about length differences too.

9/12/13
Compositional stuff:
  - Scoring MMF3 composed phrases now. Have some prelim results on MMF10 - recall about 50%, but that's a lower bound b/c some gold translations aren't accurate b/c of alignment errors. Precision looks pretty low. Need to add more features and try to improve that. Then do P-R curve.
  - Also started run of MMF3 composed phrases, which is about twice as big (3.5G file)

9/13/13
Compositional stuff:
 - Added more features: using stop word lists and word alignments
   (example results using MMF 10 and threshold on keeping translations = 0.4)
   - Before: precision on training data 97%, recall 61.38%
   - Adding length features (content, stop, and all): precision 96.8%, recall 58.62%
   - Adding percent of aligned src and trg words features: 96.91%, recall 58.26%
   - +logistic: precision 92.74, 69.68
 - Composing translations for MMF5 
 - Predicing on test set MMF10 with new features and logistic

9/18/13
LREC paper:
 - Wrote script to process data for a given state's papers and then another to extract number of words of data for each state and write that to an output file: /home/anni/joshCrawls/processNewspaperMapOut
 - Wrote R script to read the state-word-count file and make a US map with colors according to amount of data: /Users/anni/Dropbox/irvine-bigone/lrec-crawls/figures
 - Also wrote R script to make time series
 - Drafted LREC abstract
 - Running jobs with sports words too
Class:
 - Prepped Information Theory class

9/23/13:
UMBC class:
 - Prepped intro to CFG class: at first thought there was no material here but managed to turn it into a whole lecture. 2 weeks on parsing coming up. 
 - Did a bit of thinking about how to do following 2 weeks on unsupervised learning: forward-backward, EM, regular languages/FSTs, HMMs. Using a combination of Jason's stuff (ice cream chart), old slides (clustering), classic stuff (FST and regex examples), and dunno (HMMs)... 
 - After that will be machine learning-y stuff and then applications, including MT and IR
BIRDS project:
 - Opened up old paper, read through meeting notes from a few weeks ago. Generated top-k lists for all test set items as discussed (added parameter to hAlign.py script and decode.py)
 - The idea (from meeting before): make the paper about glossing these scientific names, learning a model from very noisy data. 
   - We have students annotated which are true glosses, and then we measure accuracy vs. those 'official glosses'
   - We have students annotate our output: good gloss or bad gloss: 
     	     :  cut -f 1-2 testData.allalign.it5 | less to see at least top-10 glosses for all words

9/24/13:
BIRDS:
 - Emailed David update; ready to annotated some output glosses once have human eval plan ready.
UMBC:
 - Taught intro to (P)CFGs

9/25/13
Compositional:
 - Fixed some feature functions that were broken. Now have 27 features, about the same ones that I used for the NAACL short lexicon induction paper. Function written in classify.py and can be called to get features for any arbitrary phrase pair (given alignments strings, etc.), so that's nice. Also not numbering them explicitly, so can add more super easily whenever I want.
 - On supervision data, over 90% recall AND precision, so that's really good, way higher recall than before. 
 - Updated paper just a tiny bit
 - Using high performing supervised model to score composed translations of set of interesting dev and test sets. Problem is that already only have about 50% recall, with minimum monolingual frequency of 3 (very small). Will probably need very high precision to get any good MT performance... but also note that that 50% recall includes some bad references, so may be conservative. But choosing about on average 20k compositional translations or something like that, so super big reranking space. 
 - We'll see. Jobs running scoring MMF3 and MMF10 sets, which have slightly different recalls (about 53% and about 47%), MMF3 of course a little higher but much bigger space to rerank

-BIRDS:
 - Meeting with David
   - Users in arbitrary language X, want to understand these names
   - % of preferred matches and acceptable matches
   - other direction - know how to describe, want to look it up in scientific book; or teaching aid for biologists learning about species names (language learning in this domain). cite numbers about the size of the community, number of biologists who might use this type of system
   - localize massive list of scientific names into, for example, estonian. better to be faithful to latin form than english form, pejorative to assume that we'll hinge through english. common MT task to want to translate directly from latin into estonian. from reviewers: so why aren't you doing this on spanish? we don't have data? so how will this work at all? 
   - Ok, so have Josh do annotation: for each bird and some long-ish number of glosses, mark all that are acceptable and all that are preferable. If none is good, can also enter another gloss. But FIRST have him just enter single best preferred gloss for some subset of test set items. Need to segment these into two sets for this: maybe generate for 30% and mark acceptability for all. Need enough overlap from generation set to be able to measure his consistency

9/26/13:
UMBC:
 - Made parsing slides awesomely pretty
Compositional:
 - Wrote script to filter output of predict.py:
     python filterDisplay.py oovComposedMMF3/composed.feats.filtered 10 2 
     - First number is a max top-k to keep
     - Second number is a threshold
     - Writes summary output statistics and output file with filtered translation pairs
     - Could do precision/recall curves... but on type or token level?
     - Also need to go ahead and add pairs to phrase table

9/27/13:
Meeting with CCB:
 - George Foster's paper on reducing size of phrase table; it was something about redundancy and compositionality
 - Remember to include Nikesh's stuff
 - Outline thesis
 - Look into lexical, e.g. chi square, stuff for identifying monolingually phrases that are compositional or not
 - Teach Josh about suffix arrays and dynamic suffix arrays: chile and pizza?

10/14/13:
LREC paper:
 - Continuing to get together. Deadline extension by 10 days, so nbd.
UMBC:
 - Worked on FST lecture alllll day.

10/30/13:
Domain scoring experiments: 
  - Getting better subset of new-domain-like pages (got rid of numbers!):
    On CLSP grid, here: /home/anni/damt/lm_classify_mono/ngramRecall
    - Run submit.py once for each of subs, emea, and science domains to get geometric mean of ngram recall on unigrams, bigrams, trigrams, and 4grams (submit runs wikiNR.sh which runs allWiki.py which uses overlap measure in ngram_recall.py)
    - When these jobs are all over, results will be in oct13_{science,emea,subs}
    - Will then need to just rank and create subdirectories containing only top X pages for each domain (on COE grid probably)

11/1/13:
Domain scoring experiments:
 - Have FR-EN pages ranked for all domains (geometric mean between 1,2,3,4-gram overlap between English article and English side of training data - all for emea and science, small for subs)
 - 637505 pages
 - Copying fr-en page pairs to COE from CLSP:
   anni@a02:/export/ws12/damt/data/pp/wikipedia/pagepairs$ scp -r 6/ airvine@external.hltcoe.jhu.edu:/export/common/data/corpora/comparable/wikipedia/pagepairs/en-fr/normpairs/
  - submitting qsub jobs to do this, see /export/ws12/damt/data/pp/wikipedia/pagepairs/cp.sh; only 4 at a time, submitting slowly
- Converting hansard-full tables to old format for scoring: /home/hltcoe/airvine/damt/moses_exp/hansard-newtestdev-{science,emea,subs}/
- Test scoring run of old-style table and subset of some science-related pages ready to go here: /export/projects/tto16/users/anni/damt/domainCCScoring/scoring

11/3/13:
Domain scoring experiments:
 - Finished copying all fr data to COE
 - Hansard-full jobs are converted to old
 - Successful Test scoring run: /export/projects/tto16/users/anni/damt/domainCCScoring/scoring  
 - Making top 50k, 100k, 200k, 300k subdirectories
 - When subdirectories are done (/export/ofs/projects/tto16/users/anni/damt/domainCCScoring/wikiSorted) go ahead and actually score using different subsets of pages in different domains

UMBC
 - Updated assignment 3 to make part 2 more clear
 - Worked on some slides about EM for Tuesday: they're almost done, then going to do spreadsheet too. Should be able to finish just on Tuesday all day


11/5/13:
Domain scoring experiments:
 - Subdirectories are done, started scoring using top50k page pairs
UMBC
 - Updated today's slides, added questions and quiz at the end. 

11/20/13:
* Domain scoring experiments:
 Done: Scoring and using PTs based on top 5k, 10k, 25k, 50k, and 100k science wikipedia pages
* Compositionality:
 - Working on getting my head back into this project, to be ready to resurface after class winds up (just 3 (?!) more lectures if Mike guest lectures!)
 - Composed files:
    oovComposedMMF*/composed written by composeTrans.py
    Right now the 'phrases of interest' are just oov phrases where each word also only appears a max of 5 times. This results in only about 1200 phrases, which isn't really enough to affect overall performance...
    UPDATES: 
        New list of phrases of interest here: /home/hltcoe/airvine/ptRules/toInduce_11_20_13
        -> Made this list *MUCH* bigger, and will go ahead and score it... filter set that you append to the phrase table LATER in this pipeline
        -> Also added all subphrases to list of OOV phrases (so if a-b-c-d in list, a and a-b-c and a-b and b-c, etc. will all also be in list)
	-> Got rid of requirement that words have some low frequency. Phrases that have stopwords in the middle we may still want to translate. Essentially now segmenting sentences using punctuation and names, and within those segments extracting all phrases that have some frequency (printing all w/ freq < 100 now, should go ahead and score all of those)
	-> Added some punctuation that wasn't in list before
        ----> Up to 4928 now
 - RUNNING: composing translations for those ~5k source phrases now. 
    -> Updated composeTrans.py script to get rid of nan's, where were a result of averaging over over lists that had null entries. should be better averages now, not just turning nan's into 0.0's
 - TO DO: score the much bigger list of phrases using wiki and crawls, and then evaluate ranked lists
 - predict.py does the following:
    - [optionally] Reads pl-crawls and pl-wiki-25% sample scores from phrase tables 
* BLI NAACL '13 paper data for Sravana: /export/projects/tto16/users/anni/bli_naacl13_data

11/21/13:
* WADE: emailed with UMD girl a bit about WADE. She wants to used alignments output from cdec instead of translation-details verbose output from Moses, of course. Told her to write the method herself for reading those. Also promised I'd work on the multiple references thing soon.
* Compositionality:
 -> Composing translations still running. Taking forever, composed just over 1200 in ~30 hours. Need to parallelize; could *easily* parallelize over src list; it doesn't really take that long to read in all of the initial files. Do this *asap*

11/25/13:
Compositionality:
 - Scoring oovComposedMMF3_11_20_13 composed table (it did finish in 50 hours, still at some point should parallelize)

12/3/13:
Josh Crawls:
- Running processor to update language crawls for all languages, so that I can share with Avneesh at CMU
 -> Nope, couldn't find any new data since August! Emailed Josh
 -> Before was running /home/anni/joshCrawls/processOut/process.sh which looks for data here, but none /export/a04/babel/Resources/joshCrawls/Cleaned/
- Processing US crawls as well right now for LREC paper, though acceptance isn't until January 31

12/4/13:

Josh Crawls:
 - He has to make directory of US newspaper dumps before I can process them for myself. So forget that.
 - He is working on fixing the all-languages crawls to get them cleaned

Compositionality:
 - Updated classify.py to return a list of feature names as well as features. Also updated classify.py to train a model on all 10 folds for use later
 - Reran supervised learning for use now: /export/projects/tto16/users/anni/compPP/getSupervision/sup_mmf3_thresh0.3_logistic_120413/train.all.model
 - Also updated predict.py to write list of features that it's using
 - Baseline result was 13.47 Rerunning again, will average over three tuning runs as usual
 - Oracle: before in oracle only added OOV phrase translations (and saw 14.27). Since I've expanded that set of src-phrases-of-interest, need a new oracle.
   - Ran getGold.py on new set of source phrases of interest: qsub -q text.q -cwd -l mem_free=20G,h_rt=50:00:00 -V getGold.py gold.config 
      - In this directory: /export/projects/tto16/users/anni/compPP/intrinsicResults/gold
      - Reran makeOracleTable.py for new oracle experiments here: /export/projects/tto16/users/anni/compMTExps/es-en/oracle
      -> Running corresponding new oracle MT experiment here: /export/projects/tto16/users/anni/compMTExps/es-en/oracle; so far tuning scores look excellent! great! over 3 bleu higher in tuning
 - Wiki scoring on composed table finished, so running classification step - predict.py
 - Also went ahead and started full wiki scoring on composed phrases
 - TO DO: work on paper! Intrinsic and End to end MT experiments when clsasification complete
 ---- OMG bug in extracting phrases, only had from dev set not test set sooooo rerunning that. then will need to re-compose and re-score and re-predict
 ---- Also: oracle includes ALL translations of src phrases of interest, not just compositional phrases :( Need to do a second oracle...
Parallelized composition.
 - First split srcphrasesofinterst into lists of 100: /home/hltcoe/airvine/ptRules/toInduce_12_4_13/parallelize.py
 - The wrote config file and ran compose job for each file: /export/projects/tto16/users/anni/compPP/cpConfig.sh
   - Then: GLOBIGNORE="*.cannottranslate"
   - And: cat parallel/composed.* > composed

12/5/13:
Compositionality:
 - Rerunning oracle: have two oracles, really: ALL translation for all src words of interest (what did really well yesterday) and only the composeable ones:
   - Remember: getting translations from phrase table extracted from aligned train-dev-test data 
   - Now output of getGold.py gives both: /export/projects/tto16/users/anni/compPP/intrinsicResults/gold
    - Running both: /export/projects/tto16/users/anni/compMTExps/es-en/oracle
    - FULL oracle: 16.52
    - Composeable oracle: 14.95
    - Baseline: 13.46
    - Result using FULL dataset: 23.40
    - Result using 20k lines: 18.49 <- find amount of parallel data that would need to get performance of our 'hack'
    - Up to 1.5 BLEU to be gotten by composing phrase translations
 - Parallel composition flew by overnight, running scoring now, when that's done run predict, which failed on the old set for some reason
 -* Comment: results might be better if I use a dataset with multiple references instead
 - IDEA: add features about which dictionaries the translations come from (induced or bilingual), will probably be useful, at least intuitive
 Trying another oracle: using bilingual dictionary only, not induced, see how much i'm getting from that because might be annoying to talk about in paper: 
    - composed here: oovComposedMMF3_noinduceddict_12_4_13_parallel
    - When done, another oracle experiments w/ those composed translations

12/6/13:
Compositionality:
 - Composed w/o induced dictionary:
     oovComposedMMF3_noinduceddict_12_4_13_parallel
     Running oracle experiment w/ composeable w/o induced dict: /export/projects/tto16/users/anni/compPP/intrinsicResults/gold/pt-srcphrasesofinterest-composeable-noinducedict
     - In tuning looks like only marginally worse than including induced dictionary
     - Composeable w/o induced oracle: 14.55 (0.4 worse than w/ induced, 1.1 above baseline)
     ->>>> Don't need to score these phrase pairs since they're a subset of the full composeable set that I'm scoring now. When those are done, just read in the files and print the subset
 - Added a few features to getFeatures that report what percent of alignments are in each of three dictionaries (induced, biling-regular, biling-stemmed); rerunning classify.py to get models for use later

12/7/13:
Compositionality:
 - Running predict.py w/ wiki-10 sample even though 25 will be done soon, to see if works ok 

12/9/13:
Compositionality:
 - Parallelized predict.py, writing features took too long. Running on same datasets that used for composition, up to file 96. Weirdly about 17 jobs stopped running w/o any output though. Investigate tomorrow...

12/10/13:
Compositionality:
 - Nodes keep dying randomly. Using dependencies on submitted jobs to only let 2 run at a time. Increase to 3 tonight if seems ok

12/12/13:
Compositionality:
 - Finally prediction is almost done on all featurized compositions. When that's done can move on to intrinsic (ranking eval) and extrinsic (MT) experiments
 - Worked on related work section of ACL paper

12/13/13:
Compositionality:
 - Predict finished, extracted lots of subsets w/ thresholds and top-ks defined. 
 - Generated precision-recall curve and put in ACL paper. Output files: /export/projects/tto16/users/anni/compPP/oovComposedMMF3_12_4_13_parallel/parallelsmall/concat/results
 - Worked on ACL paper a little
 - Wrote script to merge thresheld list and old phrase table, including getting alignments from original composed file. 
 - Started end-to-end MT job w/ translations from threshold=0.3 and topk-3 list
Meeting with CCB:
 - Good, good. Keep chugging along.

12/31/13:
Compositionality:
 - Ran lots more points for precision-recall curve with no threshold just top-k values. Add BLEU scores to that curve as they come in (just one data point now)
 - Started moses jobs of nofeats just added phrase pairs for 'interesting' points on precision-recall curve
 - Found that only 5584 phrases of the total 14165 phrases of interest have a correctly composeable translation in the set of all composed. This should have been the number added in the oracle composeable experiment
 - Scoring baseline table: /export/projects/tto16/users/anni/compMTExps/es-en/s2000_PL3_baseline/scoredTable
 - To do: MT experiments w/ added phrase pairs AND added features on both old and new phrase pairs: waiting for original PT to be monoingually scored: going to take a while, ugh...

1/2/14:
UMBC:
 - SUBMITTED GRADES!
Compositionality:
 - Worked on paper a little, send very rought draft to ccb
 - Running experiment w/ thresh=0.3, k=3,10 and an indicator feature as well as the classifier feature (vals 1.0 on old pairs); maybe will be able to better distinguish between top 3
 - 9623 source phrases of interest; made note of that in the paper

1/3/14:
Compositionality:
 - Added M threshold parameter to makeSuppTable.py
 - Here was only writing top translations for multiword phrases: oovComposedMMF3_12_4_13_parallel/parallelsmall/concat/Dec31out/
 - Updatd predict.py to do precision/recall only on multiword phrases but write all to output. About ~14k source phrases if including unigrams now
 - Updated makeSuppTable.py to read in all other information to write same set of features as were used in scoring compositional phrase pairs. 

1/4/14:
Compositionality:
 - Got makeSuppTable.py working with all 31 additional features. Running initial experiment now, and one w/ same set of phrase pairs but w just IndictCScore 2 features
 - Running makeSuppTable on lots of different M values and k values now too, and a predict.py job for k=100,thresh=0.3
 - Wrote script to remove all but unigrams from supplemental pairs (just check that feature); unigram baseline

1/5/14:
Compositionality:
 - Realized that wasn't thresholding unigrams supplemented w/ bilingual frequency, so added unigram biling frequency to file:
    /home/hltcoe/airvine/ptRules/toInduce_12_4_13/srcToInduce.DevAndTest.allunigrams.wfreq
 - Realized that oracle tables in motivation experiments didn't include unigrams. So rerunning gold extraction here for both composeable w/o induced dict and composeable w/ induced dict:
   /export/projects/tto16/users/anni/compPP/intrinsicResults/gold
 - FullOracle_WUnigrams:
   - Concatenated original PT with all pt-srcphrasesofinterest translations from oracle table for the src words of interest
   - Reordering table from old reordering table and average scores for all new phrase pairs
   - Rerunnning fulloracle experiment, winduced oracle, and w/o induced oracle: /export/projects/tto16/users/anni/compMTExps/es-en/oracle

1/6/14:
Compositionality:
 - Wrote script to score oracle table w/ additional features, when it runs, run that experiment
 - Realized that oracle translations shouldn't have bilingually estimated scores, so recombined those tables and restarted moses oracle experiments.
 - Wrote script to score oracle phrase pairs with all features: scoreOracleTable.py
 - Wrote script to merge prev no biling features tables with all features: mergeWFeats.py
 - Combined tables w/o biling scores and scored oracle tables and running three moses runs
 - New section in results spreadsheet with all oracle/baseline results

1/7/14:
Compositionality:
 - When I composed and scored all phrases, had just used M up to 100, so can't go higher than that now. Wanted to know how many phrases there are w/ biling freq > 100, so generated all here: /home/hltcoe/airvine/ptRules/toInduce_1_7_14
 - To look at difference between this list and the one used in experiments (12_4_13), run lookAtDiff.py
   Only 1515 total elements not included in the list that I did, and 1215 phrases. The UNIGRAMS not included look like mostly names and english words. ok good, so really including everything. Should eventually go back and score this list instead of 12_4_13 list, but it's ok for now
 - Generated just a single precision-recall curve w/ threshold at -inf essentially. Maybe just present those results w/ BLEU scores along entire thing at M=max and threshold=Min. Now only parameter is k in top-k which is nice and clean
 - Did lots of paper writing, looks like a paper now, just waiting to fill in some results and details. Sent to CCB

1/8/14:
Compositionality:
 - Running numbers on average number of composed translations for each bigram and trigram: compPP/composedDataStats.py
 - Lots of analysis edits to the paper, scripts for that are here, including per-sentence BLEU script: /export/projects/tto16/users/anni/compMTExps/es-en/analysis
 - OOPS. NAMES LIST INCLUDES WAY TOO MUCH STUFF. SHOULD BACK THAT OFF TO REALLY JUST INCLUDE NAMES SOMEHOW... it includes hacia, una, usar, todo, tambien, ... tons of stuff! 133k source phrases of interest w/o that list. Oops. Should rerun everything with that bigger list. Still truth in paper b/c excluding that name list, the name list is just way bigger than it should be...
 -> Produced new set of src phrases of interest to compose translations for: /home/hltcoe/airvine/ptRules/toInduce_1_8_14
    - Composing phrase translations for all. 
    - How do the unigram pairs get in the table for scoring? Need to make sure this happens ->>> This is written at the end by composeTrans.py, so no worries. Just make sure to unique concatenated composed files before scoring
    - Note that in PL scored tables there are six features, one carried over from 7 features written by composeTrans.py. Want the last five. Got it. classify.readPLTable accounts for this already. Good. 
    - Composed all in a couple of hours. Concatenated all and uniqued. That reduced size a bit, though Scoring would have only scored unique phrase pairs anyway. 
    - 78610841 in oovComposedMMF20_1_8_14_parallel/composed; actually about the same size as oovComposedMMF3_12_4_13_parallel/composed (77181331) b/c had adjusted monolingual frequency cutoff. So many scoring will finish in a reasonable time, if jobs start... oh, but still have to extract and write features, which does take a while. could do MT w/ only 10 tuning iterations or something. we'll see. would be nice to get an idea of what output will look like anyway
    - started scoring jobs. 

1/9/14:
Compositionality: 
 - Wikipedia and Crawls scoring is going on the new list of composed translations (MMF of 20, but not excluding names list). Have come to terms with this not finishing by deadline. It's ok, can still submit paper and see what happens. 
 - Running oracle on new set of full phrase pairs just to see difference: Generated oracle pt-srcphrasesofinterest lists. Scored lists using all features.
 - justPR.py to read phrase table or list of phrases and compare with oracle gold/pt-srcphrasesofinterest and print precision and recall. The baseline DOES have some of the correct answers because translated those w/ frequency up to 100. Using this to confirm all others (had underestimed before b/c not all src phrases have gold oracle translation extracted b/c of word alignment errors I guess)
 - Running k=1 and k=25 jobs w/o features now to make some statement in results about how it's the features that are letting us distinguish between good and bad phrase pairs. Hopefully results are worse than they are w/o features
 - writing, writing

1/10/14:
Compositionality:
 - Writing, writing
 - Submitted paper

1/13/14:
Crawls:
 - Updated language dictionary here: /home/anni/joshCrawls/processOut
 - Running process.sh to output new data from Josh
 - Stuck to Wikipedia language codes (http://meta.wikimedia.org/wiki/List_of_Wikipedias) when possible, otherwise made up things.
 - Problem now w/ multiple directories w/ different versions of names of languages (e.g. Spanish and Espanol...) and only writing output for each day once, so losing some data. Update or do I care? -> Changed names to _2, _3, etc. Ask Josh to fix up. In the meantime, concat when copy over to COE?

Admissions:
 - Reviewed all applications. Dazone with that.

1/14/14:
Compositionality:
 - Writing features for new set of composed phrases
 - Split into 6 parallel sets now instead of 500+: oovComposedMMF20_1_8_14_parallel/parallelsmaller/
   cat oovComposedMMF20_1_8_14_parallel/parallelsmall/composed.0 oovComposedMMF20_1_8_14_parallel/parallelsmall/composed.1* > oovComposedMMF20_1_8_14_parallel/parallelsmaller/composed.1
   cat oovComposedMMF20_1_8_14_parallel/parallelsmall/composed.2* > oovComposedMMF20_1_8_14_parallel/parallelsmaller/composed.2
   cat oovComposedMMF20_1_8_14_parallel/parallelsmall/composed.3* > oovComposedMMF20_1_8_14_parallel/parallelsmaller/composed.3
   cat oovComposedMMF20_1_8_14_parallel/parallelsmall/composed.4* > oovComposedMMF20_1_8_14_parallel/parallelsmaller/composed.4
   cat oovComposedMMF20_1_8_14_parallel/parallelsmall/composed.5* > oovComposedMMF20_1_8_14_parallel/parallelsmaller/composed.5
   cat oovComposedMMF20_1_8_14_parallel/parallelsmall/composed.6* oovComposedMMF20_1_8_14_parallel/parallelsmall/composed.7* oovComposedMMF20_1_8_14_parallel/parallelsmall/composed.8* oovComposedMMF20_1_8_14_parallel/parallelsmall/composed.9* > oovComposedMMF20_1_8_14_parallel/parallelsmaller/composed.6
 - Then ran this: cpConfigPredict.sh
 - Also running wiki full scoring b/c would be nice to have, may not finish
 
1/15/14
Compositionality:
 - Changed MMF to 20 for supervision extraction
 - getSup.py needed to write output lexical pairs for wiki and crawls monolingual scoring, but shouldn't include the lexical items in learning the model for prediction.
 - Modified getSup.py to not only write lexcial items w/i composed pairs as positive supervision but also random pairs as negative supervision.
 - Scoring w/ crawls and wiki

1/16/14:
Crawls Cleaning:
 - Fixed up script to recognize English (stupidly, using just % of unigrams that appear in En-Wikipedia more than 200 times), fixed other checkers that recognize HTML garbage, etc. Output looks good for a handful of checked languages: ht, ast, id, so, pt, ja
 - Running cleaning part of processing script for all languages now. Josh is still fixing up language names and dates. Could parallelize later if decide it's too slow, but shouldn't need to run from scratch very often (it checks if files already exist and doesn't reprocess those)
 - Will also need to run the rest of the script (commented out now) that does the date matching w/ english. Want to get language names sorted out before doing that. 

Wikipedia data:
 - Wikipedia only allows 2 dumps to download at once, so working through them slowly for all crawls languages as well here: ./download.pl x y where x and y are ranges of languages in languages file to process at once
 - Then unzip: ./unzip.pl 0 65
 - Then directions here: /export/a02/anni/wikitools/wikiextractor/tools

1/17/14:
Meeting with CCB:
- Manual gold word alignments for Spanish-English (+ one Indian language) to make oracle experiments cleaner. CCB sent HIT, prep data w/i sandbox to post and then get login information from him
- Computational Linguistics journal article
- Get thorough outline of thesis together and meet with all committee members about it
- Write at least a couple of hours everyday!
- We'll meet every other week, Fridays at 2pm


1/21/14:
Worked on thesis: 
 - Updated outline a bit
 - wrote intro to data section and parallel corpus section. Remade plot of number of speakers of languages vs. size of available parallel corpora. 

1/22/14:
Thesis:
Wrote bilingual dictionaries section and added new, clean table with information from ALL dictionaries.
David's dictionaries organized by language and script, and combined here: /home/hltcoe/airvine/Resources/Dictionaries/davidCombined
 - Extracting Wikipedia data for all languages using /export/a02/anni/wikitools/wikiextractor/tools/extractAll.pl languages_test /export/a02/wiki/dumps/ /export/a02/wiki/
 -> had to update java pointer in extract.sh to just default instead of JAVA_HOME, don't know what happened to that install...

1/23/14:
Thesis:
 - Wikipedia data: only got about 2k extracted pages for Danish, which isn't enough according to the Wikipedia site, which says there are 184,972 pages. Dumps don't look cutoff. What is the discrepancy?
 -> Problem is that Wikipedia is changing the way they get those links for display. Documentation here: http://lists.wikimedia.org/pipermail/xmldatadumps-l/2013-March/000711.html
 - Solution is going to be to write files with foreign language names:
 Running now: perl extractAll.pl languages_011614 /export/a02/wiki/dumps/ /export/a02/wiki/
 From: /export/a02/anni/wikitools/wikiextractor/tools
SDL: 130k + 13% bonus and 3% 401k matching offered

1/24/14:
Thesis:
 - Worked on Wikipedia processing: generating files w/ original names, and processing sql English pages file to get page ids
 - WARNING: commented out escaped characters in Moses tokenization perl script on clsp machines!
 - Copied over lots of special characters to escape in processing language crawls from here: http://www.thesauruslex.com/typo/eng/enghtml.htm

1/25/14:
Getting Wikipedia interwikilinks:
  sed s'/),(/\n/'g enwiki-latest-page.sql > enwiki-latest-page.sql.formatted
  delete headers and footers
  sed s"/INSERT\ INTO\ \`page\`\ VALUES\ (//"g enwiki-latest-page.sql.formatted  > enwiki-latest-page.sql.formatted2
  sed "s/);$//g" enwiki-latest-page.sql.formatted2 > enwiki-latest-page.sql.formatted

  Wrote script to process sql.formatted file:
    /export/a04/anni/wiki/readEnWikiTitles.py
    Needs a lot of memory: like 30g
  Finished script to use interwiki-links dumps, extracted new page directories, and big sql file of english titles and page ids to copy linked pages to new directory
    /export/a02/anni/wikitools/wikiextractor/tools/pairWEnglish.py
    -> Probably going to be slow to process so much data for all languages...
    Warning: English titles page has underscores in names, all other files have spaces. Had to write regex to fix this in pairWEnglish.py
    Then problems with whitespace in names too...
    Also does sentence splitting, tokenization, and lowercasing very similarly as is done for web crawls
  Updating all files for all other languages now as well, while I'm at it. Should be much more data now than 4 (?!) years ago. Downloaded and unzipped files, extracting them now to same dir as others: /export/a02/wiki/pages-originaltitles/. Then match w/ english 
   -> pairWEnglish.py is almost right! Then should run it for ALL extracted languages and NEW English extracted data!

LREC:
 - Waiting for Josh to make new dump of newspapermap crawls similar to /export/a12/jlangfus/10_20_2013_corpus/

1/26/14:
 - pairWEnglish.py is really right now except for slashes in file names, rerunning now w/ added replace to get rid of them...
 - Exact same number of ha and lg pages (117)? Maybe coincidence. Running on ti to double check. 

1/27/14:
New EN Wikipedia data extracted
Running pairing jobs for ha, lg, and ti, as well as a-mn. Continue to start but not too many at once b/c writing so much data to a02

1/28/14:
Thesis:
 - Wrote script to append lines in duplicate language directories to a single directory: /export/a04/babel/Resources/Crawls/joshOut_012414/append.py 
   - First arg is directory that you want to be final, and second is directory that you want supplemented into the first
 - Crawls processing doing 2-3 pages per day for big languages (English, French, German, Spanish, Sindhi)... each file is tens of thousands of lines long after processing, only processing 2-3 files per day! Going to take enormous amounts of time. Should parallelize by timestamp for these languages. 
 - Analysis section: worked on writing setup for low resource TETRA and WADE experiments. Started getting together experiment directories for four languages (bn, hi, es, and ur) here:
    /export/projects/tto16/users/anni/s4Analysis/
    Datasets sorted out and described in detail in the thesis
    Declared that will do WADE for each sentence w/ the best matching reference at the sentence level, not combining multiple reference translations at the subsentential level, which is possible but wouldn't be trivial to do correctly
    Translation details w/ multiple references isn't a problem b/c just writing single decode of input sentences. Will want to use that script that I wrote before to get BLEU scores for each reference and choose the best
    Running 1k, 2k, 4k, and 8k LR models for Bengali
    Getting automatic word alignments on the dev and test sets here: 
      /export/projects/tto16/users/anni/s4Analysis/Data/bn/alignAll/alignmentJob 
    Alignment error b/c of sentence length ratio in dev and test sets. Fix with help of this old script, rerun baselines, rerun alignments...:
       /home/anni/data/gizaclean-indian-parallel-corpora

1/29/14:
 Alignments for train+dev+devtest for bn, ur, hi, es: running all four jobs at: /export/projects/tto16/users/anni/s4Analysis/Data/$lang/alignAll/alignmentJob
 WADE test run:
   Get WADE inputs script: /export/projects/tto16/users/anni/s4Analysis/bn/baseline_1k
   Have WADE results for BN 1k, 2k, 4k, and 8k.
 PLAN: Going to run for ES for 1k through full, doubling training data each run. Can then plot how WADE categories shift (% on Y-axis) w/ training data amounts (on X-axis, probably log)
   ES problem w/ empty lines: cleaned all training data (removed about 4k of 1.7 million lines), then sampled from that. Did same for ur. Didn't have problem w/ hi and bn. Did have to clean dev/devtest for aligning all jobs). 
   Running now: es-alignall, hi-alignall, ur-alignall, es-1k, es-2k, es-4k, es-8k, es-16k, hi-1k, ur-1k

Jobs:
 -Rejected APL, MITRE, and SDL-govt

1/30/14:
Thesis:
 - Got WADE visualizations for Bengali working. Updated viz.py to ignore sentences longer than a given length in the wade config file. Also added config spec for the max number of sentence triples in each .html file
 - Added viz to thesis and some text
 - Plotted change in WADE aggregate stats for Bengali 1k, 2k, 4k, 8k; added to thesis
 - Waiting for es-align job to finish to run WADE on es
 - Wrote TETRA.py script for making +seen, +sense, and +score tables that takes four input pointers: LR phrase table, LR reordering table, HR phrase table, HR reordering table and writes output +seen, +sense, and +score both pt and reordering tables. Running +seen BN experiment now to make sure that everything checks out. Supplemented 1k model w/ 8k model. *SHOULD USE FULL MODELS FOR HR MODELS in the future, just checking functionality now
 - Wrote script to write output CSV file for MTurk manual alignments:
     /export/projects/tto16/users/anni/s4Analysis/manualAlignments/writeMTurkCSV.py
     Uses GDF alignments, GIZA forward, GIZA backward, and writes output files w/ source, target, sure (intersection) alignments, possible (GDF) alignments, unaligned src word indices, and unaligned trg word indices
     Can change sentence length w/i script
     -> Hindi is generated but only w/ best refs from 8k set, not full. Should use WADE best refs from full instead
     -> Can't generate Spanish until have allAlign/alignmentJob done for devtest alignments


1/31/14:
Wikipedia data processing: looks like a lot of data, but there is a lot of English in the foreign language pages. SHOULD IDENTIFY THOSE PAGES AND IGNORE IN PROCESSING LATER
Running lots of TETRA+WADE jobs
qsub -q text.q -cwd -l mem_free=10G,h_rt=4:00:00 -V wadeRunAll.sh

Meet with CCB:
* MTurk manual alignments:
 - Max sentence length: < 25 words
 - double check pink lines clicking
 - ccb has script for visualizing comparison of outputs with turker alignments
 - first open to everyone and then spot check; give personal qualification to some workers
 - check out och's paper on manual word alignment comparison 
 - two annotations per sentence
 - Pricing: $0.50
 Log in information:
 - callisonburch@gmail.com
 - texted mturk password (first)
 - correspondence via gmail (second)
-> 
 - Spanish and Hindi - will at the very least use for WADE analysis throughout experiments, possible other things as well
 -> Grab Josh's code for paper in github repo
- Local news corpus / newspaper map corpus
  - Plos paper on gender/age differences, could apply to states or something

2/1/14:
 Added some TETRA and WADE results to paper

2/2/14:
 Running datastats.sh for all states in Josh's crawl

2/3/14:
LREC:
 - Regenerating plots from in submitted paper.
 - Taking a long time to run topics.sh and topicsTime.sh...
Thesis - Analysis Chapter:
 - Reorged Chapter 2: moved big tables to appendix. Wrote narrative for WADE aggregate and TETRA results. Just need to fill in Spanish TETRA and all processed data and then much later linguistic section
 - Bad tokenization of te and ta, trying to find perl 5.10 to use...
 - Ugh... instead just copying to COE to run there. 
 - EN submitting jobs to copy: /export/a02/wiki/ 
 - Script for identying English pages in Wikipedia data before copying to COE:
    /export/a02/wiki/coecopy.py
Thesis - Domain Scoring Experiments:
 Creating directories of topk page pairs for remaining science, subs, and emea: /export/projects/tto16/users/anni/damt/domainCCScoring/wikiSorted
 Started science scoring jobs for 200k and 300k  
 Start scoring for other domains once page pairs directories are written
 Started EMEA baseline to get translation-details

2/4/14:
Thesis:
 RESTARTING ALL WIKIPEDIA copying... just running pairing on CLSP machines, then going to copy over to COE paired directories. Then do tokenization, removing english pages, etc. on COE.
 Pairing is super fast. What was taking so long before was 2 second timeouts after each perl script ran
 Copied TE and TA and started BN
 -> Running pairing for all languages in dictionaries table
 -> Testing EN-identificiation processing on TE 

2/5/14:
English ID on Telugu looks like it's working well. So will use those nonengpages directories.
 - Continuing to copy over paired language directories: 
   /export/a02/wiki/pagepairs_new/coecp.sh

 - When copied, process on COE machines: 
   /export/common/data/corpora/comparable/wikipedia/pagepairs_2014/pagepairs_new/process.sh
   qsub -cwd -q text.q -l mem_free=5G,h_rt=199:00:00 -V process.sh bn

Finishing TETRA and WADE analysis on Spanish:
 qsub -cwd -q text.q -l mem_free=5G,h_rt=12:00:00 -V wadeRunAll.sh
 -> Spanish +seen and +sense TETRA mt jobs running

2/6/14:
 TETRA scoring all done. Wrapped up writing in that analysis section. Just need manual word alignments to finish a couple points there (how WADE analysis changes and just note how they contribute to sense errors... measure automatic alignment performance vs. those, f-measure)

DAMT scoring: Generated lists with 10, 250, 500, and 1000 word cutoff instead of 100:
    /home/anni/damt/lm_classify_mono/ngramRecall
  The lists with bigger lenth limits look much better...

LREC paper: added a few notes, formatted, looks nice... sent prepub emails

Writing:
 - Wrote up some stuff for DAMT section. Added table w/ top-10 ranked pages for each subdomain

2/7/14:
Met with Boyan: 4 hours of Indonesian or Somali translator
Full French Wikipedia scoring is dying because of some bad pagenames. Deleting one by one? Ugh.
Realized Wikipedia extraction was from the wrong dump file. Also running for all of Ellie's dictionaries. So rerunning tons of wiki jobs now...
Wrote script to generate easy-ish to browse top-k ranked lists of induced translations for a given language, given .scored output files and final ranked files:
 /home/hltcoe/airvine/inducePhraseTable/LIMT/Experiments/LexInductExps/getExamples.py
@shreejit:this seems relevant. Huge section between previous comment and this had me lost. Multiple papers I guess

2/10/14:
Manually cleaned up mturk.gu dictionary really well in prep for BLI experiments for Boyan: /home/hltcoe/airvine/Resources/Dictionaries/MTurkDicts/mturk.gu
 -> used cleanSplitDict.py w/ a few different splitting characters to really clean it up

EMEA domain experiments: realized that wasn't using .short and .unseen test set, was using full set instead. Also was using full set for tuning, which is why it took forever. 
  -> Rerunning baseline (from which to get WADE results too), and a few scoring experiments

Script to get rid of <p> in Wikipedia output: 
  /export/common/data/corpora/comparable/wikipedia/pagepairs_2014/pagepairs_new/rmemptylines.sh
  Usage: ./rmemptylines.sh lang/
  Haven't run at all though languages; do after all copying/processing done

Script to get page title pairs for any language: /export/a02/anni/wikitools/wikiextractor/tools
  python getPageTitlePairs.py gu
  qsub -cwd -l mem_free=10g,h_vmem=10g -V getPageTitlePairs.sh it
  Ran for all languages

Setting up translit module:
 - /export/common/data/corpora/comparable/wikipedia/pagepairs_2014/pagepairs_new/lettercounts.sh  to get list of all lang and en words in wikipedia data
 - /export/projects/tto16/users/anni/translit/data/getData.py
@shreejit:generates translit dictionary which is used late ron
   - writes training, dev, and test sets for input language
   - English pages that include the phrase "born in" and "is located" included in translit learning pairs
   - test data is all words in specified dictionaries AND lang wikipedia
   - LM data is all english words in wikipedia set
   - learn moses model and decode all of these words
   -> Learning GU model here: /export/projects/tto16/users/anni/translit/data/gu
   -> then reconcile output and make a translit dictionary for use in BLI experiments

2/11/14:
Processing Josh's crawls: 
 - Without tokenization: ta, te, ml, hi, bn, gu, si, ne, mr
 - All of the rest w/ tokenization
 - Getting top 100k pages w/ length limit 1k for all three domains
 -> Spent all day fixing html codes in process.py omg...

2/12/14
 Reconciled translit output for gu: /export/projects/tto16/users/anni/translit/data/reconcile.py
 Copied all page title files for future translit jobs here: /export/common/data/corpora/comparable/wikipedia/pagepairs_2014/pagepairs_new/titlePairs
 Trying to identify and delete empty French wikipedia files w/ bad filenames that are causing scoring jobs failure: /export/projects/tto16/users/anni/damt/domainCCScoring/scoring/clean.py
 Simplified BLI framework here: /export/projects/tto16/users/anni/BLI
    - Getting top 1000 candidates for all source words in the dictionary
    - Thoroughly semi-manually cleaned up vi and ceb mturk dictionaries. Will need to do this for all, they're pretty messy...
    - Running for: gu-both, ceb-crawls, vi crawls (need vi (hasn't copied yet)  and ceb (processing slowly) wikipedia data) 

2/14/14
 - Reviewed ACL papers
 - Meeting with CCB:
   - MTurk: post alignment HITs: sent scripts
   - Datasets all (almost) in place. Huge table with over 150 languages
       - language id at sentence level
       - high precision set as well using python plugin at sentence level: when agree keep that data
       - Also Paul McNamee's language ID tool: in email 
   - Updated LREC paper
       - release individually. Need to zip up files
       - email CCB current draft TODAY
   - Will get ACL paper reviews next week
   - Spent some time writing script to clean up MTurk dictionaries (a lot of source English words for some languages b/c no lang id on Wikipedia)
       - Be in touch with Ellie about these dictionaries
   - Running Gujarati, Cebuano, and Vietnamese BLI experiments for Boyan (required getting translit set up, and data for translit
   - DAMT scoring experiments: slight improvement. Just thought of using English data for additional LM too, should help, can still fit cleanly into short paper
       - Add domain-specific LM too
       - Do significance testing too
       - Write!

2/17/14: 
 BLI:
  - Getting reranking working on new data:
    - python rerank.py ceb test/ rerankll.config 
@shreejit:this is is anni/BLI
    - Getting wiki freq files for languages in /export/projects/tto16/users/anni/BLI/wikiFreqs/
    - Add some features! Now's a good time. Something about burstiness and any other info theory metrics. Then could do analysis on whether better at predicting bursty words or others! Use features to do that type of analysis
 DAMT:
  - looks like science and emea 1k-50k moses jobs are tuning ok; not hurting performance anyway...
  - Need to modify scoring babel code to read a list of files instead of all files in a directory! Would make all of this DAMT stuff easier to run
  - Add more features to phrase tables here too? Burstiness types of features?
  -> Wrote some in paper in this section
 Crawls lang id: 
  - Using Paul McNamee's python module
  - Testing line-by-line language id for ZU: /export/common/data/corpora/comparable/crawls/langid.sh
  - qsub -cwd -q text.q -l mem_free=25G,h_rt=199:00:00 -V langid.sh zu

2/18/14:
Language ID on crawls:
 - More playing with language id; installed cld python language id package: https://github.com/mzsanford/cld
 - Doing high precision set as EITHER lang id matches given
 - Wrote this section up
Significance Testing:
 - Got package from http://www.ark.cs.cmu.edu/MT/
 - Not working with perl version on the COE cluster, which is annoying. Does work on my laptop. Needs sgm files though.
 - Back to jonathan clark's (MUCH!) easier-to-use system
 - Bash script for running significance testing here:
   - /export/projects/tto16/users/anni/damt/domainCCScoring/useScoredPhraseTables/emea.100kscores/sigtest.sh
   - Need to run three runs of optimizer for all baselines and hypotheses for this to be useful!

2/19/14:
Burstiness, etc.:
  - Thought about adding this to babel code, but it's fundamentally different: these metrics (like frequency) are computed totally language-independently. Then they can just be subtracted or something for writing features. But it's not like there's a signature that we're collecting and then doing a fancy (e.g. cosine similarity) comparison . They're really more monolingual features and less comparable corpora features. But still should estimate them on comparable corpora I guess to hope to get the best correspondance. But it's ok to do w/i python feature writing code. The only risk is that it'll take a while. 
  ----> Pre-writing files for all Wiki/Crawls files that have word freqs: /export/common/data/corpora/comparable/wikipedia/pagepairs_2014/pagepairs_new/wordFreqsByPage.py
  - Should also compute edit distance on the fly in rerank.py? This would take a while and it already takes a while! Need to do profiling to see why so slow...  and parallelize probably no matter what.
  - Profiled what was taking so long in that script and it was all of the write statements, which is in avoidable in writing so many different files. But I updated rerank.py to avoid so many write statements and changed writeFeatures to returnFeatures, one write statement per pair instead of per feature
  - Also .startswith comparison for identity feature was really slow. Changed that to just == string comparison. Not really sure why I'd done anything else before... that was an order of magnitude slower than anything else
  ----> Precomputing burstiness, IDF, variance, entropy, raw freq in burstiness.py from precomputed per-file frequency files
  - So for WORDS this is taken care of. Pre-compute burstiness for phrases in the same way? Maybe. Not for 150 languages though, omg.
  - TO DO: Use the above in rerank.py
BABEL CODE:
 - Modified some things to allow for reading a sublist of files from a single file instead of a directory of existing files, so don't have to crazily copy domain-like subsets all over the place
 - Still iterates through all files in directory structure and checks if they're in the set of pages of interest, so won't make scoring any faster (if anything, slower, since iterating through bit list), but it will avoid the copying-files step
 - babeljar021914.jar
 - Tested and seems to be working here: /export/projects/tto16/users/anni/damt/domainCCScoring/scoring/testsublist
BLI:
 - Profiling rerank.py:
 - Hopefully 'startswith' problem goes away!:
	  qlogin -q text.q -l h_rt=3:00:00
	  python rerank.py ceb fast/ rerankll.config
 - Added burstiness measures as features in rerank.py

2/20/14:
 - Submitted ACL author response
 - TACL review for that clinical narrative time annotation paper
 - Compositionality:
    - Trying to remember what was happening last 
    - Had written new supervision set: /export/projects/tto16/users/anni/compPP/getSupervision/supervision.wlex.011514
    - Submitted jobs to score this set here: /export/projects/tto16/users/anni/compPP/getSupervision/
    - Have done wiki-25 and crawls scoring and written all features for new composed set here: oovComposedMMF20_1_8_14_parallel/parallelsmaller/ <- these are ready to concatenate and predict over. Just need trained model from new supervised set...
 - All of these damn jobs are waiting for those damn langid jobs to finish to run... busy cluster. Should be writing anyway.
BLI:
 - Wrote script to browse ranked test predictions output /export/projects/tto16/users/anni/BLI/browse.py
@shreejit:could be used for evaluation

2/21/14:
 - Still babysitting processing jobs (crawls lang id, wiki copying/processing/wordFreq)
 - The BLI rerank.py profiling made a huge difference in terms of how fast featurized files are written! Will make things better going forward!
 - Burstiness features didn't help ceb BLI performance (top-1 and top-100 accuracy down a bit, top-10 up a bit, basically a wash); but still will be useful to profile whether performance is better on bursty words or not
 - Added raw target word frequency to BLI features. Also fixed a couple bugs; wasn't using ranks in a couple places but scores instead
Analysis Section:
 - Running two experiments to disentangle effects of bad word alignments and of incomplete training data
   - Script to get HR word alignments for LR dataset: /export/projects/tto16/users/anni/s4Analysis/ur/getHRWordAlignments.py
Manual Alignment HITs:
 - Got HITs all ready, picked out data for Spanish and Hindi; sentences that are no more than 20 words long
 - Will do about 500 sentences two times redundantly; pay something like $0.50 per HIT
 - I manually aligned ten Spanish sentences and also posted those to be done 25 times redundantly; I'll use those results to give a special qualification to workers to work on additional HITs
 - Got ten Hindi sentences ready to go for annotation by Vijay and Pushpendre. 

2/24/14:
Thesis Writing:
 - Worked on linguistics diversity section of data.tex
 - Got a bunch of WALS data for set of 112 languages (probably will be pruned a little more still)
 - Data stats in: /export/projects/tto16/users/anni/s4Analysis/languagesAnalysis
MTurk:
 - Measured precision, recall, and f-measure of initial alignments given to MTurk workers, added to thesis. Will want threshold to be higher, of course!
 - Reposted Spanish 10 sentence pairs at $0.35 each and 25 times redundantly done. NO ONE had done hits at $0.20 from last Friday
 - Also posted 10 Hindi sentence pairs at $0.35 each and 25 times redundantly. 

2/25/14:
Wikipedia processing:
 - Found a few languages not in the pipeline (th, as, ast, mi), so started those. Babysat others.
DAMT scoring: 
 - Found 0 pages. Wrote script to convert lists to have working pointers: /export/projects/tto16/users/anni/damt/domainCCScoring/wikiSorted/cleanlists.sh
Worked on MTurk dictionaries a little: just going to use 0.6 threshold, 1.0 google overlap threshold (i.e. don't care about good overlap) and my script for removing duplicates (usually english words) and parentheticals and splitting multiple en translations

2/26/14:
Compositionality:
 - Start working on compositionality experiments for another language: Hindi or Tamil/Telugu. 
 -> Need phrase (unigram/bigram/trigram) frequencies like have for Spanish here: /export/common/data/corpora/comparable/wikipedia/pagepairs/en-es/phrases
 -> Started putting them here but can't find script to do it: /export/common/data/corpora/comparable/wikipedia/pagepairs_2014/pagepairs_new/hi/phrases
 -> Need to update oovPhrases.py once have that: /home/hltcoe/airvine/ptRules
Thesis:
 - Finished processing and recording all MTurk dictionaries from Ellie. Some of them super messy...

2/27/14:
Crawls langid:
 - Wrote script to parallelize lang-id by year: 
   qsub -cwd -q text.q -l mem_free=10G,h_rt=299:00:00 -V langid.byyear.sh ur 2014
Hindi phrase frequencies:
 - Getting from here: /export/common/data/corpora/comparable/wikipedia/pagepairs_2014/pagepairs_new/hi/phrases

2/28/14:
 - Wrote script to get MTurk worker's precision, recall, and f-measure compared w/ gold annotations. Just need to wait for all annotations to come in and then grant special qualifications
 - Babyset processing jobs..
 - CCB delayed meeting, home early to crossfit

3/1/14:
MTURK ALIGNMENTS:
 Updated MTurk script a little to accept/reject HITs and write output CSV. But CCB said not to worry about rejecting, so accepting them all. Hindi almost done. Spanish slower.
COMPOSITIONALITY:
 Running oovPhrases.py for HI to get toInduce list: changed minMonoFreqCutoff to 10 instead of 20 (since more Spanish data, higher cutoff); toInduce_2_26_14_hi
 /export/projects/tto16/users/anni/compPP/getSupervision/ <- trying to wiki-score new supervision list for Spanish
 Was going to compose here: /export/projects/tto16/users/anni/compPP/composeTrans.parallel.hi.config, but need induced unigram dictionary, so building that here: /home/hltcoe/airvine/ptRules/bli/sample2000/hi
 -> When scoring done, rerank and then use for composing phrase translations

3/2/14:
COMOPOSITIONALITY:
 - HI BLI done for all dev/test unigrams. 
 - Running reranking: qsub -q text.q -cwd -l mem_free=5G,h_rt=199:00:00 -V featuresScore.sh hi rerankAllTrans.config

3/3/14:
MTURK:
 - Hindi findWorkers is done: have 12 good workers identified
 - Spanish findWorkers still running: have 7 good workers identified so far
 - Big datasets ready to go for annotation: 815 (es) and 537 (hi) sentence pairs for each.
 - Narrowed down to longest 500 of each for round number
 - Hindi qualification granted to 12 workers; Big Hindi hit posted
 - Waiting for Spanish to finish
BLI chapter:
 - Worked through the first parts: introduction and similarity metrics
 - Wrote script to get highest and lowest bursty words using B and IDF: python /export/projects/tto16/users/anni/BLI/browseBursty.py burstinessMeasures/burst.ro.en
@shreejit:burstiness measurement done here
 - Added citations to burstiness papers and worked out comparison
 - Done up to 4.2.2
 - Actually wrote for 2 whole hours!

Meet with CCB:
 - MTurk:
   - Have 10 sentence pairs gold annotated. 1-10 annotations/worker, average precision, recall, and f-measure. 
   - Have 12 HI workers identified; two emailed saying they've passed qualification in the past for Hindi-English. Add them? -> No, don't worry about it.
   - Just glancing at output, it looks like Hindi annotators are actually better
 - DAMT short paper: 
   - Performance not that impressive, really just experimental paper. WMT would probably be more appropriate; go ahead and submit if ready for ACL short deadline
 - ACL paper:
   - Good reviews
   - Running same set of experiments for Hindi now. Hindi is slightly higher resource but should be representative of Indian languages. 
   - Can add manual alignments to intrinsic results (instead of automatic) 
   - If not accepted, resubmit to COLING or CONLL; deadlines both close, decide if rejected
 - Language ID:
   - Implemented for crawls, results not in table yet
 - Language, Data, and Analysis chapter feedback?; not yet
 - BLI is next up: added burstiness features. Solid results on Gujarati and Cebuano; copy CCB on email to boyan
 - Iceland trip details? COE paying? Airline? -> Ask next time!

3/4/14
 - Babysat processing, langid jobs as usual. Some nodes had died. 
Compositionality:
 - Thought I was done with translating dev/devtest hi words (actually, now that I think of it, I've almost certainly done this elsewhere before...), but had forgotten translit dictionary. Rerunning: /home/hltcoe/airvine/ptRules/bli/sample2000/hi. But, yup: /home/hltcoe/airvine/inducePhraseTable/LIMT/Experiments/IndianLangsCorpus/hi-en/rerank_BestScoreAllPruneFewer10/test.subfeats.reranked.scored
 - Ran 60 HI compose jobs with: /export/projects/tto16/users/anni/compPP/cpConfig.sh (using previous induced dict above)
 - Finishes sooo fast!!
Manual Alignments:
 - Hindi big MTurk annotation done! So fast! Spanish findWorkers job still not finished
 - Doing decode of subset of HI that I have manual alignments for: 
    /export/projects/tto16/users/anni/s4Analysis/hi/baseline_1k/wade_manual_test_alignment
 - Got WADE results for Hindi manual alignments
 - Added some text to manual alignments section of paper
 - Trying to also compare a LR alignment model for the WADE analysis, but COE queue down: /export/projects/tto16/users/anni/s4Analysis/Data/hi/alignAll/LRAlign1kandDevTest
BLI:
 - Fixed feature set: made smaller and more manageable. Not going to do greedy search for features either. 
 TO RUN rerank.sh for ceb here when coe cluter gets its act together: /export/projects/tto16/users/anni/BLI
@shreejit:seems like a central worker file. Worth looking at
DAMT Scoring paper:
 - Really think I need to get this to work with fewer than all of the Wikipedia pages! Otherwise it's not really domain-targeted.
 - So working with 5k set again: using 0.1 as a floor minimum on the new features instead of 0.0000001 as before, which is way small. This ay performance shouldn't be hurt so much anyway
 
3/5/14
BLI:
 - Running ceb job
 - Burstiness features seem to be hurting a lot, so trying to isolate that problem and figure it out
 - I think the target raw frequency was the problem... changed it to inverse raw frequency (1/rawfreq). Also comparing 1/log-rawfreq+1 Hopefully that'll help. Otherwise just get rid of it.
 - BAZAM: that worked awesomely. Got up to 11% on top-1 Cebuano. Way higher than before w/o bursty and target freq.
 - BUT: oops, MRR wasn't really real. Updating that to legit MRR... oh, actually, I think I did it in a separate script before for NAACL paper. Just wrapping into main rerank.py now. 
@shreejit: rerank.py seems like a wrapper
 - Testing new MRR in rerank.py and including idf and burstiness ratio features: in test_getrank
DAMT Scoring: 
 - Running minScore job to update 5k science PT; trying 0.1 and 0.01
 ---- Running 0.1 Moses job
Compositionality:
 - Prepped HI scoring jobs (concatenating composed files)
 ---- Ran wiki & crawls scoring jobs; crawls done
 ->>>> Need to getSupervision for HI and score it, get a model. 
 - Then from there repeat Spanish experiments again too. 
WADE Manual alignments: 
 - Spanish is coming along really slowly...
 - Went ahead and gave ES qualification to 8 spanish workers, and started that big job. Might take a while. 
 - Added LR alignment model for WADE analysis. Had problem w/ alignments Sentence mismatch error in GIZA symmetrization but was problem w/ length ratios. Fixed that and discovered needed to un-html-ify MTurk results before looking stuff up in PT, etc. So also fixed that: /export/projects/tto16/users/anni/s4Analysis/hi/baseline_1k/wade_manual_test_alignment/mturkout/cleanmturkout.py

3/6/14:
BLI:
 - Realized context<PruneIfOccursFewerThan> is actually used for selecting source candidates from dictionary. Changed that from 10 to 3
 - Testing new MRR in rerank.py and including idf and burstiness ratio features
 - Updated getRank implementation so that a dictionary is precomputed with ranks for all signals and all en words, instead of getting ranks on the fly. Huge speed-up!
 - Still takes some time to make predictions from VW model, but that's just a call to VW, so can't speed it up. 
 - Added text to thesis
 - Updated feature set to make nice and clean (no ranks-based features)
 - Also adding suffix and prefix scoring and jobs for now; won't have to use if can make case that don't improve performance later. In the meantime repeat a nice, clean set of experiments. 
 ---- running testing for gu and ceb
 ---- Damnit, realized gu needs transliteration. Running scoring Simple, Prefix, and Suffix w/ translit
 ---- Letting withAffixFeats and withoutAffixFeats runrerank.py continue for gu w/o translit just to test
Compositionality:
 ---- Running getSup.py for hi; then will need to score it.
 ---- Running wiki scoring for ES new supervision set
 ->>>> THEN: run predictions on scored ES and HI sets
DAMT Scoring:
 - Moses job died. Upped memory and restarted. Filtering takes a lot of time.
WADE Manual alignments:
 - Just waiting for Spanish to finish.
 - Stupid Spanish annotators. Had to take away qualification from two workers who were just submtiting the HITs. Rejected and reposted all [30] but about 60 HITs, which I'd accidentally approved. Will repost those later. Have a badWorkers list optional for readCSV now.
Data-Thesis:
 - Filled in crawls high-precision data amounts for all languages for which crawls processing is complete
 - Made word cloud to illustrate the amount of monolingual data available for lots of languages: /Users/anni/Dropbox/irvine-bigone/paper/data-figures/monodatawordcloud/

3/7/14:
BLI
 - Check gu runrerank jobs
 - Run rerank if any sixlets of scoring done
 - Run more scoring
 - Running moses translit job; reconcile output when done: bn, ur, ne, hi
 - Get translit dicts for other language pairs that need it when freq files done (fa, uk,...)
 - Update rerank.py to read combined frequency files for languages. Will save some time w/ bigger languages to precompute
 - Debugged readCFileAffix; had error on line 142 trans[0]=="{". Also just made sure readCAffix function worked as advertised.
Compositionality:
 - HI getSup.py; running again b/c seems to have died. But stopped at same place?
 - ES sup wiki scoring?
Temporal Similarity:
 - Went nuts with discrete fourier transforms. Added section to thesis about the comparison. Scripts all in cdcrawls

3/8/14:
BLI:
 - Running more scoring/rerank jobs. Would be great to have all scored w/ prefix and suffix features. Then can do hold-out features experiments to show they're unnecessary (or not, it doesn't really matter, once have them will have them so whatevs).
 - Wrote R script shell so that can just copy results from rerank experiments as they finish and have plot ready to generate
Temporal Similarity:
 - New DFT comparison directory: BLI/temporalSimilarityComparison/
 - Modifed dfttempsigs.py to do all pairwise comparisons when have two directories of signatures and output average ranks and list of ranks

3/9/14:
Babysitting, babysitting jobs
BLI:
 - Scoring edit distance is taking forever for translit languages. Maybe too long. For now: just let run and gather results for others.
 - Individual features script ready to run after have complete withAffixFeats/ jobs... those are taking way longer than others...
 - Got by-freq output ready to go here: blindRunPlusByFreqOutput.py
 - Also will want to do similar thing for burstiness, more importantly even

3/11/14:
Overhaul of jobs running:

Wiki:
 Burstiness Freqs (running): ja, sr, vi, uk
 wikiFreqs: vi
BLI: Starting over w/ MMF10
 Finish Wiki: hu, pl, es
 Need to run moses translit job: bg, fa
 Ready to run scoring: lv, ne, ro, sk, so, ta, tr, ur, uz, cy
 Running scoring: bn, hi, gu, ceb, az, sq, bs, id
 Note: if scoring not fast enough, could also change lists to top 500 instead of 1000
DAMT:
 Science experiments:
   Old list: 
      - 0.01 killed; had also been set to 48hrs runtime
      - 0.1 died (runtime was set to 48hrs) before evaluation. tuning done.
   May as well use 1k list: converted min 0.1, 0.01, 0.001
   -> These tables ready to go; run once translit and mturk-manual-alignments moses jobs are done
MTurk manual alignments:
  Aligning Spanish 1ktrain+dev+devtest for comparison w/ LR automatic alignments
  Running Moses job now: /export/projects/tto16/users/anni/s4Analysis/Data/es/alignAll/LRAlign1kandDevTest
  Running Moses to get decode of just ES for which have manual alignments: /export/projects/tto16/users/anni/s4Analysis/es/baseline_1k/wade_manual_test_alignment
  Finished ES alignment experiments and added text to data.tex
Temporal signature similarity measure:
  Getting EN temporal signatures for es, and ceb and en for ceb. Need to run for id too.
  Wrote up experimental details in thesis

3/12/14:
DAMT:
 Running 1k: 0.1, 0.01, and 0.001 tables running Moses jos
 Running 100k-0.1 Moses decoding: 26.56, about 0.3 over baseline!
Temporal signature similarity measures:
 Running dfttempsig.py comp for es. ceb, id done.
BLI: Starting over w/ MMF10
 Finish Wiki: pl
 Wiki Freq: es, sv
 Running translit: uk
 Ready to run scoring: ur, fa, bg, sr, vi
 Running scoring: tr, ta, uz, hi, cy, hu
 Running reranking: sq, az, bs, so, lv, ne, gu, sk, bn, id, ro
 Reranking done: ceb
 Note: if scoring not fast enough, could also change lists to top 500 instead of 1000
 Note: if reranking w/ affixes isn't fast enough, could not get candidates from them, just use features. Probably shouldn't. 

Temporal signature similarity measure:
 Wrote a bit more about experimental setup for temporal similarity measure comparison. 
 Added CEB results. Just copy and paste list of ranks into a python interactive mode to get top-10 and top-100 accuracies
 R script to generate histograms for a source word and (1) raw frequency based temporal signatures, (2) DFT of raw frequency based temporal signatures:
   irvine-bigone/paper/monofeats-figures/dftanalysis
 Script to write example output for a particular source language word:
   BLI/temporalSimilarityComparison/getExampleOuts.py
@shreejit:could be used for evaluation
 Wrote tons of plotting scripts. Wrapped up that section, just waiting for results to fill in table for ID and ES

LREC paper:
 Zipping up corpus to distribute:
    /export/a04/babel/Resources/Crawls/zcorpus.cleaned.sh and zcorpus.splittoklc.sh should work and make nice tgz file that unzips into nice directories.
    Will want to change name of top directory before zipping after by-state datastats done (12 left now)
 Rerunning all bytime and bystate scripts to update charts

3/13/14:
Compositionality:
 - Got CoNLL paper to fit in 8 pages. Annoying small trimming, nothing major. Commented out some text that might want to put back in for longer versions
 - Scoring HI supervision: /export/projects/tto16/users/anni/compPP/getSupervision
 - Met with David: going to resubmit birds paper to CoNLL. Sent him the draft, which is cut down to 8 pages (from eacl, but he doesn't know that) along with old acl reviews

3/14/14:
BLI:
 - Debugged problem that was making translit languages so slow, I think (instead of checking if src word is in list of hashmap keys, just check if result is null or not. Duh). Rerunning gu and bn jobs to see
CoNLL:
 - Submitted compositionality paper as-is, essentially (8 page version)
 - Sent 8 page version of birds paper to David for work...
MTurk:
 - Shared data with Jonny (two sets of single-annotator alignments for each sentence pair sent to him)
 - Getting more data to post on MTurk from es/hi training sets: 
    /export/projects/tto16/users/anni/s4Analysis/manualAlignments
    Post 1,500 HITs for each language after that

Meet with CCB:
 - LREC data updated. All but one figure in paper. He should look at paper. Almost 4 pages.
 - CoNLL resubmit of ACL paper: it fits in 8 pages now. Content essentially the same. Still not done with Hindi experiments. Dual submissions ok for WMT
 - Resubmitting birds David-paper to CoNLL too.
 - Working on BLI chapter this week 
     - bunch of DFT experiments for comparing temporal signatures; help from Mike Carlin. Should have done a long time ago, but doesn't seem to matter, so that's good
     - Added burstiness features. Going to do analysis by burstiness measure too, like I had done by-frequency before
 - Went over thesis outline with David. He totally approves. Really likes all of the associated datasets, of course
 - MTurk alignment annotation is done: 500 sentences for Spanish-English and Hindi-English. Have done WADE analysis using those. Will also use for other things (evaluating composed phrase translations, e.g.)
 - Next chapter to work on is WMT paper basically; going to rerun fewer of those experiments than the BLI stuff. What I have is actually very clean already.

---- Post more Hindi and Spanish word alignment HITs: total of 2k for each
---- Final-final draft to Hal and David just before LREC. 
---- Beginning of May: chapters to CCB

3/17/14:
LREC paper:
 - Zipping up crawls dataset: /export/a04/babel/Resources/Crawls
Compositionality:
 - HI supervised set done scoring; learning classification models:
   qsub -q text.q -cwd -l mem_free=50G,h_rt=48:00:00 -V classify.py supervisedLearning.hi.config 
 - Got gold info... looks like % of src phrases in table that are in composed table is much smaller than for ES, so performance might not be as good?
 - Predicting HI composed: 
   qsub -q text.q -cwd -l mem_free=50G,h_rt=199:00:00 -V predict.py predictComposed.hi.config 
DAMT:
 - Problem w/ some .newformat files, that's why BLEU scores were so low
 - Recreating a couple; results in a google docs spreadsheet. WMT deadline in just over 2 weeks, so actually really need to get on this b/c Moses jobs take so long

3/18/14:
Compositionality:
 - HI: scoring old table before can make supplemental tables: /export/projects/tto16/users/anni/s4Analysis/hi/baseline_2k/scoredTable
 - Extracted phrases from manual aligned set for eval: /export/projects/tto16/users/anni/s4Analysis/manualAlignments/first500s/es/model
 - Wrote a bunch starting from intro of compositionality chapter. Rerunning some of the motivating experiments testing compositionality over manually word aligned data:
   /export/projects/tto16/users/anni/compPP/testCompositionality
 - Next up: get rid of removing compositional phrase pairs, that's weird! Just add oracle experiments from submitted paper where I'm ADDING them to LR models. Makes much more sense... and copy and paste!
 -> Will need to run for Hindi
Translit:
 - Damnit: realized max phrase limit was 1 in all of those translit systems set up. Should probably be 3 instead. 
 - Don't worry about it?

3/19/14:
LREC data:
 - Splitting up .cleaned and .split.tok.lc files, will zip them each up separately to share whichever people want
   /export/a04/babel/Resources/Crawls/
BLI: Starting over w/ MMF10
 Finish Wiki: pl
 Running translit: uk, vi
 Ready to run scoring: ur, fa, bg, sr, sv, es
 Running scoring: ta[need to restart wiki-pre and wiki-suf], hu[need wiki-pre and wiki-suf]
 Ready for reranking: tr
 Running reranking: sq, az, bs, lv, sk, bn, id, ro
 Need reranking w/ affix: lv, bs, az, sq, ro, bn, id, sk
 Reranking done: ceb, gu, hi, cy, uz, ne, so
 Note: if scoring not fast enough, could also change lists to top 500 instead of 1000
 Note: if reranking w/ affixes isn't fast enough, could not get candidates from them, just use features ---> did this.
Compositionality:
 Writing, writing. 
 Realized it would be useful to have oracle word alignments for dev sets too
 - Posted HI and ES dev sets (all sentence pairs length<=20) to MTurk
 - Got Hindi best dev set references as had test set for WADE before here: /export/projects/tto16/users/anni/s4Analysis/hi/baseline_full
 -> When have set of phrases extracted from manually aligned test data, rerun oracle compositionality experiments for both ES and HI adding those compositional phrase pairs (identify as identified test set compositional phrase pairs in testCompositionality/ ) to baseline table. Will have to rerun testC.py for dev set translations to split found from not found. Also will have to score those phrase pairs w/ comparable corpora
 -> Do tuning only over the set for which have oracle phrase translations, and same with evaluation; run baselines on these tuning and test sets too
Word Alignemnts:
 - Separately, decided I should get word alignments for another less related language pair too, because I know David will ask. Going for Telugu b/c have parallel corpus already used a lot and 3 speakers in CLSP and probably good workers on MTurk.
 - Running alignAll job to get initial test set 
 - Running baseline_full to get wadeBestRefs to use for alignments, then will need to run WADE 
 - MTurk get data script ready to run once have those things: /export/projects/tto16/users/anni/s4Analysis/manualAlignments/writeMTurkCSV.py

3/20/14:
Word Alignments:
 - Set up TE MTurk sandbox HITs for three Telugu speakers at CLSP. Have the rest ready to go too
Thesis:
 - Started a 'contributions' list in intro and wrote about 'language packs'
DAMT:
 - Moses runs started for Science!
 - Started making EMEA cutoff tables
BLI:
 - Started with affix rerank jobs WITHOUT using those candidates. Definitely don't need those candidates if they're not in the other lists
Compositionality:
 - Making Hindi supp tables! Wazam! Then will just need to run Moses jobs. 
 **** Should probably use translit dictionary in addition to induce dictionary for additional unigrams? -> think about this re: old WMT paper

3/21/14:
Birds:
 - Submitted COLING paper
MTurk:
 - Got Telugu speakers to give gold alignments
Compositionality:
 - Making full new style PT for scored Hindi
 - Writing supp tables for Hindi: /export/projects/tto16/users/anni/compMTExps/hi-en/supp

3/24/14:
DAMT:
 - Should score phrase tables with same number of random pages for another baseline comparison
 - Making directories with copies of files. Running scoring is SO MUCH FASTER THAN USING SUBLISTS
 - Working my way through a bunch of DAMT experiments. Started draft of WMT paper. 
Compositionality:
 - Ready to run Moses w/ a Hindi table: /export/projects/tto16/users/anni/compMTExps/hi-en/supp/threshmin_top1_mmax_allfeats
 - But going to wait for DAMT Moses jobs to finish b/c feature numbers are different
Did ACL short reviews

3/25/14:
DAMT Scoring:
 - Running learned models on test2 for learning curves and to evaluate performance at different minimum score thresholds
 - Worked on paper a lot
Thesis:
 - Emailed Hal an outline, a little back and forth

3/26/14:
DAMT Scoring:
 - Worked on paper a lot (again)
 - Babysat jobs...
MTurk alignments:
 - Posted TE find workers HITs
Thesis:
 - Sent Mike DFT stuff to review

3/27/14:
TODOs:
- Email Boyan
- Revisit compositionality paper/chapter and figure out what other Hindi experiments need to be run

- Starting after WMT submissions on 4/1, will:
   ->  (1) integrate DAMT stuff into thesis while fresh in my mind
   ->  (2) wrap up compositionality and BLI chapters.
   ->  (3) Next up after those two, copy WMT '13 paper into that chapter and clean up the scoring chapter (DON'T RUN ANY NEW EXPERIMENTS FOR THOSE!!!! except maybe some wade analyses?)
   ->  (4) Then: lit review, intro 
DAMT: 
 - Did a round of revisions on WMT paper, babysitting jobs still
 - Targeting min score threshold of 0.1 for all experiments
 - Emailed draft to ccb
Compositionality:
 - Realized have two versions of moses scripts and can segregate between running damt jobs and running compositionality jobs, which have different numbers of added features. 
    - That was easy actually, configs were already set up to point to different scripts directories
 - efeats2 for compositionality jobs and efeats for damt jobs
 - Running Hindi compositionality MT jobs right now for top 1, 10, and 100 
 - Making tables for top 3, 5, 25, and 50 now
 ----> Will run script to remove all supplemental phrase pairs except unigrams from top-5 table; that's second baseline
 - Getting unigram BLI performance here: /export/projects/tto16/users/anni/compMTExps/hi-en/analysis
 -> With the above, will have all results needed for a figure like Figure 3 for Hindi
 - Oracle experiments:
    - Composing Hindi translations without induced unigram translations: 
        In compPP: qsub -q text.q -cwd -l mem_free=20G,h_rt=199:00:00 -V composeTrans.py composeTrans.parallel.hi.noinduced.config
    - To get features on all supplemental phrases, running: 
      	In compMTExps/hi-en/oracle:  qsub -cwd -q text.q -l mem_free=20G,h_rt=24:00:00 -V scoreOracleTable.py  /export/projects/tto16/users/anni/compPP/intrinsicResults/gold/031714hi/pt-srcphrasesofinterest
    - Moses oracle setup: 
        - All oracle composed w/ induced and no features: compMTExps/hi-en/oracle/OraclesNoBilingScores/ComposeableOracle_WInduced_WUnigrams
	   ----> This one is ready to run
	- All oracle composed w/ induced and with features: waiting for scoreOracleTable.py to run <- done
	  Now have complete table here: /export/projects/tto16/users/anni/compPP/intrinsicResults/gold/031714hi/pt-srcphrasesofinterest.allfeats 
	  ----> Remove those zeroes and otherwise use directly?
	   Need to figure this out carefully. Looks like want to also not use bilingually estimated scores for new phrase pairs
        - All oracle composed w/o induced and no features: waiting for composeTrans.py to run, <- done
	  - Running getGold.py gold.hi.noinduced.config  <- done
	  ----> Now make oracle table from this
	- All oracle composed w/o induced and with features: need both of above
	- Baseline w/ features: Running job here: /export/projects/tto16/users/anni/s4Analysis/hi/baseline_2k/mosesOnScored <---- only uses 9 CC features, not other 22.

Thesis:
 - Got feedback from Mike on DFT temporal signature stuff and made a few changes. 
 - Ignoring first (magnitude) dimension of DFTs and only comparing half of dimensions. Rerunning ceb job now: /export/projects/tto16/users/anni/BLI/temporalSimilarityComparison/
@shreejit:could be important


3/28/14:
Compositionality:
  - Hindi experiments
    - Baselines/Oracles:
       - Baselines:
          - Baseline with top-5 induced unigram translations: w/ M=0 (consistent w/ es experiments) <- running makeTable jot to get k=5, M=0 table, then will remove all but unigrams
	        ready to run: /export/projects/tto16/users/anni/compMTExps/hi-en/supp/threshmin_top5_m0_allfeats_unigramsonly
		*running
	  - Baseline w/ original PPs but all 31 features: 
	        ready to run: /export/projects/tto16/users/anni/s4Analysis/hi/baseline_2k/mosesOnScored/all31Feats
		*running
       - Oracles:
         - W/ Induced:
	   - W/O Features (just 1 indicator on composed): 
	        ready to run: /export/projects/tto16/users/anni/compMTExps/hi-en/oracle/OraclesNoBilingScores/ComposeableOracle_WInduced_WUnigrams_BaselineFeatures
	   - W/ Features (+30 features, all as 31 except no classifier score): ran scoreOracleTable.py on w/ induced oracle model 
	        ready to run: /export/projects/tto16/users/anni/compMTExps/hi-en/oracle/OraclesNoBilingScoresWAllFeats/ComposeableOracle_WInduced_WUnigrams_AllFeats
		*running
	 - W/O Induced:
	   - W/O Features (just 1 indicator on composed):
	        Making Oracle Table w/o features now (then just run scoreOracleTable.py when want +30 features)
	        ready to run: /export/projects/tto16/users/anni/compMTExps/hi-en/oracle/OraclesNoBilingScores/ComposeableOracle_NoInduced_WUnigrams_BaselineFeatures
	   - W/ Features (+30 features, all as 31 except no classifier score): ran scoreOracleTable.py on w/o induced oracle model
	        ready to run: /export/projects/tto16/users/anni/compMTExps/hi-en/oracle/OraclesNoBilingScoresWAllFeats/ComposeableOracle_NoInduced_WUnigrams_AllFeats
		*running

3/29/14:
DFT Temporal signatures: 
  - Updated DFT comparison to ignore first dimension and then only compare first half of signatures. First dimension is size away from mean or something, was dominating dot products
  - Running getExamples for Spanish (riqueza, adventurero, and terremoto) as well as rerunning jobs to get mean ranks, etc for ceb, id, and es
DAMT:
  - Babysitting jobs, updating results. Second baseline (CC scores using random doc pairs) is between first baseline and our method, which is great
BLI:
  - Running a few reranking jobs

3/31/14:
Compositionality:
 - Ran Hindi Oracle/Baseline/Augment experiments; all three-tuning-runs averaging
 - Made composeStatsSimpleHindi.R figure like Spanish one. Results are reasonably consistent and positive. So yay. 
 - To make figure ran justPR.py python script in compPP. Fast and easy run over tab separated or phrase table style input
DAMT Scoring:
 - Babysitting jobs, but results look ok. Science slow.
 - Sent to CCB again for feedback...
BLI:
 - Running more reranking jobs

4/1/14:
DAMT Scoring:
 - Wrapped up paper; sent to CCB. Experiments still to run for thesis, but good enough for WMT submission
Compositionality:
 - Added all Hindi results (All finished w/ three tuning runs!) to paper; revised some of the writing to reflect Hindi results

4/2/14: 
Thesis: 
 - Wrote all of DAMT chapter. 
 - Just need to fill in Tables 26 and 28, and update Figure 39 as exp. results come in
 - Got dev set oov rates and list of words for which should induce translations here: /export/projects/tto16/users/anni/damt/analysis

4/3/14:
Thesis:
 - Updated DFT section w/ pairs of signatures and some writing
 - Added OOV rates to DAMT chapter
 - Babysat DAMT MT jobs
Compositionality: 
 - Worked through some old stuff on learning decision trees (took some energy to get motivation to start, but now fine), sort of rewrote old script using a config file, but otherwise pretty similar. Think it will be a great, thorough section motivating compositionality. From filtering to composing sort of natural build up. Need to really work through writing tomorrow, update everything to make jive with what end up doing in compose algorithm
   - /export/projects/tto16/users/anni/filterDTs/learndt.py

4/4/14:
BLI babysitting: 
 - Running fr on random 50k sublist from damt experiments
 - Running tr eval separately (b/c ran learning w/ learnonly.sh b/c died in original job)
Compositionality/Decision Trees: 
 - Wrote first two sections of DT motivation, moved sections around in that chapter
 - Made feature extraction function much more clean
 - Writing supervision for each language to reuse, otherwise some randomness in reruns
 - Running lots of jobs w/ varying IG thresholds

4/5/14;
Running TE baseline_2k job, so that can then have PT for all compositionality experiments... need to go ahead and work through that pipeline
 - After that's done, write prob dicts here: /home/hltcoe/airvine/ptRules/bli/probDicts/te; done.
 - After that, need to:
    - Induce unigram translations
    - Get phrase counts here: /export/common/data/corpora/comparable/wikipedia/pagepairs_2014/pagepairs_new/te/phrases; done
Added DT results for hi and es to table 15

4/6/14:
TE compositionality experiments:
 - Getting translit dictionary for te: running getData.py (after wikiFreqs gotten, rerun getData)
 - Moses job queued up to run after that (in hqw)
 - THEN should run scoring jobs and reranking and then induce unigram translations from which to compose phrase translations

4/7/14:
DAMT
 - Running FR rerank to get model for inducing translations for FR new-domain OOVs
Compositionality:
 - TE Moses translit job run
 - Now scoring BLI training job; then rerank; then get unigram translation dict
 - Filtering stuff:
   - Running filtering w/ 1:100 ratio for ES and HI
   - Found bug in decision_tree_learner; reran all jobs w/ ratio 10. 
   - Added ratio to config instead of inside
   - Compared w/ using FMeasure and varying Beta values
   -> Also running  w/ FMeasure instead of IG; varying FMeasure threshold AND beta values. Have plan to show curves at some chosen threshold for each language and then the DTs corresponding to the high-precision and high-recall outputs at either end of the curve. Hopefully those trees can then motivate compositionality alg.

4/8/14:
DAMT:
 - Getting FR burstiness files from old Wiki data, no need to wait for new just for that
Compositionality:
 - Realized was using induced dictionaries on dev/test only but full PT. Got new training data for ES and HI from filtered tables only. Rerunning some experiments. Hopefully will only make induced dictionaries more informative, b/c that's what I end up using in final algorithm
 - Cleaned up filtering writing
 - Got new, clean IG trees and FMeas trees for Spanish and Hindi
 - Also working through Scaling Up subsubsection, filtering all English phrases in comparable EN wikipedias
 - Thought I found a bug in the compose function in composeTrans.py, not just looking up translations for content words. AM just considering content words on the target side when lookup target phrases that have those words... seems like it takes care of itself most of the time, but fixed just in case (added function composeSrcContentOnly that does it as reported in paper)... don't rerun old compositionality experiments, omg. 
 - Wrote script (testDTFilters.py) to use composers (make Composer object), compose translation for each src phrase, and check precision and recall. Debugged w/ Hindi. Should run lots of jobs overnight
 - Made beta-varying-graph and added to thesis
 - Started composing jobs w/ varying freq for what is implemented in compositonality paper. Running for hi and es, minMonoFreqs at 1, 5, 10, 20, 50, 100

4/9/14:
Decision Tree Filters:
 - Thought there was a problem w/ freq filters being too good, but was dividing by number of src phrases incorrectly. So ok now. 
     - In the meantime had:
         - Tried using different test set that's the same as what's used in compositionality paper instead of supervision test set.
	 - Now recall was over any translation for each of a set of source phrases from dev/test(?) sets, not over pairs of src-trg phrases
	 - Wrote a new freq filter test script: testFreqFilter.py (original is restored at testFreqFilterOriginal.py)
 - Chose these filters to showcase for both Hindi and Spanish
     - Freq filters
     - allSrcContentTransInInitialInducedStem
     - allSrcTransInInitial
     - gteq1SrcTransInInitialInducedStem
 - Implemented all in Composer object in compbination with testDTFilters.py
 - Ran jobs to vary minMonoFreq and get prec-recall results 
 - Made omni-graffle diagrams of each of three filters 
 - Added precision-recall figures to thesis with some text
 - Wrote top-down vs. bottom-up intro to next section
 - Somehow integrated both approaches that arrive at the same thing w/ "the careful reader would have noticed..."
 - In the end did decide to use same set of phrase pairs derived from manual alignments to test filters in Scaling Up section. Makes jive with compositionality reachability experiments waaaay better.
 - So, updated prec-rec curves w/ all filters for both languages. 
DAMT:
 - Started a job w/ scored PT (EMEA 1k) and no old domain LM. Will want to run through all of those w/ three tuning runs too...

4/10/14:
- Updated text of Scaling Up section to use test set based on manual alignments just like the compositionality section that comes after
- Added all text from compositionality paper to current chapter
- Wrote new script to get phrasal OOV rates for any language in s4 directory: /export/projects/tto16/users/anni/s4Analysis/Data/getOOVNGramRate.py and replaced old Urdu graph w/ new ones for Hindi and Spanish b/c those are what I experiment with in this chapter
- Read over entire compositionality chapter; draft done!

4/11/14:
Compositionality:
 - Made Spanish ngram oov graph
Filtering Results:
 - Rerunning GTE filters for es 1 and 5.
 - Need to rerun allSrcContent... for hi 1 and es 1 and 3
BLI:
 - Temporal signatures looked funny:
     - Realized it's because only showing intersection of dates; tried doing union (irvine-bigone/paper/monofeats-figures/dftanalysis/esExamples/matchDaysUnion.py), but those look funny, so just added note about intersection to caption. Whatevs.
 - Started rerunning individual features experiments b/c don't have results for all feats using now. Ugh. 
     - All setup already though: ./qsubmitIndFeats.sh lang
@shreejit:seems like a wrapper
     - Submitted job for ro, waiting to start
 - Writing already in decent shape, just need to fix some results up and add by-burstiness (will look kind of like by-freq I guess) results as Boyan suggested

Meeting with CCB:
 -> Footnote for each chapter indicating any publications, add if work is extended (look at his for example) 
 -> Ask David about strong opinion about 1st person sing/pl
 -> \figure[short title]{long title} for figure captions
 - CCB can start consuming chapters beginning of May
 - Lit review in separate chapter

4/12/14:
DAMT:
 - Going to go ahead and score FR words for DAMT using rand and topk sets of DAMT Wiki pages. 
 - Getting lists of words w/ training frequencies under some threshold (100? 10? OOV?) here: /export/projects/tto16/users/anni/damt/BLI
 - Submitted wiki-5k scoring job for science and emea words-to-induce using the top5k for each. Also submitted scoring for emea+science words-to-induce using random5k
BLI:
 - Submitted lots of scoring jobs, a few individual features. 
So much in qw nothing is getting done...ugh

4/13/14:
DAMT:
 - Getting FR model using wiki scored mturk dict pairs on rand50k; using burstiness measures from those SAME pages! 
 - Burstiness measuring here: /export/projects/tto16/users/anni/damt/domainCCScoring/wikiSorted/
    python burstiness.py fr science.all.sorted.1000lengthlimit.random50k.subdirs/ BurstinessFiles/random50k
 - Then learning rerank model here:
    /export/projects/tto16/users/anni/damt/BLI
    python rerank.wikionly.py fr wikiOnlyRerank/ rerankll.config

4/14/14:
DAMT:
 - Learned FR BLI model for DAMT using only wiki features
 - Now using different scoring directories to rerank test translations, then will need to supplement baseline tables w/ and w/o scores (or just with?)
 - Running experiments w/ CC LM too
 - Updated DAMT thesis text w/ empty tables and some writing for BLI experiments

4/15/14:
BLI:
 - Wrote script to get individual features' performance, just update list of languages within it as they complete: evalIndFeats.py
@shreejit:this is in anni/BLI/evalIndFeats.sh
 - Got R script ready to plot individual features: indfeats.png at script bw.r
@shreejit: in anni/THESIS/irvine-bigone/paper/monofeats-figures/featureExps
 - Wrote script to get by-burstiness results: evalByBurstiness.py  
   Have by-burstiness accuracy plots for all languages for which reranking on full feature set is done
 - Updated by-Freq results using raw Wikipedia frequencies in burstiness files
 - Added Spearman's rank correlation coefficient to by-freq and by-burstiness analyses
DAMT: 
 - Updated results tables to confirm what experiments I still have to run

4/16/14:
THESIS:
 - Started 'wish list' for things that I probably shouldn't waste time on, but that would be nice to deal with if there's time in the end (slash just be aware that they're still open issues)
DAMT:
 - Getting unigram supplement jobs working
 - Something funny happened: realized in converting from old format to new was changing order of phrase table features. see convertPTOldToNew.py script for current order:
    editDist, wikiPhraseContext, wikiPhraseTopic, wikiLexContext, wikiLexTopic
 - Got script for supplementing baseline reordering and phrase tables w/ induced translations working. 
   - Had to fix problem w/ edit distance feature, if hadn't been in top-ranked, then ED would be 0, which is actually good... changed so computing on the fly for all translations that are added (not so many)
   - Should be working now but will know for sure when get complete tables written and moses jobs running...
BLI:
 - Wrote script for using model trained on one language to induce translations for another:
    - crossLangTrainEval.py
@shreejit:this is present in anni/BLI/crossLangTrainEval.py
    - Not hard to run, predictions can take a while though. 
    - Added some experimental details and an empty table to thesis
 - Updated Figure w/ monolingual data vs. top-1 and top-1 BLI performance. Just need to keep updated as results come in
 - Moved comparison of projection dictionaries and temporal signatures to appendices
 - Worked through rest of the chapter. Should be in shape so just waiting for results to come in by end of Thursday!

4/17/14:
DAMT:
 - Babysat some MT experiments
BLI:
 - Babysat individual features and useothertraining experiments
PT Scoring:
 - Worked my way through that chapter end-to-end
 - Updated chart
 - Got rid of all reordering references
 - Cleaned up in general, was basically a 1-hour copy and paste job from eacl paper when started. 
 - Made thesis shorter :(
 - Still at 103 pages, but tigher, cleaner

4/18/14:
DAMT:
 - First run of adding BLI translations didn't help performance at all (worse than baseline w/o features even). 
 - Changing default phrase table values for missing features from 1.0 to 0.1 and then will retry... tuning BLEU scores look really good, which is funny
Zero/Small Chapter: 
 - Integrated WMT Indian languages paper into that chapter and moved around the zero parallel text from the previous. 
 - I think the structure makes more sense now, though it's kind of a hodgepodge of results
Data Chapter:
 - Adding two subruns of TETRA +score experiments, with reordering table features and phrase table features separately, before just had together
 - This will set up the discussion at the beginning of zero/small chapter better
 - Also will allow a bit of discussion about reordering there, which is where I should add whatever Hal was talking about in his email
Sublanguage stuff:
 - Got Satoshi Sekine paper to read as Hal suggested
Intro:
 - Made first version of a figure outlining the entire thesis, with chapters labeled

4/19/14:
- Babysitting, organized notes a little
- Updated crawls table and mono data word cloud. Going ahead and including mono data for languages for which don't have dictionary (david will want to know), just marked in table w/ *

4/20/14:
BLI:
 - individual features and cross lang training jobs kept killing nodes. Getting the hell off of the COE grid, copying data to CLSP. Running better there so far...
 - Finished all individual features runs

4/21/14:
BLI:
 - CLSP machines not working for cross lang jobs for some reason. VW gives segmentation fault w/ exactly same command and memory as on COE grid. I guess there's a VW versioning problem which I want to deal with even less than the COE grid
 - Getting evalIndFeats
 - Got crossLang jobs to work on CLSP grid using Hal's newer version of VW and retraining models for all langs (super fast)
 - Ugh, but results are crap. WTF? Now I'm worried that indFeats results are also going to be crap...
 - Playing with learning parameters? 
 - No, ok, now using old version of VW but just have to delete train cache file. Now results exactly as before. Let run for all langs, THEN run cross-lang stuff. I think ind lang stuff will be ok b/c trained new models on old vw there too
 - IndFeats figure done
 - Running crosslang on clsp
OTHER:
 - CoNLL hallucinating and WMT domain adpatation w/ CC for scoring papers accepted?! perrrfect. Only remaining paper is birds, in submission to COLING, wouldn't have to present there!
THESIS:
 - Filled out the dictionaries table w/ complete concatentated dictionaries stats

4/22/14:
BLI:
 - Finished cross-lang and individual features experiments, added updated figures and text to thesis
Appendix:
 - Update chart of language and WALS info
 - Got rid of token-to-type ratios, wasn't really that insightful and had to do for lots more languages to complete
THESIS:
 - Big sweeping updates to pretty much all sections, got rid of some lingering to-dos; added new WMT and CoNLL self-citations. 
LIT REVIEW:
 - Started, mostly grabbed text from long-ago gbo proposal, which isn't bad, thankfully

4/23/14:
WMT Paper:
 - Adding experiment to use +Orthographic feature only first. Could also do others separately? Reviews didn't seem to care as much about that though, and the issue there is would have to do for both random and top, which would be annoying. Orth doesn't actually depend on CC...
 - Getting +Orth tables for EMEA and Science
DAMT:
 - Latest MT experiment adding translations, not good results
 - Getting new tables w/ only top-1 translations (instead of 10) and different deafult PT values (mirrored what did in EMNLP MM paper)

4/24/14:
- Generally feel a little paralyzed waiting for MT results to come in. Maybe that means should work on lit review...
WMT DAMT-Scoring:
 - Ran sig testing on all experiments. Just waiting for +orth experiments to finish (hopefully results are as expected... if not just drop that feature from paper all together?)
CoNLL:
 - Statistical significance testing: set up three tuning runs for ES BLEU scores shown on p-r curve results
Lit Review:
 - Worked on lit review, brought in more stuff from proposal and filled in some newer citations
 - Fixed up a bunch of citations
 - Still need to go through the whole thing and make clean

4/25/14:
BLI:
 - Went ahead and using sv, sr, and hu results WITHOUT affix features. Results are pretty much the same, really not worth it to do long, expensive jobs. 
 - So only waiting for uk and es
 - Working on Haghighi comparison:
    -> Use "all" for all tagsets instead of just noun
    -> Write seed dictionary as output (and i won't use for evaluation)
    -> Evaluation done on most frequent words in corpora, so restrict lexicon input to eval set
    - Need a seed dictionary to use for H system and as (1) projection dictionary and (2) reranker training data. 100 word pairs? Common or whatever from H system is fine? 
    -> Run that first w/ input lexicon (not most frequent nouns!), and then run my scoring on same data
    -> top-1 evaluation b/c H just writes that output dictionary
    -> Haghighi strategy is super sketch: limits EN candidate space to words in dictionary to induce, so matrix is pretty damn small. The good news is that I can get the wiki scores for this in a phrase table format, don't have to worry about extraneous candidates, but just rank all. Then make train and test files from there w/ those features and rerank...
    - Writing directions for how to set everything up (might want to do comparison w/ more than just es-en?) here: /export/projects/tto16/users/anni/BLI/Haghighi/unsuplex-1.0/anni-wiki-es-en/README
@shreejit: a readme! looks like a comparison for her method and haghighi's for spanish and english

Meeting with CCB:
 - Get rid of motivating experiment in CoNLL paper? Or move to beginning of results as short oracle exp?
   -> Move to beginning of results is fine
 - Need to reduce latest (WMT) version by 1 page anyway
 - Schedule defense?
   -> June 16.

4/26/14:
BLI
 - Got Haghighi code working and his output evaluated, running my reranking to compare top-1 performance. Jamming my system into his framework, a little concerned about beating it...
WMT paper:
 - Few updates, prepped for new set of results (no learning curve, varying lm experiments, exp w/ just +orth feats)
COE grid down. All jobs killed. WTF.

4/28/14:
WMT camera-ready:
 - Updated, pretty much ready except jobs still running. Camera-ready extension by one week though, which is excellent.
BLI:
 - Got Haghighi comparison running

4/29/14:
AHA! Found mistake in wiki-only ranking that I was using for DAMT+translation as well as Haghighi comparison: wasn't including edit distance feature, which is *hugely* important
- Rerunning reranking for both
DAMT:
 - Running BLI +trans: 
    - Reranking w/ emea-top5k, science-top5k, rand5k-emea, and rand5k-science
    - VW scoring test in wikiOnlyRerankWithEdit
BLI, Haghighi comparison:
 - Running runrerank.py, output directory: rerankwedit has all features
 - Full eval and 1k eval (_temp) w/o ortho feature just b/c had ready
 - Started writing up Haghighi comparison in thesis
CoNLL paper:
 - Totally refactored motivation/oracle experiments; moved motivating experiment to before experimental results
 - Fixed a couple of errors in Algorithm 1
 - Added citations for Smith, Quirk, and Toutanoval (NAACL 2010) and Munteanu and Marcu (CL 2006) for other uses for comparable corpora

4/30/14:
DAMT-Scoring WMT paper: 
  - All results in and interpretted, stat sig measured, paper updated. Very polished, just read over couple more times before submitting Monday
CoNLL
  - Added table of results
  - Measured stat sig. for es and hi
  - Updated graphs (made consistent w/ results in table)
  - Generally polished the paper up
  - Why Hindi performance drop w/ OOV translations compared to WMT paper? no transliterations? 31 features instead of smaller set? different training data, 2k vs. all
  - Camera ready submitted
BLI
  - Reranking on Hag comparison finished; running eval and also extracting features for just ortho+context experiment

5/1/14:
May?!
DAMT:
 - Updated thesis to be in sync with WMT paper. Might want to add some of the experiments back into WMT paper? Can't decide. 
LR:
 - Added Charles Shafer stuff to chapter 2
 - Added Joy Zhang OOV stuff
 - Added Sekine sublanguage, domain, etc. discussion to LR
Moved language pack information to data chapter and updated intro a little
Few fixes all over the place

5/2/14:
 - Morning: worked on all of it. Restructured and updated lit review (it's much better now). Also added outline and publications to intro, just like CCB's thesis has. 
 - Afternoon: worked on intro mostly
 - Started FR data sampling to do motivation experiments for fr-en. Going to simulate (1) low resource conditions, and (2) new-domain translations over varying amounts of old-domain data on same learning curves (or keep separate?)

5/5/14:
BLI:
 - Changed Haghighi comparison to only use 1k test set pairs b/c Hungarian algorithm was taking ages on my scores. 
 - Added a table and a bit of text to set up the evaluation in Chapter 4
DAMT-Scoring:
 - WMT camera ready submitted
Thesis:
 - Worked on intro a little; added two LR experiments

5/6/14:
Thesis:
 - Finished full draft of Introduction!
 - Full revision of Introduction
 - Full revision of Lit Review
 - Full revision of Data

5/7/14:
Thesis:
 - Spent all morning getting thesis in the JHU thesis style. Included updating captions, adding caption bug fixes, using chapters instead of sections
 - Chapter 7:
     - Updated compositionality to be consistent w/ CoNLL camera ready (add table, update algorithm, etc)
     - Moved a bunch of the decision tree stuff to appendices, just left the introduction to that section in the main text
 - Did a complete once over and fixed figures, tables, etc. to make spacing correct in new format. 
 - Finished writing Hag BLI comparison 

5/8/14:
BLI:
 - Updated individual features plot and made pdf instead of png
Thesis: 
 - Got entire thesis to compile w/o error w/ jhu template
 - Talked to Mike about DFT stuff and cleared all of those notes
 - Worked on intro and abstract

5/9/14:
Thesis:
 - Few edits to BLI chapter
 - Started LC jobs on CLSP cluster, wayyyyyyyyyy faster. omg.
 - Made new learning curve w/ all results from CLSP. So f'ing fast. Got it done in 3 hours.
 - Edited intro and abstract and rest as much as possible
 - Sent to CCB
 
5/21/14:
Thesis: 
 - Added new learning curves over monolingual data. That's set up to run for more languages if want to, once wiki data is available/copied

6/12/14:
Old reordering tables not accessible, so using instead maxfreq0-top1-old1.0new1.0 from science and maxfreq0-top1-old0.1new0.1 from emea

6/18/14:
Indian Languages WADE analysis:
  - Getting translation-details files for select outputs from those 6 learning curves
  - Had alignment error rerunning entire MT pipeline for BN baseline. Instead copying model/ directory from run3 and pointing to alignment file, phrase table, and reordering table in config
    - New config lines: 
     moses-script-dir = /home/hltcoe/airvine/Code/mosesdecoder/scriptsind/
     word-alignment = $working-dir/model/aligned.1
     phrase-translation-table = $working-dir/model/phrase-table.1.gz
     reordering-table = $working-dir/model/reordering-table.1
     decoder-settings = "-translation-details $working-dir/translation-details"

6/19/14:
Indian Languages WADE analysis:
 - Getting word alignments for complete dataset for ML and TA so can do WADE analysis:
    /export/projects/tto16/users/anni/s4Analysis/Data/[ta,ml]/alignAll/alignmentJob
@shreejit:this is present
 - Getting Urdu Post data devtest set word alignments:
    /export/projects/tto16/users/anni/s4Analysis/Data/ur/alignAll/allWPostData
 - All alignments done.
 - WADE results from all of those experiments ALMOST done. 

7/8/14:
Updated DAMT chapter with all +trans results, wrote some text interpreting. Read and edited all

7/9/14:
- Read and edited all Chapter 6
- Added WADE section to after results in small bitext part

7/10/14:
- Finished multi-reference WADE in chapter 6, and pointer from chapter 4
- Debriefing at COE
- Started writing script to get at orthogonality of signals and started section for writing in chapter 4

7/11/14:
 - Did and wrote up orthogonality examination w/ results in chapter 4. 4-5 new pages of content. 


Edits:
MUST DO:
 -> Read through whole thing, Revise all
 -> Revise conclusion w/ David's feedback
 -> Generate tar files for all languages to distribute (will make life easier later), both wikipedia and crawls (can be after deadline)
REALLY SHOULD DO:
 - Show a screen shot of alignment HIT in appendix about manual alignments (from home on big screen) (but TACKY!)
 - Expand Chapter 5 intro a little more to make stronger/make case for why interesting
 - Examples of compositional translations for 7.2 intro
 - In general Chapter 7: more examples: what gets pruned and what doesn't, what is compositional and what is not
 - Examples of induced translations in 8.5.1 - BLI for DAMT
SHOULD BUT MIGHT NOT BECAUSE TIME:
 - *Full page table of example translation predicted by MRR and Supervised model for a few languages
 - Change Chapter 6 intro to make focus less on s4 and more on zero/lr problems (? don't really know what that means, since they are the problems)
COULD DO BUT PROBABLY WON'T BECAUSE STUPID:
 - Reproduce Schafer's table of hierarchy of text resources?
 - Example translations of highly bursty words to 4.3.3
 - Examples of how frequency similarity filteres out false positives
 - In Chapter 4 mention dimensionality reduction, why it is done sometimes and why we don't do it
 - In Chpater 5 whole new set of experiments, lots of new worms
 - In Chapter 8 DAMT induced BLIs, used random 50k (will take too long to run that, and data might be lost anyway)




Wish List:
 - Add avg frequency and burstiness of words to Zero/Small chapter 
 - 1 wordFreq wiki jobs
 **-- Average tuning runs over three for TETRA experiments (can do this if other Moses jobs ever come to an end)
 - Add Telugu stuff to manual word alignment analysis (maybe not b/c claim don't have in compositionality chapter)
 - Compositionality stuff:
     - update spanish results w/ that bigger list of phrases (not excluding name list)
     - remove lexical items for purpose of learning classifier and presenting 10-fold validation results on just this classification
     - rerun prediction and MT w/ new featurized phrase pair list. Also get another classifier using this MMF=20 before do prediciton (working on this)
     - experiment w/ just bigram supplement and thresholds

BLI Status:
 Ready to run scoring: ur, fa, es, pl
 Simple scoring:
    - need: [in original paper: fa, pl]
    - running: es[0.25both]
 Affix scoring:
    - need: ur, fa, es, sv, hu, pl
 Reranking w/o affix feats:
    - done [25]: sq, az, bs, ceb, so, ne, gu, bn, lv, sk, id, ro, uz, cy, hi, ta, tr, bg, te, hu, vi, sv, ur[bad], sr, uk
    - running [0]: es[0.1]
    - need [0]: pl?
 Reranking w/ affix feats (no cands):
    - done [21]: sq, az, bs, ceb, so, ne, gu, bn, lv, sk, id, ro, uz, cy, hi, ta, tr, te, bg, vi, sr
    - running[0]: uk
    - need [4]: hu, sv, es, pl
 Individual features:
    - coe done [3]: ceb, so, uz
    - clsp done [21]: id, bn, az, ceb, tr, vi, bs, so, uz, gu, cy, lv, sk, ta, hi, ro, ne, te, bg, sq, sr, uk
    - running on clsp [0]: 
    - need: es? hu? sv? pl? [haven't scored affix for any but could still do the rest of the features...]

BLI: As finish, update:
 - By frequency plots (and update Spearman's rank results)
 - By burstiness plots (and update Spearman's rank results)
 - Tables 6 and 7 and Figure 12 (best predicting feature and overall accuracies)
 - By-monolingual-frequency text graphs
 - Run individual features jobs
 - Run cross-lingual training jobs

CRAWLS PROCESSING (7): 
 - Running: ru[2004;232/367, 2005;202/366, 2006;135/366, 2007;136/366, 2008;106/367, 2009;47/366, 2010;83/366]
 - Run, for example: qsub -cwd -q text.q -l mem_free=10G,h_rt=299:00:00 -V langid.byyear.sh sk 2014

WIKIPEDIA PROCESSING:
     - Running word freqs by page and Running burstiness: fr

DATA SHARE:
 Wikipedia:
	 From /export/a02/wiki/pagepairs_new/:
   	 ./sharewiki.sh lang
 Crawls
	 From /export/projects/tto16/users/anni/crawlsShare
	 ./ziplangfiles.sh lang
 Then move to:
         /users/anni/public_html/crawlsdata 
	 On gradx, and make readable


TO DO: 
------------------------------------------------------------------------------------------------
CL OT Submission:
 - Explain ganging up effects earlier in paper, and controversy surrounding them (in the 90s there were arguments made against them in phonology lit)
 - Make it clear that architecture of Phonology and Syntax are different (Heinz and Idsardi 2011, 2013)
 - Don't spend so much space talking about batch vs. online learning
 - Theoretical results from Riggle
 - Proposed outline:
1) IF we take an OT-view of syntax THEN a natural question arises as to whether HG or classic OT is more appropriate as this is debate which is occuring in phonology. A crucial aspect of this debate is gang-up effects (explain what these are with examples).

2) When we analyze Czech word order, we observe that a successful linguistic analysis with the alignment constraints we posit *requires* a gang-up effect (so some of the content of 6.4 goes here). Clearly explain why other classic OT analyses fail: why changing the constraints doesn't help (or does it?), etc.

3) In fact, the OT-HG learner can recover this linguistic grammar from a corpus of actual Czech sentences, that we have cleaned up as follows. Here is how we implemented the learner, etc. and here is why we can say it learns what it is supposed to.

4) Compare (or at least mention) other approaches. Contrary to the last line of the current paper, there has been a LOT of work on learning syntax over the years. See Berwick's thesis et seq., Partha Niyogi's book, work by Charles Yang, and a recent book by Clark and Lappin. All of that is completely absent from the current paper. While in-depth comaprisions are unnecessary, the paper should say that it is aware of these non-OT approaches to learning syntax.

5) Conclusion: IF one adopts an OT view of syntax, THEN Czech is a case-study which favors an OT-HG analysis as opposed to a clasic OT analysis because of the gang-up effects. As a bonus, we see that there is enough evidence in a corpus for a HG learner to arrive at the right right kind of grammar.

Bane, Max, Jason Riggle, & Morgan Sonderegger (2010) The VC dimension of constraint-based grammars. Lingua 120(5): 1194-1208
Berwick, Robert. 1985. The acquisition of syntactic knowledge. Cambridge, MA: MIT Press.
Clark, Alexander and Shalom Lappin. 2011. Linguistic Nativism and the Poverty of the Stimulus. Wiley-Blackwell.
Goldwater, Sharon, Thomas L. Griffiths, and Mark Johnson. A Bayesian framework for word segmentation: Exploring the effects of context. Cognition 112 (1), pp. 21--54. 2009
Heinz, Jeffrey and William Idsardi. What Complexity Differences Reveal About Domains in Language. Topics in Cognitive Science, 5(1):111-131, 2013.
Heinz, Jeffrey and William Idsardi. Sentence and Word Complexity. Science, 333(6040):295-297, July 2011.
Niyogi, Partha. 2006. The Computational Nature of Language Learning and Evolution. Cambridge, MA: MIT Press.
Riggle, Jason (2009) Violation Semirings in Optimality Theory. Research on Language and Computation 7(1): 1-12
Riggle, Jason (2009) The Complexity of Ranking Hypotheses in Optimality Theory. Com- putational Linguistics 35(1): 47-59

---From reviewer B
1. (p. 4) Somewhat separate are the efforts to model the particular learning algorithm that allows humans to acquire the grammar of a language This claim/assertion requires further substantiation. Please provide support for this. 

2. In Section 4.3 (p. 6), A state that unlike that work (referring to Costa 1997, 2001), we use alignment constraints on grammatical structures in addition to information structure markers. A bit more clarification is required here, because its not immediately clear to me how As constraints are different from Costas. 

 3. Please consider illustrating your discussion in the final paragraph of Section 4.3 (p. 7) with tableaux or some other sort of visual convention. 

4. Bottom of p. 13, please clarify (for the sake of the reader) why it is a desired result that you want/expect less frequent attributes to have the chance to achieve the same constraint weight magnitude as more frequent attributes. I understand what youre getting at, but this is a crucial point that must be explicated in detail to the reader. 

5. (p. 15) w.r.t. the discussion re: the observation that learners do not learn a consistent alignment preference for the verb (in Czech), is this something that you would predict of all languages that allow scrambling or just the one germane to your current investigation? In other words, is this a potential problem for languages where SVO and OVS are not the most common orderings? 

6. (p. 21) Instead of inventing and adding additional constraints, would it be possible to combine certain already-existing constraints? It would complicate the story a bit here, because you would have to derive some story for the well-formedness of the combination of these constraints, but it would reduce your inventory of constraints  any thoughts on this idea? 

7. (p. 22, final paragraph before the conclusion)  The performance of the MaxEnt and the perceptron models is similar. The perceptron grammar outperforms the MaxEnt model slightly with respect to accuracy and the MaxEnt model outperforms the perceptron grammar by a fair amount in predicting variation. Its not clear to me what the decided (clear-cut) winner is here, or whats being compared here. This needs to be clarified, because, correct me if Im mistaken, but what you intend to compare is the GLA and MaxEnt, correct? As this currently reads, it appears that you are (unintentionally, I assume) comparing MaxEnt and the perceptron. 

Grammatical errors:

There are numerous typos and grammatical issues in this manuscript (see e.g. especially p. 15 where the entire paragraph is rife with multiple errors). Prior to submitting a revised version of this manuscript to the editor, please have a native English speaker proofread this article. 
------------------------------------------------------------------------------------------------



